{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40df0b87",
   "metadata": {},
   "source": [
    "# Import Libraries and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbae32c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "import google.generativeai  as genai\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1deef04",
   "metadata": {},
   "source": [
    "## API KEY SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f37dc1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c86303",
   "metadata": {},
   "source": [
    "## NLTK Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0e82d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Deep\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Deep\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Deep\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Deep\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Deep\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bc4b52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Project directories created !\n"
     ]
    }
   ],
   "source": [
    "def create_project_directories():\n",
    "    directories = [\n",
    "        'data/raw',\n",
    "        'data/processed',\n",
    "        'data/faiss_index',\n",
    "        'logs',\n",
    "        'config'\n",
    "    ]\n",
    "\n",
    "    for directory in directories:\n",
    "        Path(directory).mkdir(parents = True, exist_ok = True)\n",
    "    print(\"‚úÖ Project directories created !\")\n",
    "\n",
    "create_project_directories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1baa4cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_setup():\n",
    "    # Test Gemini\n",
    "    try:\n",
    "        model = genai.GenerativeModel('gemini-2.5-pro')\n",
    "        response = model.generate_content(\"Test\")\n",
    "        print(\"‚úÖ Gemini Working\")\n",
    "    except:\n",
    "        print(\"‚ùå Gemini Failed\")\n",
    "    \n",
    "    # Test NLTK\n",
    "    try:\n",
    "        sent_tokenize(\"Test sentence.\")\n",
    "        print(\"‚úÖ NLTK Working\")\n",
    "    except:\n",
    "        print(\"‚ùå NLTK Failed\")\n",
    "    \n",
    "    # Test FAISS\n",
    "    try:\n",
    "        index = faiss.IndexFlatIP(384)\n",
    "        print(\"‚úÖ FAISS working\")\n",
    "    except:\n",
    "        print(\"‚ùå FAISS Failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5b992e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Gemini Working\n",
      "‚úÖ NLTK Working\n",
      "‚úÖ FAISS working\n"
     ]
    }
   ],
   "source": [
    "validate_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725fb881",
   "metadata": {},
   "source": [
    "# Document Loading & Initial Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c803401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_single_pdf(file_path):\n",
    "    \"Load a single PDF\"\n",
    "    try:\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        pages = loader.load()\n",
    "\n",
    "        full_text = '\\n'.join([page.page_content for page in pages])\n",
    "\n",
    "        # Extract\n",
    "        metadata = {\n",
    "            'filename': os.path.basename(file_path),\n",
    "            'file_path': file_path,\n",
    "            'total_pages': len(pages),\n",
    "            'total_chars': len(full_text)\n",
    "        }\n",
    "\n",
    "        return full_text, metadata\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e4ed093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_documents(document_folder):\n",
    "    \"Load all PDFs from a folder\"\n",
    "    pdf_files = glob.glob(f\"{document_folder}/*.pdf\")\n",
    "\n",
    "    all_documents = []\n",
    "    total_pages = 0\n",
    "\n",
    "    print(f\"Found {len(pdf_files)} PDF files\")\n",
    "\n",
    "    for file_path in tqdm(pdf_files, desc='Loading Documents'):\n",
    "        text, metadata = load_single_pdf(file_path)\n",
    "        if text:\n",
    "            all_documents.append({\n",
    "                'text': text,\n",
    "                'metadata': metadata\n",
    "            })\n",
    "            total_pages += metadata['total_pages']\n",
    "    print(f\"‚úÖ Loaded {len(all_documents)} documents, {total_pages} total_pages\")\n",
    "    return all_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dee1b079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 PDF files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Documents: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [02:07<00:00, 31.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 4 documents, 2578 total_pages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "documents = load_all_documents(\"data/raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4846816",
   "metadata": {},
   "source": [
    "# Document Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68e95045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_document_collection(documents):\n",
    "    \"Analyze your document collection\"\n",
    "\n",
    "    total_docs = len(documents)\n",
    "    total_pages = sum(doc['metadata']['total_pages'] for doc in documents)\n",
    "    total_chars = sum(len(doc['text']) for doc in documents)\n",
    "\n",
    "    # Calculate averages\n",
    "    avg_pages = total_pages / total_docs if total_docs > 0 else 0\n",
    "    avg_chars = total_chars / total_docs if total_chars > 0 else 0\n",
    "\n",
    "    print(\"üìä DOCUMENT COLLECTION ANALYSIS\")\n",
    "    print(f\"Total Documents: {total_docs}\")\n",
    "    print(f\"Total Pages: {total_pages}\")\n",
    "    print(f\"Total Characters: {total_chars:,}\")\n",
    "    print(f\"Average Pages per Doc: {avg_pages:,.1f}\")\n",
    "    print(f\"Average Characters per Doc: {avg_chars:,.0f}\")\n",
    "\n",
    "    return {\n",
    "        'total_docs': total_docs,\n",
    "        'total_pages': total_pages,\n",
    "        'total_chars': total_chars,\n",
    "        'avg_pages': avg_pages,\n",
    "        'avg_chars': avg_chars\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e68b377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä DOCUMENT COLLECTION ANALYSIS\n",
      "Total Documents: 4\n",
      "Total Pages: 2578\n",
      "Total Characters: 5,405,247\n",
      "Average Pages per Doc: 644.5\n",
      "Average Characters per Doc: 1,351,312\n"
     ]
    }
   ],
   "source": [
    "stats = analyze_document_collection(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363ed53d",
   "metadata": {},
   "source": [
    "# Document Preview Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "306cc1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview_document(document, preview_length = 500):\n",
    "    \"Preview first few characters of a document\"\n",
    "\n",
    "    filename = document['metadata']['filename']\n",
    "    text = document['text']\n",
    "\n",
    "    print(f\"\\nüìÑ DOCUMENT: {filename}\")\n",
    "    print(f\"Pages: {document['metadata']['total_pages']}\")\n",
    "    print(f\"Characters: {len(text):,}\")\n",
    "    print(\"\\n--- Preview ---\")\n",
    "    print(text[:preview_length] + \"...\" if len(text) > preview_length else text)\n",
    "    print(\"--- END ---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae646168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ DOCUMENT: Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf\n",
      "Pages: 758\n",
      "Characters: 1,684,466\n",
      "\n",
      "--- Preview ---\n",
      "\n",
      "Information Science and Statistics\n",
      "Series Editors:\n",
      "M. Jordan\n",
      "J. Kleinberg\n",
      "B. Scho¬®lkopf\n",
      "Information Science and Statistics \n",
      "Akaike and Kitagawa: The Practice of Time Series Analysis. \n",
      "Bishop:  Pattern Recognition and Machine Learning. \n",
      "Cowell, Dawid, Lauritzen, and Spiegelhalter: Probabilistic Networks and\n",
      "Expert Systems. \n",
      "Doucet, de Freitas, and Gordon: Sequential Monte Carlo Methods in Practice. \n",
      "Fine: Feedforward Neural Network Methodology. \n",
      "Hawkins and Olwell: Cumulative Sum Charts and Char...\n",
      "--- END ---\n",
      "\n",
      "\n",
      "üìÑ DOCUMENT: Deep Learning by Ian Goodfellow, Yoshua Bengio, Aaron Courville.pdf\n",
      "Pages: 801\n",
      "Characters: 1,769,202\n",
      "\n",
      "--- Preview ---\n",
      "\n",
      "Deep Learning\n",
      "Ian Goodfellow\n",
      "Yoshua Bengio\n",
      "Aaron Courville\n",
      "Contents\n",
      "Website vii\n",
      "Acknowledgments viii\n",
      "Notation xi\n",
      "1 Introduction 1\n",
      "1.1 Who Should Read This Book? . . . . . . . . . . . . . . . . . . . . 8\n",
      "1.2 Historical Trends in Deep Learning . . . . . . . . . . . . . . . . . 11\n",
      "I Applied Math and Machine Learning Basics 29\n",
      "2 Linear Algebra 31\n",
      "2.1 Scalars, Vectors, Matrices and Tensors . . . . . . . . . . . . . . . 31\n",
      "2.2 Multiplying Matrices and Vectors . . . . . . . . . . . . . . . . . . 34\n",
      "2....\n",
      "--- END ---\n",
      "\n",
      "\n",
      "üìÑ DOCUMENT: Hands_On_Machine_Learning_with_Scikit_Learn_and_TensorFlow.pdf\n",
      "Pages: 570\n",
      "Characters: 1,069,600\n",
      "\n",
      "--- Preview ---\n",
      "Aur√©lien G√©ron\n",
      "Hands-On  \n",
      "Machine Learning  \n",
      "with Scikit-Learn  \n",
      "& TensorFlow  \n",
      "CONCEPTS, TOOLS, AND TECHNIQUES  \n",
      "TO BUILD INTELLIGENT SYSTEMS\n",
      "powered by\n",
      "\n",
      "Aur√©lien G√©ron\n",
      "Hands-On Machine Learning with\n",
      "Scikit-Learn and TensorFlow\n",
      "Concepts, Tools, and Techniques to\n",
      "Build Intelligent Systems\n",
      "Boston Farnham Sebastopol TokyoBeijing Boston Farnham Sebastopol TokyoBeijing\n",
      "978-1-491-96229-9\n",
      "[M]\n",
      "Hands-On Machine Learning with Scikit-Learn and TensorFlow\n",
      "by Aur√©lien G√©ron\n",
      "Copyright ¬© 2017 Aur√©lien G√©ron. ...\n",
      "--- END ---\n",
      "\n",
      "\n",
      "üìÑ DOCUMENT: understanding-machine-learning-theory-algorithms.pdf\n",
      "Pages: 449\n",
      "Characters: 881,979\n",
      "\n",
      "--- Preview ---\n",
      "Understanding Machine Learning:\n",
      "From Theory to Algorithms\n",
      "c‚Éù2014 by Shai Shalev-Shwartz and Shai Ben-David\n",
      "Published 2014 by Cambridge University Press.\n",
      "This copy is for personal use only. Not for distribution.\n",
      "Do not post. Please link to:\n",
      "http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning\n",
      "Please note:This copy is almost, but not entirely, identical to the printed version\n",
      "of the book. In particular, page numbers are not identical (but section numbers are the\n",
      "same).\n",
      "\n",
      "Understanding Machi...\n",
      "--- END ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(documents)):\n",
    "    preview_document(documents[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ebae03",
   "metadata": {},
   "source": [
    "# Text Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6955dae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_document_text(text):\n",
    "    \"Clean and normalize document text\"\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Remove page number and header/footers\n",
    "    text = re.sub(r'\\n\\d+\\n', '\\n', text)\n",
    "\n",
    "    # Remove excessive newlines\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "\n",
    "    # Remove special characters that causes issues\n",
    "    text = text.replace('\\x00', '')\n",
    "\n",
    "    # Strip Leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3836ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All document cleaned!\n"
     ]
    }
   ],
   "source": [
    "for doc in documents:\n",
    "    doc['text'] = clean_document_text(doc['text'])\n",
    "\n",
    "print(\"‚úÖ All document cleaned!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74411aa1",
   "metadata": {},
   "source": [
    "# Save Processed Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa63eb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_processed_documents(documents, file_path = 'data/processed/processed_documents.pkl'):\n",
    "    \"Save processed documents for later use\"\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(documents, f)\n",
    "    print(f\"‚úÖ Saved {len(documents)} documents to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0d38451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_documents(file_path = 'data/processed/processed_documents.pkl'):\n",
    "    \"Load previously processed documents\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        documents = pickle.load(f)\n",
    "    print(f\"‚úÖ Loaded {len(documents)} documents from {file_path}\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1f3e152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved 4 documents to data/processed/processed_documents.pkl\n"
     ]
    }
   ],
   "source": [
    "save_processed_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec0ea5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
