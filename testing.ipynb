{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40df0b87",
   "metadata": {},
   "source": [
    "# Import Libraries and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbae32c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "import google.generativeai  as genai\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1deef04",
   "metadata": {},
   "source": [
    "## API KEY SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f37dc1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c86303",
   "metadata": {},
   "source": [
    "## NLTK Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0e82d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Deep\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Deep\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Deep\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Deep\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Deep\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bc4b52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Project directories created !\n"
     ]
    }
   ],
   "source": [
    "def create_project_directories():\n",
    "    directories = [\n",
    "        'data/raw',\n",
    "        'data/processed',\n",
    "        'data/faiss_index',\n",
    "        'logs',\n",
    "        'config'\n",
    "    ]\n",
    "\n",
    "    for directory in directories:\n",
    "        Path(directory).mkdir(parents = True, exist_ok = True)\n",
    "    print(\"‚úÖ Project directories created !\")\n",
    "\n",
    "create_project_directories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1baa4cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_setup():\n",
    "    # Test Gemini\n",
    "    try:\n",
    "        model = genai.GenerativeModel('gemini-2.5-pro')\n",
    "        response = model.generate_content(\"Test\")\n",
    "        print(\"‚úÖ Gemini Working\")\n",
    "    except:\n",
    "        print(\"‚ùå Gemini Failed\")\n",
    "    \n",
    "    # Test NLTK\n",
    "    try:\n",
    "        sent_tokenize(\"Test sentence.\")\n",
    "        print(\"‚úÖ NLTK Working\")\n",
    "    except:\n",
    "        print(\"‚ùå NLTK Failed\")\n",
    "    \n",
    "    # Test FAISS\n",
    "    try:\n",
    "        index = faiss.IndexFlatIP(384)\n",
    "        print(\"‚úÖ FAISS working\")\n",
    "    except:\n",
    "        print(\"‚ùå FAISS Failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5b992e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Gemini Working\n",
      "‚úÖ NLTK Working\n",
      "‚úÖ FAISS working\n"
     ]
    }
   ],
   "source": [
    "validate_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725fb881",
   "metadata": {},
   "source": [
    "# Document Loading & Initial Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c803401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_single_pdf(file_path):\n",
    "    \"Load a single PDF\"\n",
    "    try:\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        pages = loader.load()\n",
    "\n",
    "        full_text = '\\n'.join([page.page_content for page in pages])\n",
    "\n",
    "        # Extract\n",
    "        metadata = {\n",
    "            'filename': os.path.basename(file_path),\n",
    "            'file_path': file_path,\n",
    "            'total_pages': len(pages),\n",
    "            'total_chars': len(full_text)\n",
    "        }\n",
    "\n",
    "        return full_text, metadata\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e4ed093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_documents(document_folder):\n",
    "    \"Load all PDFs from a folder\"\n",
    "    pdf_files = glob.glob(f\"{document_folder}/*.pdf\")\n",
    "\n",
    "    all_documents = []\n",
    "    total_pages = 0\n",
    "\n",
    "    print(f\"Found {len(pdf_files)} PDF files\")\n",
    "\n",
    "    for file_path in tqdm(pdf_files, desc='Loading Documents'):\n",
    "        text, metadata = load_single_pdf(file_path)\n",
    "        if text:\n",
    "            all_documents.append({\n",
    "                'text': text,\n",
    "                'metadata': metadata\n",
    "            })\n",
    "            total_pages += metadata['total_pages']\n",
    "    print(f\"‚úÖ Loaded {len(all_documents)} documents, {total_pages} total_pages\")\n",
    "    return all_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dee1b079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 PDF files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Documents: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [01:17<00:00, 19.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 4 documents, 2578 total_pages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "documents = load_all_documents(\"data/raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4846816",
   "metadata": {},
   "source": [
    "# Document Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68e95045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_document_collection(documents):\n",
    "    \"Analyze your document collection\"\n",
    "\n",
    "    total_docs = len(documents)\n",
    "    total_pages = sum(doc['metadata']['total_pages'] for doc in documents)\n",
    "    total_chars = sum(len(doc['text']) for doc in documents)\n",
    "\n",
    "    # Calculate averages\n",
    "    avg_pages = total_pages / total_docs if total_docs > 0 else 0\n",
    "    avg_chars = total_chars / total_docs if total_chars > 0 else 0\n",
    "\n",
    "    print(\"üìä DOCUMENT COLLECTION ANALYSIS\")\n",
    "    print(f\"Total Documents: {total_docs}\")\n",
    "    print(f\"Total Pages: {total_pages}\")\n",
    "    print(f\"Total Characters: {total_chars:,}\")\n",
    "    print(f\"Average Pages per Doc: {avg_pages:,.1f}\")\n",
    "    print(f\"Average Characters per Doc: {avg_chars:,.0f}\")\n",
    "\n",
    "    return {\n",
    "        'total_docs': total_docs,\n",
    "        'total_pages': total_pages,\n",
    "        'total_chars': total_chars,\n",
    "        'avg_pages': avg_pages,\n",
    "        'avg_chars': avg_chars\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e68b377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä DOCUMENT COLLECTION ANALYSIS\n",
      "Total Documents: 4\n",
      "Total Pages: 2578\n",
      "Total Characters: 5,405,247\n",
      "Average Pages per Doc: 644.5\n",
      "Average Characters per Doc: 1,351,312\n"
     ]
    }
   ],
   "source": [
    "stats = analyze_document_collection(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363ed53d",
   "metadata": {},
   "source": [
    "# Document Preview Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "306cc1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview_document(document, preview_length = 500):\n",
    "    \"Preview first few characters of a document\"\n",
    "\n",
    "    filename = document['metadata']['filename']\n",
    "    text = document['text']\n",
    "\n",
    "    print(f\"\\nüìÑ DOCUMENT: {filename}\")\n",
    "    print(f\"Pages: {document['metadata']['total_pages']}\")\n",
    "    print(f\"Characters: {len(text):,}\")\n",
    "    print(\"\\n--- Preview ---\")\n",
    "    print(text[:preview_length] + \"...\" if len(text) > preview_length else text)\n",
    "    print(\"--- END ---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae646168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ DOCUMENT: Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf\n",
      "Pages: 758\n",
      "Characters: 1,684,466\n",
      "\n",
      "--- Preview ---\n",
      "\n",
      "Information Science and Statistics\n",
      "Series Editors:\n",
      "M. Jordan\n",
      "J. Kleinberg\n",
      "B. Scho¬®lkopf\n",
      "Information Science and Statistics \n",
      "Akaike and Kitagawa: The Practice of Time Series Analysis. \n",
      "Bishop:  Pattern Recognition and Machine Learning. \n",
      "Cowell, Dawid, Lauritzen, and Spiegelhalter: Probabilistic Networks and\n",
      "Expert Systems. \n",
      "Doucet, de Freitas, and Gordon: Sequential Monte Carlo Methods in Practice. \n",
      "Fine: Feedforward Neural Network Methodology. \n",
      "Hawkins and Olwell: Cumulative Sum Charts and Char...\n",
      "--- END ---\n",
      "\n",
      "\n",
      "üìÑ DOCUMENT: Deep Learning by Ian Goodfellow, Yoshua Bengio, Aaron Courville.pdf\n",
      "Pages: 801\n",
      "Characters: 1,769,202\n",
      "\n",
      "--- Preview ---\n",
      "\n",
      "Deep Learning\n",
      "Ian Goodfellow\n",
      "Yoshua Bengio\n",
      "Aaron Courville\n",
      "Contents\n",
      "Website vii\n",
      "Acknowledgments viii\n",
      "Notation xi\n",
      "1 Introduction 1\n",
      "1.1 Who Should Read This Book? . . . . . . . . . . . . . . . . . . . . 8\n",
      "1.2 Historical Trends in Deep Learning . . . . . . . . . . . . . . . . . 11\n",
      "I Applied Math and Machine Learning Basics 29\n",
      "2 Linear Algebra 31\n",
      "2.1 Scalars, Vectors, Matrices and Tensors . . . . . . . . . . . . . . . 31\n",
      "2.2 Multiplying Matrices and Vectors . . . . . . . . . . . . . . . . . . 34\n",
      "2....\n",
      "--- END ---\n",
      "\n",
      "\n",
      "üìÑ DOCUMENT: Hands_On_Machine_Learning_with_Scikit_Learn_and_TensorFlow.pdf\n",
      "Pages: 570\n",
      "Characters: 1,069,600\n",
      "\n",
      "--- Preview ---\n",
      "Aur√©lien G√©ron\n",
      "Hands-On  \n",
      "Machine Learning  \n",
      "with Scikit-Learn  \n",
      "& TensorFlow  \n",
      "CONCEPTS, TOOLS, AND TECHNIQUES  \n",
      "TO BUILD INTELLIGENT SYSTEMS\n",
      "powered by\n",
      "\n",
      "Aur√©lien G√©ron\n",
      "Hands-On Machine Learning with\n",
      "Scikit-Learn and TensorFlow\n",
      "Concepts, Tools, and Techniques to\n",
      "Build Intelligent Systems\n",
      "Boston Farnham Sebastopol TokyoBeijing Boston Farnham Sebastopol TokyoBeijing\n",
      "978-1-491-96229-9\n",
      "[M]\n",
      "Hands-On Machine Learning with Scikit-Learn and TensorFlow\n",
      "by Aur√©lien G√©ron\n",
      "Copyright ¬© 2017 Aur√©lien G√©ron. ...\n",
      "--- END ---\n",
      "\n",
      "\n",
      "üìÑ DOCUMENT: understanding-machine-learning-theory-algorithms.pdf\n",
      "Pages: 449\n",
      "Characters: 881,979\n",
      "\n",
      "--- Preview ---\n",
      "Understanding Machine Learning:\n",
      "From Theory to Algorithms\n",
      "c‚Éù2014 by Shai Shalev-Shwartz and Shai Ben-David\n",
      "Published 2014 by Cambridge University Press.\n",
      "This copy is for personal use only. Not for distribution.\n",
      "Do not post. Please link to:\n",
      "http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning\n",
      "Please note:This copy is almost, but not entirely, identical to the printed version\n",
      "of the book. In particular, page numbers are not identical (but section numbers are the\n",
      "same).\n",
      "\n",
      "Understanding Machi...\n",
      "--- END ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(documents)):\n",
    "    preview_document(documents[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ebae03",
   "metadata": {},
   "source": [
    "# Text Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6955dae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_document_text(text):\n",
    "    \"Clean and normalize document text\"\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Remove page number and header/footers\n",
    "    text = re.sub(r'\\n\\d+\\n', '\\n', text)\n",
    "\n",
    "    # Remove excessive newlines\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "\n",
    "    # Remove special characters that causes issues\n",
    "    text = text.replace('\\x00', '')\n",
    "\n",
    "    # Strip Leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3836ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All document cleaned!\n"
     ]
    }
   ],
   "source": [
    "for doc in documents:\n",
    "    doc['text'] = clean_document_text(doc['text'])\n",
    "\n",
    "print(\"‚úÖ All document cleaned!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74411aa1",
   "metadata": {},
   "source": [
    "# Save Processed Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa63eb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_processed_documents(documents, file_path = 'data/processed/processed_documents.pkl'):\n",
    "    \"Save processed documents for later use\"\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(documents, f)\n",
    "    print(f\"‚úÖ Saved {len(documents)} documents to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0d38451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_documents(file_path = 'data/processed/processed_documents.pkl'):\n",
    "    \"Load previously processed documents\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        documents = pickle.load(f)\n",
    "    print(f\"‚úÖ Loaded {len(documents)} documents from {file_path}\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1f3e152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved 4 documents to data/processed/processed_documents.pkl\n"
     ]
    }
   ],
   "source": [
    "save_processed_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eabd610",
   "metadata": {},
   "source": [
    "# NLTK Text Processing & Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cec0ea5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_intelligent_chunks(text, target_chunk_size=800, overlap_sentences=2):\n",
    "    \"\"\"Create chunks based on sentence boundaries using NLTK\"\"\"\n",
    "    \n",
    "    # Tokenize into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_length = len(sentence)\n",
    "        \n",
    "        # If adding this sentence exceeds target size, finalize current chunk\n",
    "        if current_length + sentence_length > target_chunk_size and current_chunk:\n",
    "            # Join sentences into chunk\n",
    "            chunk_text = \" \".join(current_chunk)\n",
    "            chunks.append(chunk_text)\n",
    "            \n",
    "            # Start new chunk with overlap\n",
    "            if len(current_chunk) > overlap_sentences:\n",
    "                current_chunk = current_chunk[-overlap_sentences:]  # Keep last N sentences\n",
    "                current_length = sum(len(s) for s in current_chunk)\n",
    "            else:\n",
    "                current_chunk = []\n",
    "                current_length = 0\n",
    "        \n",
    "        # Add current sentence\n",
    "        current_chunk.append(sentence)\n",
    "        current_length += sentence_length\n",
    "    \n",
    "    # Add final chunk if exists\n",
    "    if current_chunk:\n",
    "        chunk_text = \" \".join(current_chunk)\n",
    "        chunks.append(chunk_text)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63263ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_documents_to_chunks(documents):\n",
    "    \"\"\"Convert all documents into chunks with metadata\"\"\"\n",
    "    \n",
    "    all_chunks = []\n",
    "    chunk_metadata = []\n",
    "    \n",
    "    for doc_idx, document in enumerate(tqdm(documents, desc=\"Processing documents\")):\n",
    "        text = document['text']\n",
    "        metadata = document['metadata']\n",
    "        \n",
    "        # Create chunks for this document\n",
    "        chunks = create_intelligent_chunks(text)\n",
    "        \n",
    "        # Add each chunk with metadata\n",
    "        for chunk_idx, chunk in enumerate(chunks):\n",
    "            all_chunks.append(chunk)\n",
    "            \n",
    "            # Create metadata for this chunk\n",
    "            chunk_meta = {\n",
    "                'doc_index': doc_idx,\n",
    "                'chunk_index': chunk_idx,\n",
    "                'source_file': metadata['filename'],\n",
    "                'chunk_length': len(chunk),\n",
    "                'doc_total_pages': metadata['total_pages']\n",
    "            }\n",
    "            chunk_metadata.append(chunk_meta)\n",
    "    \n",
    "    print(f\"‚úÖ Created {len(all_chunks)} total chunks from {len(documents)} documents\")\n",
    "    return all_chunks, chunk_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2293e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00,  7.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created 12561 total chunks from 4 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_chunks, chunk_metadata = process_all_documents_to_chunks(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5410a191",
   "metadata": {},
   "source": [
    "# Advanced text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a332187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d86056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_text_preprocessing(chunk):\n",
    "    \"Advanced preprocessing for better embeddings\"\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = chunk.lower()\n",
    "\n",
    "    # Remove excessive whitespace and newlines\n",
    "    text = re.sub(r'\\s+',' ', text)\n",
    "\n",
    "    # Remove URLs and email address\n",
    "    text = re.sub(r'http[s]?:\\\\(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F]))+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "\n",
    "    # Remove excessive puctuation\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "\n",
    "    # Remove extra spaces agian\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ab0bae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['information science and statistics series editors m jordan j kleinberg b scho lkopf information science and statistics akaike and kitagawa the practice of time series analysis bishop pattern recognition and machine learning cowell dawid lauritzen and spiegelhalter probabilistic networks and expert systems doucet de freitas and gordon sequential monte carlo methods in practice fine feedforward neural network methodology hawkins and olwell cumulative sum charts and charting for quality improvement jensen bayesian networks and decision graphs marchette computer intrusion detection and network monitoring a statistical viewpoint rubinstein and kroese the cross entropy method a unified approach to combinatorial optimization monte carlo simulation and machine learning',\n",
       " 'marchette computer intrusion detection and network monitoring a statistical viewpoint rubinstein and kroese the cross entropy method a unified approach to combinatorial optimization monte carlo simulation and machine learning studen√Ω probabilistic conditional independence structures vapnik the nature of statistical learning theory second edition wallace statistical and inductive inference by minimum massage length christopher m bishop pattern recognition and machine learning christopher m bishop f r eng',\n",
       " 'wallace statistical and inductive inference by minimum massage length christopher m bishop pattern recognition and machine learning christopher m bishop f r eng assistant director microsoft research ltd cambridge cb3 0fb u k http research microsoft com h11011cmbishop series editors michael jordan department of computer science and department of statistics university of california berkeley berkeley ca 94720 usa professor jon kleinberg department of computer science cornell university ithaca ny 14853 usa bernhard scho lkopf max planck institute for biological cybernetics spemannstrasse 38 72076 tu bingen germany library of congress control number 2006922522 isbn 10 0 387 31073 8 isbn 13 978 0387 31073 2 printed on acid free paper',\n",
       " 'christopher m bishop pattern recognition and machine learning christopher m bishop f r eng assistant director microsoft research ltd cambridge cb3 0fb u k http research microsoft com h11011cmbishop series editors michael jordan department of computer science and department of statistics university of california berkeley berkeley ca 94720 usa professor jon kleinberg department of computer science cornell university ithaca ny 14853 usa bernhard scho lkopf max planck institute for biological cybernetics spemannstrasse 38 72076 tu bingen germany library of congress control number 2006922522 isbn 10 0 387 31073 8 isbn 13 978 0387 31073 2 printed on acid free paper 2006 springer science business media llc all rights reserved',\n",
       " 'assistant director microsoft research ltd cambridge cb3 0fb u k http research microsoft com h11011cmbishop series editors michael jordan department of computer science and department of statistics university of california berkeley berkeley ca 94720 usa professor jon kleinberg department of computer science cornell university ithaca ny 14853 usa bernhard scho lkopf max planck institute for biological cybernetics spemannstrasse 38 72076 tu bingen germany library of congress control number 2006922522 isbn 10 0 387 31073 8 isbn 13 978 0387 31073 2 printed on acid free paper 2006 springer science business media llc all rights reserved this work may not be translated or copied in whole or in part without the written permission of the publisher springer science business media llc 233 spring street new york ny 10013 usa except for brief excerpts in connection with reviews or scholarly analysis',\n",
       " '2006 springer science business media llc all rights reserved this work may not be translated or copied in whole or in part without the written permission of the publisher springer science business media llc 233 spring street new york ny 10013 usa except for brief excerpts in connection with reviews or scholarly analysis use in connection with any form of information storage and retrieval electronic adaptation computer software or by similar or dissimilar methodology now known or hereafter developed is forbidden the use in this publication of trade names trademarks service marks and similar terms even if they are not identified as such is not to be taken as an expression of opinion as to whether or not they are subject to proprietary rights printed in singapore',\n",
       " 'the use in this publication of trade names trademarks service marks and similar terms even if they are not identified as such is not to be taken as an expression of opinion as to whether or not they are subject to proprietary rights printed in singapore kyo 987654321 springer com this book is dedicated to my family jenna mark and hugh total eclipse of the sun antalya turkey 29 march 2006 preface pattern recognition has its origins in engineering whereas machine learning grew out of computer science however these activities can be viewed as two facets of the same Ô¨Åeld and together they have undergone substantial development over the past ten years',\n",
       " 'preface pattern recognition has its origins in engineering whereas machine learning grew out of computer science however these activities can be viewed as two facets of the same Ô¨Åeld and together they have undergone substantial development over the past ten years in particular bayesian methods have grown from a specialist niche to become mainstream while graphical models have emerged as a general framework for describing and applying probabilistic models also the practical applicability of bayesian methods has been greatly enhanced through the development of a range of approximate inference algorithms such as variational bayes and expectation propa gation similarly new models based on kernels have had signiÔ¨Åcant impact on both algorithms and applications',\n",
       " 'also the practical applicability of bayesian methods has been greatly enhanced through the development of a range of approximate inference algorithms such as variational bayes and expectation propa gation similarly new models based on kernels have had signiÔ¨Åcant impact on both algorithms and applications this new textbook reÔ¨Çects these recent developments while providing a compre hensive introduction to the Ô¨Åelds of pattern recognition and machine learning it is aimed at advanced undergraduates or Ô¨Årst year phd students as well as researchers and practitioners and assumes no previous knowledge of pattern recognition or ma chine learning concepts',\n",
       " 'this new textbook reÔ¨Çects these recent developments while providing a compre hensive introduction to the Ô¨Åelds of pattern recognition and machine learning it is aimed at advanced undergraduates or Ô¨Årst year phd students as well as researchers and practitioners and assumes no previous knowledge of pattern recognition or ma chine learning concepts knowledge of multivariate calculus and basic linear algebra is required and some familiarity with probabilities would be helpful though not es sential as the book includes a self contained introduction to basic probability theory because this book has broad scope it is impossible to provide a complete list of references and in particular no attempt has been made to provide accurate historical attribution of ideas',\n",
       " 'knowledge of multivariate calculus and basic linear algebra is required and some familiarity with probabilities would be helpful though not es sential as the book includes a self contained introduction to basic probability theory because this book has broad scope it is impossible to provide a complete list of references and in particular no attempt has been made to provide accurate historical attribution of ideas instead the aim has been to give references that offer greater detail than is possible here and that hopefully provide entry points into what in some cases is a very extensive literature for this reason the references are often to more recent textbooks and review articles rather than to original sources',\n",
       " 'instead the aim has been to give references that offer greater detail than is possible here and that hopefully provide entry points into what in some cases is a very extensive literature for this reason the references are often to more recent textbooks and review articles rather than to original sources the book is supported by a great deal of additional material including lecture slides as well as the complete set of Ô¨Ågures used in the book and the reader is encouraged to visit the book web site for the latest information http research microsoft com cmbishop prml vii viii preface exercises the exercises that appear at the end of every chapter form an important com ponent of the book',\n",
       " 'for this reason the references are often to more recent textbooks and review articles rather than to original sources the book is supported by a great deal of additional material including lecture slides as well as the complete set of Ô¨Ågures used in the book and the reader is encouraged to visit the book web site for the latest information http research microsoft com cmbishop prml vii viii preface exercises the exercises that appear at the end of every chapter form an important com ponent of the book each exercise has been carefully chosen to reinforce concepts explained in the text or to develop and generalize them in signiÔ¨Åcant ways and each is graded according to difÔ¨Åculty ranging from which denotes a simple exercise taking a few minutes to complete through to which denotes a signiÔ¨Åcantly more complex exercise',\n",
       " 'the book is supported by a great deal of additional material including lecture slides as well as the complete set of Ô¨Ågures used in the book and the reader is encouraged to visit the book web site for the latest information http research microsoft com cmbishop prml vii viii preface exercises the exercises that appear at the end of every chapter form an important com ponent of the book each exercise has been carefully chosen to reinforce concepts explained in the text or to develop and generalize them in signiÔ¨Åcant ways and each is graded according to difÔ¨Åculty ranging from which denotes a simple exercise taking a few minutes to complete through to which denotes a signiÔ¨Åcantly more complex exercise it has been difÔ¨Åcult to know to what extent these solutions should be made widely available',\n",
       " 'each exercise has been carefully chosen to reinforce concepts explained in the text or to develop and generalize them in signiÔ¨Åcant ways and each is graded according to difÔ¨Åculty ranging from which denotes a simple exercise taking a few minutes to complete through to which denotes a signiÔ¨Åcantly more complex exercise it has been difÔ¨Åcult to know to what extent these solutions should be made widely available those engaged in self study will Ô¨Ånd worked solutions very ben eÔ¨Åcial whereas many course tutors request that solutions be available only via the publisher so that the exercises may be used in class',\n",
       " 'it has been difÔ¨Åcult to know to what extent these solutions should be made widely available those engaged in self study will Ô¨Ånd worked solutions very ben eÔ¨Åcial whereas many course tutors request that solutions be available only via the publisher so that the exercises may be used in class in order to try to meet these conÔ¨Çicting requirements those exercises that help amplify key points in the text or that Ô¨Åll in important details have solutions that are available as a pdf Ô¨Åle from the book web site such exercises are denoted by www solutions for the remaining exercises are available to course tutors by contacting the publisher contact details are given on the book web site',\n",
       " 'such exercises are denoted by www solutions for the remaining exercises are available to course tutors by contacting the publisher contact details are given on the book web site readers are strongly encouraged to work through the exercises unaided and to turn to the solutions only as required although this book focuses on concepts and principles in a taught course the students should ideally have the opportunity to experiment with some of the key algorithms using appropriate data sets a companion volume bishop and nabney 2008 will deal with practical aspects of pattern recognition and machine learning and will be accompanied by matlab software implementing most of the algorithms discussed in this book',\n",
       " 'although this book focuses on concepts and principles in a taught course the students should ideally have the opportunity to experiment with some of the key algorithms using appropriate data sets a companion volume bishop and nabney 2008 will deal with practical aspects of pattern recognition and machine learning and will be accompanied by matlab software implementing most of the algorithms discussed in this book acknowledgements first of all i would like to express my sincere thanks to markus svens en who has provided immense help with preparation of Ô¨Ågures and with the typesetting of the book in latex his assistance has been invaluable',\n",
       " 'acknowledgements first of all i would like to express my sincere thanks to markus svens en who has provided immense help with preparation of Ô¨Ågures and with the typesetting of the book in latex his assistance has been invaluable i am very grateful to microsoft research for providing a highly stimulating re search environment and for giving me the freedom to write this book the views and opinions expressed in this book however are my own and are therefore not neces sarily the same as those of microsoft or its afÔ¨Åliates',\n",
       " 'his assistance has been invaluable i am very grateful to microsoft research for providing a highly stimulating re search environment and for giving me the freedom to write this book the views and opinions expressed in this book however are my own and are therefore not neces sarily the same as those of microsoft or its afÔ¨Åliates springer has provided excellent support throughout the Ô¨Ånal stages of prepara tion of this book and i would like to thank my commissioning editor john kimmel for his support and professionalism as well as joseph piliero for his help in design ing the cover and the text format and maryann brickner for her numerous contribu tions during the production phase the inspiration for the cover design came from a discussion with antonio criminisi',\n",
       " 'springer has provided excellent support throughout the Ô¨Ånal stages of prepara tion of this book and i would like to thank my commissioning editor john kimmel for his support and professionalism as well as joseph piliero for his help in design ing the cover and the text format and maryann brickner for her numerous contribu tions during the production phase the inspiration for the cover design came from a discussion with antonio criminisi i also wish to thank oxford university press for permission to reproduce ex cerpts from an earlier textbook neural networks for pattern recognition bishop 1995a the images of the mark 1 perceptron and of frank rosenblatt are repro duced with the permission of arvin calspan advanced technology center',\n",
       " 'i also wish to thank oxford university press for permission to reproduce ex cerpts from an earlier textbook neural networks for pattern recognition bishop 1995a the images of the mark 1 perceptron and of frank rosenblatt are repro duced with the permission of arvin calspan advanced technology center i would also like to thank asela gunawardana for plotting the spectrogram in figure 13 1 and bernhard sch olkopf for permission to use his kernel pca code to plot fig ure 12 17',\n",
       " 'the images of the mark 1 perceptron and of frank rosenblatt are repro duced with the permission of arvin calspan advanced technology center i would also like to thank asela gunawardana for plotting the spectrogram in figure 13 1 and bernhard sch olkopf for permission to use his kernel pca code to plot fig ure 12 17 preface ix many people have helped by proofreading draft material and providing com ments and suggestions including shivani agarwal c edric archambeau arik azran andrew blake hakan cevikalp michael fourman brendan frey zoubin ghahra mani thore graepel katherine heller ralf herbrich geoffrey hinton adam jo hansen matthew johnson michael jordan eva kalyvianaki anitha kannan julia lasserre david liu tom minka ian nabney tonatiuh pena y uan qi sam roweis balaji sanjiya toby sharp ana costa e silva david spiegelhalter jay stokes tara symeonides martin szummer marshall tappen ilkay ulusoy chris williams john winn and andrew zisserman',\n",
       " 'i would also like to thank asela gunawardana for plotting the spectrogram in figure 13 1 and bernhard sch olkopf for permission to use his kernel pca code to plot fig ure 12 17 preface ix many people have helped by proofreading draft material and providing com ments and suggestions including shivani agarwal c edric archambeau arik azran andrew blake hakan cevikalp michael fourman brendan frey zoubin ghahra mani thore graepel katherine heller ralf herbrich geoffrey hinton adam jo hansen matthew johnson michael jordan eva kalyvianaki anitha kannan julia lasserre david liu tom minka ian nabney tonatiuh pena y uan qi sam roweis balaji sanjiya toby sharp ana costa e silva david spiegelhalter jay stokes tara symeonides martin szummer marshall tappen ilkay ulusoy chris williams john winn and andrew zisserman finally i would like to thank my wife jenna who has been hugely supportive throughout the several years it has taken to write this book',\n",
       " 'preface ix many people have helped by proofreading draft material and providing com ments and suggestions including shivani agarwal c edric archambeau arik azran andrew blake hakan cevikalp michael fourman brendan frey zoubin ghahra mani thore graepel katherine heller ralf herbrich geoffrey hinton adam jo hansen matthew johnson michael jordan eva kalyvianaki anitha kannan julia lasserre david liu tom minka ian nabney tonatiuh pena y uan qi sam roweis balaji sanjiya toby sharp ana costa e silva david spiegelhalter jay stokes tara symeonides martin szummer marshall tappen ilkay ulusoy chris williams john winn and andrew zisserman finally i would like to thank my wife jenna who has been hugely supportive throughout the several years it has taken to write this book chris bishop cambridge february 2006 mathematical notation i have tried to keep the mathematical content of the book to the minimum neces sary to achieve a proper understanding of the Ô¨Åeld',\n",
       " 'finally i would like to thank my wife jenna who has been hugely supportive throughout the several years it has taken to write this book chris bishop cambridge february 2006 mathematical notation i have tried to keep the mathematical content of the book to the minimum neces sary to achieve a proper understanding of the Ô¨Åeld however this minimum level is nonzero and it should be emphasized that a good grasp of calculus linear algebra and probability theory is essential for a clear understanding of modern pattern recog nition and machine learning techniques nevertheless the emphasis in this book is on conveying the underlying concepts rather than on mathematical rigour',\n",
       " 'however this minimum level is nonzero and it should be emphasized that a good grasp of calculus linear algebra and probability theory is essential for a clear understanding of modern pattern recog nition and machine learning techniques nevertheless the emphasis in this book is on conveying the underlying concepts rather than on mathematical rigour i have tried to use a consistent notation throughout the book although at times this means departing from some of the conventions used in the corresponding re search literature v ectors are denoted by lower case bold roman letters such as x and all vectors are assumed to be column vectors a superscriptt denotes the transpose of a matrix or vector so thatx t will be a row vector uppercase bold roman letters such as m denote matrices',\n",
       " 'a superscriptt denotes the transpose of a matrix or vector so thatx t will be a row vector uppercase bold roman letters such as m denote matrices the notation w1 w m denotes a row vector with m elements while the corresponding column vector is written as w w1 w m t the notation a b is used to denote theclosed interval from a to b that is the interval including the valuesa and b themselves while a b denotes the correspond ing open interval that is the interval excludinga and b similarly a b denotes an interval that includes a but excludes b for the most part however there will be little need to dwell on such reÔ¨Ånements as whether the end points of an interval are included or not',\n",
       " 'similarly a b denotes an interval that includes a but excludes b for the most part however there will be little need to dwell on such reÔ¨Ånements as whether the end points of an interval are included or not the m m identity matrix also known as the unit matrix is denoted im which will be abbreviated toi where there is no ambiguity about it dimensionality it has elementsiij that equal 1 if i j and 0 if i j a functional is denoted f y where y x is some function the concept of a functional is discussed in appendix d the notation g x o f x denotes that f x g x is bounded asx for instance ifg x 3 x2 2 then g x o x2 the expectation of a functionf x y with respect to a random variablex is de noted by ex f x y',\n",
       " 'for instance ifg x 3 x2 2 then g x o x2 the expectation of a functionf x y with respect to a random variablex is de noted by ex f x y in situations where there is no ambiguity as to which variable is being averaged over this will be simpliÔ¨Åed by omitting the sufÔ¨Åx for instance xi xii mathematical notation e x if the distribution of x is conditioned on another variablez then the corre sponding conditional expectation will be writtenex f x z similarly the variance is denotedvar f x and for vector variables the covariance is writtencov x y w e shall also usecov x as a shorthand notation forcov x x the concepts of expecta tions and covariances are introduced in section 1 2 2',\n",
       " 'similarly the variance is denotedvar f x and for vector variables the covariance is writtencov x y w e shall also usecov x as a shorthand notation forcov x x the concepts of expecta tions and covariances are introduced in section 1 2 2 if we haven values x1 xn of ad dimensional vectorx x1 x d t we can combine the observations into a data matrixx in which the nth row of x corresponds to the row vectorxt n thus the n i element of x corresponds to the ith element of thenth observation xn for the case of one dimensional variables we shall denote such a matrix byx which is a column vector whosenth element is xn note that x which has dimensionalityn uses a different typeface to distinguish it from x which has dimensionalityd',\n",
       " 'for the case of one dimensional variables we shall denote such a matrix byx which is a column vector whosenth element is xn note that x which has dimensionalityn uses a different typeface to distinguish it from x which has dimensionalityd contents preface vii mathematical notation xi contents xiii 1 introduction 1 1 1 example polynomial curve fitting 4 1 2 probability theory 1 2 1 2 1 probability densities 1 7 1 2 2 expectations and covariances 1 9 1 2 3 bayesian probabilities 2 1 1 2 4 the gaussian distribution 2 4 1 2 5 curve Ô¨Åtting re visited 2 8 1 2 6 bayesian curve Ô¨Åtting 3 0 1 3 model selection',\n",
       " '3 2 1 4 the curse of dimensionality 3 3 1 5 decision theory 3 8 1 5 1 minimizing the misclassiÔ¨Åcation rate 3 9 1 5 2 minimizing the expected loss 4 1 1 5 3 the reject option 4 2 1 5 4 inference and decision 4 2 1 5 5 loss functions for regression 4 6 1 6 information theory 4 8 1 6 1 relative entropy and mutual information 5 5 exercises 5 8 xiii xiv contents 2 probability distributions 67 2 1 binary v ariables 6 8 2 1 1 the beta distribution',\n",
       " '6 8 2 1 1 the beta distribution 7 1 2 2 multinomial v ariables 7 4 2 2 1 the dirichlet distribution 7 6 2 3 the gaussian distribution 7 8 2 3 1 conditional gaussian distributions 8 5 2 3 2 marginal gaussian distributions 8 8 2 3 3 bayes theorem for gaussian variables 9 0 2 3 4 maximum likelihood for the gaussian 9 3 2 3 5 sequential estimation 9 4 2 3 6 bayesian inference for the gaussian 9 7 2 3 7 student s t distribution 1 0 2 2 3 8 periodic variables',\n",
       " '1 0 5 2 3 9 mixtures of gaussians 1 1 0 2 4 the exponential family 1 1 3 2 4 1 maximum likelihood and sufÔ¨Åcient statistics 116 2 4 2 conjugate priors 1 1 7 2 4 3 noninformative priors 1 1 7 2 5 nonparametric methods 1 2 0 2 5 1 kernel density estimators 1 2 2 2 5 2 nearest neighbour methods 1 2 4 exercises 1 2 7 3 linear models for regression 137 3 1 linear basis function models 1 3 8 3 1 1 maximum likelihood and least squares 1 4 0 3 1 2 geometry of least squares 1 4 3 3 1 3 sequential learning',\n",
       " '1 4 3 3 1 3 sequential learning 1 4 3 3 1 4 regularized least squares 1 4 4 3 1 5 multiple outputs 1 4 6 3 2 the bias v ariance decomposition 1 4 7 3 3 bayesian linear regression 1 5 2 3 3 1 parameter distribution 1 5 2 3 3 2 predictive distribution 1 5 6 3 3 3 equivalent kernel 1 5 9 3 4 bayesian model comparison 1 6 1 3 5 the evidence approximation 1 6 5 3 5 1 evaluation of the evidence function 1 6 6 3 5 2 maximizing the evidence function 1 6 8 3 5 3 effective number of parameters',\n",
       " '1 6 8 3 5 3 effective number of parameters 1 7 0 3 6 limitations of fixed basis functions 1 7 2 exercises 1 7 3 contents xv 4 linear models for classiÔ¨Åcation 179 4 1 discriminant functions 1 8 1 4 1 1 two classes 1 8 1 4 1 2 multiple classes 1 8 2 4 1 3 least squares for classiÔ¨Åcation 1 8 4 4 1 4 fisher s linear discriminant 1 8 6 4 1 5 relation to least squares 1 8 9 4 1 6 fisher s discriminant for multiple classes 1 9 1 4 1 7 the perceptron algorithm 1 9 2 4 2 probabilistic generative models 1 9 6 4 2 1 continuous inputs',\n",
       " '1 9 6 4 2 1 continuous inputs 1 9 8 4 2 2 maximum likelihood solution 2 0 0 4 2 3 discrete features 2 0 2 4 2 4 exponential family 2 0 2 4 3 probabilistic discriminative models 2 0 3 4 3 1 fixed basis functions 2 0 4 4 3 2 logistic regression 2 0 5 4 3 3 iterative reweighted least squares 2 0 7 4 3 4 multiclass logistic regression 2 0 9 4 3 5 probit regression 2 1 0 4 3 6 canonical link functions 2 1 2 4 4 the laplace approximation 2 1 3 4 4 1 model comparison and bic',\n",
       " '2 1 6 4 5 bayesian logistic regression 2 1 7 4 5 1 laplace approximation 2 1 7 4 5 2 predictive distribution 2 1 8 exercises 2 2 0 5 neural networks 225 5 1 feed forward network functions 2 2 7 5 1 1 weight space symmetries 2 3 1 5 2 network training 2 3 2 5 2 1 parameter optimization 2 3 6 5 2 2 local quadratic approximation 2 3 7 5 2 3 use of gradient information 2 3 9 5 2 4 gradient descent optimization 2 4 0 5 3 error backpropagation 2 4 1 5 3 1 evaluation of error function derivatives',\n",
       " '2 4 0 5 3 error backpropagation 2 4 1 5 3 1 evaluation of error function derivatives 2 4 2 5 3 2 a simple example 2 4 5 5 3 3 efÔ¨Åciency of backpropagation 2 4 6 5 3 4 the jacobian matrix 2 4 7 5 4 the hessian matrix 2 4 9 5 4 1 diagonal approximation 2 5 0 5 4 2 outer product approximation 2 5 1 5 4 3 inverse hessian 2 5 2 xvi contents 5 4 4 finite difference s 2 5 2 5 4 5 exact evaluation of the hessian 2 5 3 5 4 6 fast multiplication by the hessian 2 5 4 5 5 regularization in neural networks',\n",
       " '2 5 6 5 5 1 consistent gaussian priors 2 5 7 5 5 2 early stopping 2 5 9 5 5 3 invariances 2 6 1 5 5 4 tangent propagation 2 6 3 5 5 5 training with transformed data 2 6 5 5 5 6 convolutional networks 2 6 7 5 5 7 soft weight sharing 2 6 9 5 6 mixture density networks 2 7 2 5 7 bayesian neural networks 2 7 7 5 7 1 posterior parameter distribution 2 7 8 5 7 2 hyperparameter optimization 2 8 0 5 7 3 bayesian neural networks for classiÔ¨Åcation 2 8 1 exercises',\n",
       " '2 8 4 6 kernel methods 291 6 1 dual representations 2 9 3 6 2 constructing kernels 2 9 4 6 3 radial basis function networks 2 9 9 6 3 1 nadaraya watson model 3 0 1 6 4 gaussian processes 3 0 3 6 4 1 linear regression revisited 3 0 4 6 4 2 gaussian processes for regression 3 0 6 6 4 3 learning the hyperparameters 3 1 1 6 4 4 automatic relevance determination 3 1 2 6 4 5 gaussian processes for classiÔ¨Åcation 3 1 3 6 4 6 laplace approximation 3 1 5 6 4 7 connection to neural networks 3 1 9 exercises',\n",
       " '3 2 0 7 sparse kernel machines 325 7 1 maximum margin classiÔ¨Åers 3 2 6 7 1 1 overlapping class distributions 3 3 1 7 1 2 relation to logistic regression 3 3 6 7 1 3 multiclass svms 3 3 8 7 1 4 svms for regression 3 3 9 7 1 5 computational learning theory 3 4 4 7 2 relevance v ector machines 3 4 5 7 2 1 rvm for regression 3 4 5 7 2 2 analysis of sparsity 3 4 9 7 2 3 rvm for classiÔ¨Åcation 3 5 3 exercises 3 5 7 contents xvii 8 graphical models 359 8 1 bayesian networks',\n",
       " '3 5 7 contents xvii 8 graphical models 359 8 1 bayesian networks 3 6 0 8 1 1 example polynomial regression 3 6 2 8 1 2 generative models 3 6 5 8 1 3 discrete variables 3 6 6 8 1 4 linear gaussian models 3 7 0 8 2 conditional independence 3 7 2 8 2 1 three example graphs 3 7 3 8 2 2 d separation 3 7 8 8 3 markov random fields 3 8 3 8 3 1 conditional independence properties 3 8 3 8 3 2 factorization properties 3 8 4 8 3 3 illustration image de noising 3 8 7 8 3 4 relation to directed graphs',\n",
       " '3 9 0 8 4 inference in graphical models 3 9 3 8 4 1 inference on a chain 3 9 4 8 4 2 trees 3 9 8 8 4 3 factor graphs 3 9 9 8 4 4 the sum product algorithm 4 0 2 8 4 5 the max sum algorithm 4 1 1 8 4 6 exact inference in general graphs 4 1 6 8 4 7 loopy belief propagation 4 1 7 8 4 8 learning the graph structure 4 1 8 exercises 4 1 8 9 mixture models and em 423 9 1 k means clustering 4 2 4 9 1 1 image segmentation and compression',\n",
       " '4 2 8 9 2 mixtures of gaussians 4 3 0 9 2 1 maximum likelihood 4 3 2 9 2 2 em for gaussian mixtures 4 3 5 9 3 an alternative view of em 4 3 9 9 3 1 gaussian mixtures revisited 4 4 1 9 3 2 relation to k means 4 4 3 9 3 3 mixtures of bernoulli distributions 4 4 4 9 3 4 em for bayesian linear regression 4 4 8 9 4 the em algorithm in general 4 5 0 exercises 4 5 5 10 approximate inference 461 10 1 v ariational inference 4 6 2 10 1 1 factorized distributions 4 6 4 10 1 2 properties of factorized approximations',\n",
       " '4 6 2 10 1 1 factorized distributions 4 6 4 10 1 2 properties of factorized approximations 4 6 6 10 1 3 example the univariate gaussian 4 7 0 10 1 4 model comparison 4 7 3 10 2 illustration v ariational mixture of gaussians 4 7 4 xviii contents 10 2 1 v ariational distribution 4 7 5 10 2 2 v ariational lower bound 4 8 1 10 2 3 predictive density 4 8 2 10 2 4 determining the number of components',\n",
       " '4 8 2 10 2 4 determining the number of components 4 8 3 10 2 5 induced factorizations 4 8 5 10 3 v ariational linear regression 4 8 6 10 3 1 v ariational distribution 4 8 6 10 3 2 predictive distribution 4 8 8 10 3 3 lower bound 4 8 9 10 4 exponential family distributions 4 9 0 10 4 1 v ariational message passing 4 9 1 10 5 local v ariational methods 4 9 3 10 6 v ariational logistic regression 4 9 8 10 6 1 v ariational posterior distribution 4 9 8 10 6 2 optimizing the variational parameters',\n",
       " '5 0 0 10 6 3 inference of hyperparameters 5 0 2 10 7 expectation propagation 5 0 5 10 7 1 example the clutter problem 5 1 1 10 7 2 expectation propagation on graphs 5 1 3 exercises 5 1 7 11 sampling methods 523 11 1 basic sampling algorithms 5 2 6 11 1 1 standard distributions 5 2 6 11 1 2 rejection sampling 5 2 8 11 1 3 adaptive rejection sampling 5 3 0 11 1 4 importance sampling 5 3 2 11 1 5 sampling importance resampling 5 3 4 11 1 6 sampling and the em algorithm 5 3 6 11 2 markov chain monte carlo',\n",
       " '5 3 6 11 2 markov chain monte carlo 5 3 7 11 2 1 markov chains 5 3 9 11 2 2 the metropolis hastings algorithm 5 4 1 11 3 gibbs sampling 5 4 2 11 4 slice sampling 5 4 6 11 5 the hybrid monte carlo algorithm 5 4 8 11 5 1 dynamical systems 5 4 8 11 5 2 hybrid monte carlo 5 5 2 11 6 estimating the partition function 5 5 4 exercises 5 5 6 12 continuous latent variables 559 12 1 principal component analysis 5 6 1 12 1 1 maximum variance formulation 5 6 1 12 1 2 minimum error formulation',\n",
       " '5 6 3 12 1 3 applications of pca 5 6 5 12 1 4 pca for high dimensional data 5 6 9 contents xix 12 2 probabilistic pca 5 7 0 12 2 1 maximum likelihood pca 5 7 4 12 2 2 em algorithm for pca 5 7 7 12 2 3 bayesian pca 5 8 0 12 2 4 factor analysis 5 8 3 12 3 kernel pca 5 8 6 12 4 nonlinear latent v ariable models 5 9 1 12 4 1 independent component analysis 5 9 1 12 4 2 autoassociative neural networks 5 9 2 12 4 3 modelling nonlinear manifolds 5 9 5 exercises',\n",
       " '5 9 9 13 sequential data 605 13 1 markov models 6 0 7 13 2 hidden markov models 6 1 0 13 2 1 maximum likelihood for the hmm 6 1 5 13 2 2 the forward backward algorithm 6 1 8 13 2 3 the sum product algorithm for the hmm 6 2 5 13 2 4 scaling factors 6 2 7 13 2 5 the viterbi algorithm 6 2 9 13 2 6 extensions of the hidden markov model 6 3 1 13 3 linear dynamical systems 6 3 5 13 3 1 inference in lds 6 3 8 13 3 2 learning in lds 6 4 2 13 3 3 extensions of lds 6 4 4 13 3 4 particle Ô¨Ålters',\n",
       " '6 4 4 13 3 4 particle Ô¨Ålters 6 4 5 exercises 6 4 6 14 combining models 653 14 1 bayesian model averaging 6 5 4 14 2 committees 6 5 5 14 3 boosting 6 5 7 14 3 1 minimizing exponential error 6 5 9 14 3 2 error functions for boosting 6 6 1 14 4 tree based models 6 6 3 14 5 conditional mixture models 6 6 6 14 5 1 mixtures of linear regression models 6 6 7 14 5 2 mixtures of logistic models 6 7 0 14 5 3 mixtures of experts 6 7 2 exercises',\n",
       " '6 7 4 appendix a data sets 677 appendix b probability distributions 685 appendix c properties of matrices 695 xx contents appendix d calculus of variations 703 appendix e lagrange multipliers 707 references 711 index 729 1 introduction the problem of searching for patterns in data is a fundamental one and has a long and successful history for instance the extensive astronomical observations of tycho brahe in the 16 th century allowed johannes kepler to discover the empirical laws of planetary motion which in turn provided a springboard for the development of clas sical mechanics similarly the discovery of regularities in atomic spectra played a key role in the development and veriÔ¨Åcation of quantum physics in the early twenti eth century',\n",
       " 'for instance the extensive astronomical observations of tycho brahe in the 16 th century allowed johannes kepler to discover the empirical laws of planetary motion which in turn provided a springboard for the development of clas sical mechanics similarly the discovery of regularities in atomic spectra played a key role in the development and veriÔ¨Åcation of quantum physics in the early twenti eth century the Ô¨Åeld of pattern recognition is concerned with the automatic discov ery of regularities in data through the use of computer algorithms and with the use of these regularities to take actions such as classifying the data into different categories consider the example of recognizing handwritten digits illustrated in figure 1 1',\n",
       " 'the Ô¨Åeld of pattern recognition is concerned with the automatic discov ery of regularities in data through the use of computer algorithms and with the use of these regularities to take actions such as classifying the data into different categories consider the example of recognizing handwritten digits illustrated in figure 1 1 each digit corresponds to a28 28 pixel image and so can be represented by a vector x comprising 784 real numbers the goal is to build a machine that will take such a vector x as input and that will produce the identity of the digit0 9 as the output this is a nontrivial problem due to the wide variability of handwriting it could be 1 2 1 introduction figure 1 1 examples of hand written dig its taken from us zip codes',\n",
       " 'it could be 1 2 1 introduction figure 1 1 examples of hand written dig its taken from us zip codes tackled using handcrafted rules or heuristics for distinguishing the digits based on the shapes of the strokes but in practice such an approach leads to a proliferation of rules and of exceptions to the rules and so on and invariably gives poor results far better results can be obtained by adopting a machine learning approach in which a large set ofn digits x1 xn called a training set is used to tune the parameters of an adaptive model the categories of the digits in the training set are known in advance typically by inspecting them individually and hand labelling them',\n",
       " 'far better results can be obtained by adopting a machine learning approach in which a large set ofn digits x1 xn called a training set is used to tune the parameters of an adaptive model the categories of the digits in the training set are known in advance typically by inspecting them individually and hand labelling them we can express the category of a digit usingtarget vector t which represents the identity of the corresponding digit suitable techniques for representing cate gories in terms of vectors will be discussed later note that there is one such target vector t for each digit imagex',\n",
       " 'suitable techniques for representing cate gories in terms of vectors will be discussed later note that there is one such target vector t for each digit imagex the result of running the machine learning algorithm can be expressed as a function y x which takes a new digit imagex as input and that generates an output vector y encoded in the same way as the target vectors the precise form of the function y x is determined during thetraining phase also known as thelearning phase on the basis of the training data once the model is trained it can then de termine the identity of new digit images which are said to comprise atest set the ability to categorize correctly new examples that differ from those used for train ing is known asgeneralization',\n",
       " 'once the model is trained it can then de termine the identity of new digit images which are said to comprise atest set the ability to categorize correctly new examples that differ from those used for train ing is known asgeneralization in practical applications the variability of the input vectors will be such that the training data can comprise only a tiny fraction of all possible input vectors and so generalization is a central goal in pattern recognition for most practical applications the original input variables are typicallyprepro cessed to transform them into some new space of variables where it is hoped the pattern recognition problem will be easier to solve',\n",
       " 'in practical applications the variability of the input vectors will be such that the training data can comprise only a tiny fraction of all possible input vectors and so generalization is a central goal in pattern recognition for most practical applications the original input variables are typicallyprepro cessed to transform them into some new space of variables where it is hoped the pattern recognition problem will be easier to solve for instance in the digit recogni tion problem the images of the digits are typically translated and scaled so that each digit is contained within a box of a Ô¨Åxed size',\n",
       " 'for most practical applications the original input variables are typicallyprepro cessed to transform them into some new space of variables where it is hoped the pattern recognition problem will be easier to solve for instance in the digit recogni tion problem the images of the digits are typically translated and scaled so that each digit is contained within a box of a Ô¨Åxed size this greatly reduces the variability within each digit class because the location and scale of all the digits are now the same which makes it much easier for a subsequent pattern recognition algorithm to distinguish between the different classes this pre processing stage is sometimes also called feature extraction note that new test data must be pre processed using the same steps as the training data',\n",
       " 'this pre processing stage is sometimes also called feature extraction note that new test data must be pre processed using the same steps as the training data pre processing might also be performed in order to speed up computation for example if the goal is real time face detection in a high resolution video stream the computer must handle huge numbers of pixels per second and presenting these directly to a complex pattern recognition algorithm may be computationally infeasi ble instead the aim is to Ô¨Ånd useful features that are fast to compute and yet that 1 introduction 3 also preserve useful discriminatory information enabling faces to be distinguished from non faces these features are then used as the inputs to the pattern recognition algorithm',\n",
       " 'introduction 3 also preserve useful discriminatory information enabling faces to be distinguished from non faces these features are then used as the inputs to the pattern recognition algorithm for instance the average value of the image intensity over a rectangular subregion can be evaluated extremely efÔ¨Åciently viola and jones 2004 and a set of such features can prove very effective in fast face detection because the number of such features is smaller than the number of pixels this kind of pre processing repre sents a form of dimensionality reduction care must be taken during pre processing because often information is discarded and if this information is important to the solution of the problem then the overall accuracy of the system can suffer',\n",
       " 'because the number of such features is smaller than the number of pixels this kind of pre processing repre sents a form of dimensionality reduction care must be taken during pre processing because often information is discarded and if this information is important to the solution of the problem then the overall accuracy of the system can suffer applications in which the training data comprises examples of the input vectors along with their corresponding target vectors are known assupervised learningprob lems cases such as the digit recognition example in which the aim is to assign each input vector to one of a Ô¨Ånite number of discrete categories are calledclassiÔ¨Åcation problems if the desired output consists of one or more continuous variables then the task is calledregression',\n",
       " 'cases such as the digit recognition example in which the aim is to assign each input vector to one of a Ô¨Ånite number of discrete categories are calledclassiÔ¨Åcation problems if the desired output consists of one or more continuous variables then the task is calledregression an example of a regression problem would be the pre diction of the yield in a chemical manufacturing process in which the inputs consist of the concentrations of reactants the temperature and the pressure in other pattern recognition problems the training data consists of a set of input vectors x without any corresponding target values',\n",
       " 'an example of a regression problem would be the pre diction of the yield in a chemical manufacturing process in which the inputs consist of the concentrations of reactants the temperature and the pressure in other pattern recognition problems the training data consists of a set of input vectors x without any corresponding target values the goal in suchunsupervised learning problems may be to discover groups of similar examples within the data where it is calledclustering or to determine the distribution of data within the input space known asdensity estimation or to project the data from a high dimensional space down to two or three dimensions for the purpose ofvisualization',\n",
       " 'in other pattern recognition problems the training data consists of a set of input vectors x without any corresponding target values the goal in suchunsupervised learning problems may be to discover groups of similar examples within the data where it is calledclustering or to determine the distribution of data within the input space known asdensity estimation or to project the data from a high dimensional space down to two or three dimensions for the purpose ofvisualization finally the technique ofreinforcement learning sutton and barto 1998 is con cerned with the problem of Ô¨Ånding suitable actions to take in a given situation in order to maximize a reward',\n",
       " 'the goal in suchunsupervised learning problems may be to discover groups of similar examples within the data where it is calledclustering or to determine the distribution of data within the input space known asdensity estimation or to project the data from a high dimensional space down to two or three dimensions for the purpose ofvisualization finally the technique ofreinforcement learning sutton and barto 1998 is con cerned with the problem of Ô¨Ånding suitable actions to take in a given situation in order to maximize a reward here the learning algorithm is not given examples of optimal outputs in contrast to supervised learning but must instead discover them by a process of trial and error',\n",
       " 'finally the technique ofreinforcement learning sutton and barto 1998 is con cerned with the problem of Ô¨Ånding suitable actions to take in a given situation in order to maximize a reward here the learning algorithm is not given examples of optimal outputs in contrast to supervised learning but must instead discover them by a process of trial and error typically there is a sequence of states and actions in which the learning algorithm is interacting with its environment in many cases the current action not only affects the immediate reward but also has an impact on the re ward at all subsequent time steps for example by using appropriate reinforcement learning techniques a neural network can learn to play the game of backgammon to a high standard tesauro 1994',\n",
       " 'in many cases the current action not only affects the immediate reward but also has an impact on the re ward at all subsequent time steps for example by using appropriate reinforcement learning techniques a neural network can learn to play the game of backgammon to a high standard tesauro 1994 here the network must learn to take a board position as input along with the result of a dice throw and produce a strong move as the output this is done by having the network play against a copy of itself for perhaps a million games a major challenge is that a game of backgammon can involve dozens of moves and yet it is only at the end of the game that the reward in the form of victory is achieved',\n",
       " 'this is done by having the network play against a copy of itself for perhaps a million games a major challenge is that a game of backgammon can involve dozens of moves and yet it is only at the end of the game that the reward in the form of victory is achieved the reward must then be attributed appropriately to all of the moves that led to it even though some moves will have been good ones and others less so this is an example of acredit assignment problem a general feature of re inforcement learning is the trade off betweenexploration in which the system tries out new kinds of actions to see how effective they are andexploitation in which the system makes use of actions that are known to yield a high reward',\n",
       " 'this is an example of acredit assignment problem a general feature of re inforcement learning is the trade off betweenexploration in which the system tries out new kinds of actions to see how effective they are andexploitation in which the system makes use of actions that are known to yield a high reward too strong a focus on either exploration or exploitation will yield poor results reinforcement learning continues to be an active area of machine learning research however a 4 1 introduction figure 1 2 plot of a training data set of n 10 points shown as blue circles each comprising an observation of the input variable x along with the corresponding target variable t the green curve shows the function sin 2œÄx used to gener ate the data',\n",
       " 'however a 4 1 introduction figure 1 2 plot of a training data set of n 10 points shown as blue circles each comprising an observation of the input variable x along with the corresponding target variable t the green curve shows the function sin 2œÄx used to gener ate the data our goal is to pre dict the value of t for some new value of x without knowledge of the green curve x t 0 1 1 0 1 detailed treatment lies beyond the scope of this book although each of these tasks needs its own tools and techniques many of the key ideas that underpin them are common to all such problems one of the main goals of this chapter is to introduce in a relatively informal way several of the most important of these concepts and to illustrate them using simple examples',\n",
       " 'although each of these tasks needs its own tools and techniques many of the key ideas that underpin them are common to all such problems one of the main goals of this chapter is to introduce in a relatively informal way several of the most important of these concepts and to illustrate them using simple examples later in the book we shall see these same ideas re emerge in the context of more sophisti cated models that are applicable to real world pattern recognition applications this chapter also provides a self contained introduction to three important tools that will be used throughout the book namely probability theory decision theory and infor mation theory',\n",
       " 'later in the book we shall see these same ideas re emerge in the context of more sophisti cated models that are applicable to real world pattern recognition applications this chapter also provides a self contained introduction to three important tools that will be used throughout the book namely probability theory decision theory and infor mation theory although these might sound like daunting topics they are in fact straightforward and a clear understanding of them is essential if machine learning techniques are to be used to best effect in practical applications 1 1 example polynomial curve fitting we begin by introducing a simple regression problem which we shall use as a run ning example throughout this chapter to motivate a number of key concepts',\n",
       " '1 1 example polynomial curve fitting we begin by introducing a simple regression problem which we shall use as a run ning example throughout this chapter to motivate a number of key concepts sup pose we observe a real valued input variablex and we wish to use this observation to predict the value of a real valued target variablet for the present purposes it is in structive to consider an artiÔ¨Åcial example using synthetically generated data because we then know the precise process that generated the data for comparison against any learned model the data for this example is generated from the functionsin 2œÄx with random noise included in the target values as described in detail in appendix a',\n",
       " 'for the present purposes it is in structive to consider an artiÔ¨Åcial example using synthetically generated data because we then know the precise process that generated the data for comparison against any learned model the data for this example is generated from the functionsin 2œÄx with random noise included in the target values as described in detail in appendix a now suppose that we are given a training set comprisingn observations of x written x x1 x n t together with corresponding observations of the values of t denotedt t1 t n t figure 1 2 shows a plot of a training set comprising n 1 0 data points',\n",
       " 'the data for this example is generated from the functionsin 2œÄx with random noise included in the target values as described in detail in appendix a now suppose that we are given a training set comprisingn observations of x written x x1 x n t together with corresponding observations of the values of t denotedt t1 t n t figure 1 2 shows a plot of a training set comprising n 1 0 data points the input data set x in figure 1 2 was generated by choos ing values ofxn for n 1 n spaced uniformly in range 0 1 and the target data set t was obtained by Ô¨Årst computing the corresponding values of the function 1 1',\n",
       " 'now suppose that we are given a training set comprisingn observations of x written x x1 x n t together with corresponding observations of the values of t denotedt t1 t n t figure 1 2 shows a plot of a training set comprising n 1 0 data points the input data set x in figure 1 2 was generated by choos ing values ofxn for n 1 n spaced uniformly in range 0 1 and the target data set t was obtained by Ô¨Årst computing the corresponding values of the function 1 1 example polynomial curve fitting 5 sin 2œÄx and then adding a small level of random noise having a gaussian distri bution the gaussian distribution is discussed in section 1 2 4 to each such point in order to obtain the corresponding valuet n by generating data in this way we are capturing a property of many real data sets namely that they possess an underlying regularity which we wish to learn but that individual observations are corrupted by random noise',\n",
       " 'the input data set x in figure 1 2 was generated by choos ing values ofxn for n 1 n spaced uniformly in range 0 1 and the target data set t was obtained by Ô¨Årst computing the corresponding values of the function 1 1 example polynomial curve fitting 5 sin 2œÄx and then adding a small level of random noise having a gaussian distri bution the gaussian distribution is discussed in section 1 2 4 to each such point in order to obtain the corresponding valuet n by generating data in this way we are capturing a property of many real data sets namely that they possess an underlying regularity which we wish to learn but that individual observations are corrupted by random noise this noise might arise from intrinsically stochastic i e',\n",
       " 'example polynomial curve fitting 5 sin 2œÄx and then adding a small level of random noise having a gaussian distri bution the gaussian distribution is discussed in section 1 2 4 to each such point in order to obtain the corresponding valuet n by generating data in this way we are capturing a property of many real data sets namely that they possess an underlying regularity which we wish to learn but that individual observations are corrupted by random noise this noise might arise from intrinsically stochastic i e random pro cesses such as radioactive decay but more typically is due to there being sources of variability that are themselves unobserved',\n",
       " 'this noise might arise from intrinsically stochastic i e random pro cesses such as radioactive decay but more typically is due to there being sources of variability that are themselves unobserved our goal is to exploit this training set in order to make predictions of the value ÀÜt of the target variable for some new valueÀÜx of the input variable as we shall see later this involves implicitly trying to discover the underlying functionsin 2œÄx this is intrinsically a difÔ¨Åcult problem as we have to generalize from a Ô¨Ånite data set furthermore the observed data are corrupted with noise and so for a givenÀÜx there is uncertainty as to the appropriate value forÀÜt',\n",
       " 'this is intrinsically a difÔ¨Åcult problem as we have to generalize from a Ô¨Ånite data set furthermore the observed data are corrupted with noise and so for a givenÀÜx there is uncertainty as to the appropriate value forÀÜt probability theory discussed in section 1 2 provides a framework for expressing such uncertainty in a precise and quantitative manner and decision theory discussed in section 1 5 allows us to exploit this probabilistic representation in order to make predictions that are optimal according to appropriate criteria for the moment however we shall proceed rather informally and consider a simple approach based on curve Ô¨Åtting',\n",
       " 'probability theory discussed in section 1 2 provides a framework for expressing such uncertainty in a precise and quantitative manner and decision theory discussed in section 1 5 allows us to exploit this probabilistic representation in order to make predictions that are optimal according to appropriate criteria for the moment however we shall proceed rather informally and consider a simple approach based on curve Ô¨Åtting in particular we shall Ô¨Åt the data using a polynomial function of the form y x w w 0 w1x w2x2 wm xm m j 0 wjxj 1 1 where m is the order of the polynomial andxj denotes x raised to the power ofj the polynomial coefÔ¨Åcients w0 w m are collectively denoted by the vectorw',\n",
       " 'in particular we shall Ô¨Åt the data using a polynomial function of the form y x w w 0 w1x w2x2 wm xm m j 0 wjxj 1 1 where m is the order of the polynomial andxj denotes x raised to the power ofj the polynomial coefÔ¨Åcients w0 w m are collectively denoted by the vectorw note that although the polynomial functiony x w is a nonlinear function ofx i t is a linear function of the coefÔ¨Åcientsw functions such as the polynomial which are linear in the unknown parameters have important properties and are calledlinear models and will be discussed extensively in chapters 3 and 4 the values of the coefÔ¨Åcients will be determined by Ô¨Åtting the polynomial to the training data',\n",
       " 'functions such as the polynomial which are linear in the unknown parameters have important properties and are calledlinear models and will be discussed extensively in chapters 3 and 4 the values of the coefÔ¨Åcients will be determined by Ô¨Åtting the polynomial to the training data this can be done by minimizing anerror function that measures the misÔ¨Åt between the functiony x w for any given value ofw and the training set data points one simple choice of error function which is widely used is given by the sum of the squares of the errors between the predictionsy xn w for each data point xn and the corresponding target valuestn so that we minimize e w 1 2 n n 1 y xn w tn 2 1 2 where the factor of1 2 is included for later convenience',\n",
       " 'this can be done by minimizing anerror function that measures the misÔ¨Åt between the functiony x w for any given value ofw and the training set data points one simple choice of error function which is widely used is given by the sum of the squares of the errors between the predictionsy xn w for each data point xn and the corresponding target valuestn so that we minimize e w 1 2 n n 1 y xn w tn 2 1 2 where the factor of1 2 is included for later convenience we shall discuss the mo tivation for this choice of error function later in this chapter for the moment we simply note that it is a nonnegative quantity that would be zero if and only if the 6 1',\n",
       " 'we shall discuss the mo tivation for this choice of error function later in this chapter for the moment we simply note that it is a nonnegative quantity that would be zero if and only if the 6 1 introduction figure 1 3 the error function 1 2 corre sponds to one half of the sum of the squares of the displacements shown by the vertical green bars of each data point from the function y x w t x y xn w tn xn function y x w were to pass exactly through each training data point the geomet rical interpretation of the sum of squares error function is illustrated in figure 1 3 we can solve the curve Ô¨Åtting problem by choosing the value ofw for which e w is as small as possible',\n",
       " 'the geomet rical interpretation of the sum of squares error function is illustrated in figure 1 3 we can solve the curve Ô¨Åtting problem by choosing the value ofw for which e w is as small as possible because the error function is a quadratic function of the coefÔ¨Åcientsw its derivatives with respect to the coefÔ¨Åcients will be linear in the elements of w and so the minimization of the error function has a unique solution denoted by w which can be found in closed form the resulting polynomial isexercise 1 1 given by the functiony x w there remains the problem of choosing the orderm of the polynomial and as we shall see this will turn out to be an example of an important concept calledmodel comparison or model selection',\n",
       " 'the resulting polynomial isexercise 1 1 given by the functiony x w there remains the problem of choosing the orderm of the polynomial and as we shall see this will turn out to be an example of an important concept calledmodel comparison or model selection in figure 1 4 we show four examples of the results of Ô¨Åtting polynomials having orders m 0 1 3 a n d9 to the data set shown in figure 1 2 we notice that the constant m 0 and Ô¨Årst order m 1 polynomials give rather poor Ô¨Åts to the data and consequently rather poor representations of the function sin 2œÄx the third order m 3 polynomial seems to give the best Ô¨Åt to the function sin 2œÄx of the examples shown in figure 1 4 when we go to a much higher order polynomial m 9 we obtain an excellent Ô¨Åt to the training data',\n",
       " 'the third order m 3 polynomial seems to give the best Ô¨Åt to the function sin 2œÄx of the examples shown in figure 1 4 when we go to a much higher order polynomial m 9 we obtain an excellent Ô¨Åt to the training data in fact the polynomial passes exactly through each data point ande w 0 however the Ô¨Åtted curve oscillates wildly and gives a very poor representation of the function sin 2œÄx this latter behaviour is known asover Ô¨Åtting as we have noted earlier the goal is to achieve good generalization by making accurate predictions for new data',\n",
       " 'this latter behaviour is known asover Ô¨Åtting as we have noted earlier the goal is to achieve good generalization by making accurate predictions for new data we can obtain some quantitative insight into the dependence of the generalization performance onm by considering a separate test set comprising 100 data points generated using exactly the same procedure used to generate the training set points but with new choices for the random noise values included in the target values for each choice ofm we can then evaluate the residual value ofe w given by 1 2 for the training data and we can also evaluatee w for the test data set it is sometimes more convenient to use the root mean square 1 1',\n",
       " 'for each choice ofm we can then evaluate the residual value ofe w given by 1 2 for the training data and we can also evaluatee w for the test data set it is sometimes more convenient to use the root mean square 1 1 example polynomial curve fitting 7 x t m 0 0 1 1 0 1 x t m 1 0 1 1 0 1 x t m 3 0 1 1 0 1 x t m 9 0 1 1 0 1 figure 1 4 plots of polynomials having various orders m shown as red curves Ô¨Åtted to the data set shown in figure 1 2 rms error deÔ¨Åned by erms 2e w n 1 3 in which the division by n allows us to compare different sizes of data sets on an equal footing and the square root ensures thaterms is measured on the same scale and in the same units as the target variablet',\n",
       " 'example polynomial curve fitting 7 x t m 0 0 1 1 0 1 x t m 1 0 1 1 0 1 x t m 3 0 1 1 0 1 x t m 9 0 1 1 0 1 figure 1 4 plots of polynomials having various orders m shown as red curves Ô¨Åtted to the data set shown in figure 1 2 rms error deÔ¨Åned by erms 2e w n 1 3 in which the division by n allows us to compare different sizes of data sets on an equal footing and the square root ensures thaterms is measured on the same scale and in the same units as the target variablet graphs of the training and test set rms errors are shown for various values ofm in figure 1 5 the test set error is a measure of how well we are doing in predicting the values oft for new data observations ofx',\n",
       " 'graphs of the training and test set rms errors are shown for various values ofm in figure 1 5 the test set error is a measure of how well we are doing in predicting the values oft for new data observations ofx we note from figure 1 5 that small values ofm give relatively large values of the test set error and this can be attributed to the fact that the corresponding polynomials are rather inÔ¨Çexible and are incapable of capturing the oscillations in the function sin 2œÄx v alues ofm in the range 3 m 8 give small values for the test set error and these also give reasonable representations of the generating functionsin 2œÄx as can be seen for the case ofm 3 from figure 1 4 8 1',\n",
       " 'v alues ofm in the range 3 m 8 give small values for the test set error and these also give reasonable representations of the generating functionsin 2œÄx as can be seen for the case ofm 3 from figure 1 4 8 1 introduction figure 1 5 graphs of the root mean square error deÔ¨Åned by 1 3 evaluated on the training set and on an inde pendent test set for various values of m m erms 0 3 6 9 0 0 5 1 training test for m 9 the training set error goes to zero as we might expect because this polynomial contains10 degrees of freedom corresponding to the10 coefÔ¨Åcients w0 w 9 and so can be tuned exactly to the10 data points in the training set',\n",
       " '8 1 introduction figure 1 5 graphs of the root mean square error deÔ¨Åned by 1 3 evaluated on the training set and on an inde pendent test set for various values of m m erms 0 3 6 9 0 0 5 1 training test for m 9 the training set error goes to zero as we might expect because this polynomial contains10 degrees of freedom corresponding to the10 coefÔ¨Åcients w0 w 9 and so can be tuned exactly to the10 data points in the training set however the test set error has become very large and as we saw in figure 1 4 the corresponding function y x w exhibits wild oscillations this may seem paradoxical because a polynomial of given order contains all lower order polynomials as special cases',\n",
       " 'however the test set error has become very large and as we saw in figure 1 4 the corresponding function y x w exhibits wild oscillations this may seem paradoxical because a polynomial of given order contains all lower order polynomials as special cases them 9 polynomial is therefore capa ble of generating results at least as good as them 3 polynomial furthermore we might suppose that the best predictor of new data would be the functionsin 2œÄx from which the data was generated and we shall see later that this is indeed the case',\n",
       " 'them 9 polynomial is therefore capa ble of generating results at least as good as them 3 polynomial furthermore we might suppose that the best predictor of new data would be the functionsin 2œÄx from which the data was generated and we shall see later that this is indeed the case we know that a power series expansion of the function sin 2œÄx contains terms of all orders so we might expect that results should improve monotonically as we increase m we can gain some insight into the problem by examining the values of the co efÔ¨Åcients w obtained from polynomials of various order as shown in table 1 1 we see that asm increases the magnitude of the coefÔ¨Åcients typically gets larger',\n",
       " 'we know that a power series expansion of the function sin 2œÄx contains terms of all orders so we might expect that results should improve monotonically as we increase m we can gain some insight into the problem by examining the values of the co efÔ¨Åcients w obtained from polynomials of various order as shown in table 1 1 we see that asm increases the magnitude of the coefÔ¨Åcients typically gets larger in particular for them 9 polynomial the coefÔ¨Åcients have become Ô¨Ånely tuned to the data by developing large positive and negative values so that the correspond table 1 1 table of the coefÔ¨Åcients w for polynomials of various order observe how the typical mag nitude of the coefÔ¨Åcients in creases dramatically as the or der of the polynomial increases',\n",
       " 'in particular for them 9 polynomial the coefÔ¨Åcients have become Ô¨Ånely tuned to the data by developing large positive and negative values so that the correspond table 1 1 table of the coefÔ¨Åcients w for polynomials of various order observe how the typical mag nitude of the coefÔ¨Åcients in creases dramatically as the or der of the polynomial increases m 0 m 1 m 6 m 9 w 0 0 19 0 82 0 31 0 35 w 1 1 27 7 99 232 37 w 2 25 43 5321 83 w 3 17 37 48568 31 w 4 231639 30 w 5 640042 26 w 6 1061800 52 w 7 1042400 18 w 8 557682 99 w 9 125201 43 1 1',\n",
       " 'observe how the typical mag nitude of the coefÔ¨Åcients in creases dramatically as the or der of the polynomial increases m 0 m 1 m 6 m 9 w 0 0 19 0 82 0 31 0 35 w 1 1 27 7 99 232 37 w 2 25 43 5321 83 w 3 17 37 48568 31 w 4 231639 30 w 5 640042 26 w 6 1061800 52 w 7 1042400 18 w 8 557682 99 w 9 125201 43 1 1 example polynomial curve fitting 9 x t n 1 5 0 1 1 0 1 x t n 100 0 1 1 0 1 figure 1 6 plots of the solutions obtained by minimizing the sum of squares error function using the m 9 polynomial for n 1 5 data points left plot and n 100 data points right plot we see that increasing the size of the data set reduces the over Ô¨Åtting problem',\n",
       " 'example polynomial curve fitting 9 x t n 1 5 0 1 1 0 1 x t n 100 0 1 1 0 1 figure 1 6 plots of the solutions obtained by minimizing the sum of squares error function using the m 9 polynomial for n 1 5 data points left plot and n 100 data points right plot we see that increasing the size of the data set reduces the over Ô¨Åtting problem ing polynomial function matches each of the data points exactly but between data points particularly near the ends of the range the function exhibits the large oscilla tions observed in figure 1 4 intuitively what is happening is that the more Ô¨Çexible polynomials with larger values ofm are becoming increasingly tuned to the random noise on the target values',\n",
       " 'ing polynomial function matches each of the data points exactly but between data points particularly near the ends of the range the function exhibits the large oscilla tions observed in figure 1 4 intuitively what is happening is that the more Ô¨Çexible polynomials with larger values ofm are becoming increasingly tuned to the random noise on the target values it is also interesting to examine the behaviour of a given model as the size of the data set is varied as shown in figure 1 6 we see that for a given model complexity the over Ô¨Åtting problem become less severe as the size of the data set increases another way to say this is that the larger the data set the more complex in other words more Ô¨Çexible the model that we can afford to Ô¨Åt to the data',\n",
       " 'we see that for a given model complexity the over Ô¨Åtting problem become less severe as the size of the data set increases another way to say this is that the larger the data set the more complex in other words more Ô¨Çexible the model that we can afford to Ô¨Åt to the data one rough heuristic that is sometimes advocated is that the number of data points should be no less than some multiple say 5 or 10 of the number of adaptive parameters in the model however as we shall see in chapter 3 the number of parameters is not necessarily the most appropriate measure of model complexity also there is something rather unsatisfying about having to limit the number of parameters in a model according to the size of the available training set',\n",
       " 'however as we shall see in chapter 3 the number of parameters is not necessarily the most appropriate measure of model complexity also there is something rather unsatisfying about having to limit the number of parameters in a model according to the size of the available training set it would seem more reasonable to choose the complexity of the model according to the com plexity of the problem being solved we shall see that the least squares approach to Ô¨Ånding the model parameters represents a speciÔ¨Åc case ofmaximum likelihood discussed in section 1 2 5 and that the over Ô¨Åtting problem can be understood as a general property of maximum likelihood by adopting abayesian approach thesection 3 4 over Ô¨Åtting problem can be avoided',\n",
       " 'we shall see that the least squares approach to Ô¨Ånding the model parameters represents a speciÔ¨Åc case ofmaximum likelihood discussed in section 1 2 5 and that the over Ô¨Åtting problem can be understood as a general property of maximum likelihood by adopting abayesian approach thesection 3 4 over Ô¨Åtting problem can be avoided we shall see that there is no difÔ¨Åculty from a bayesian perspective in employing models for which the number of parameters greatly exceeds the number of data points indeed in a bayesian model theeffective number of parameters adapts automatically to the size of the data set for the moment however it is instructive to continue with the current approach and to consider how in practice we can apply it to data sets of limited size where we 10 1',\n",
       " 'indeed in a bayesian model theeffective number of parameters adapts automatically to the size of the data set for the moment however it is instructive to continue with the current approach and to consider how in practice we can apply it to data sets of limited size where we 10 1 introduction x t lnŒª 18 0 1 1 0 1 x t lnŒª 0 0 1 1 0 1 figure 1 7 plots of m 9 polynomials Ô¨Åtted to the data set shown in figure 1 2 using the regularized error function 1 4 for two values of the regularization parameter Œª corresponding to ln Œª 18 and ln Œª 0 the case of no regularizer i e Œª 0 corresponding to ln Œª is shown at the bottom right of figure 1 4 may wish to use relatively complex and Ô¨Çexible models',\n",
       " 'introduction x t lnŒª 18 0 1 1 0 1 x t lnŒª 0 0 1 1 0 1 figure 1 7 plots of m 9 polynomials Ô¨Åtted to the data set shown in figure 1 2 using the regularized error function 1 4 for two values of the regularization parameter Œª corresponding to ln Œª 18 and ln Œª 0 the case of no regularizer i e Œª 0 corresponding to ln Œª is shown at the bottom right of figure 1 4 may wish to use relatively complex and Ô¨Çexible models one technique that is often used to control the over Ô¨Åtting phenomenon in such cases is that ofregularization which involves adding a penalty term to the error function 1 2 in order to discourage the coefÔ¨Åcients from reaching large values',\n",
       " 'the case of no regularizer i e Œª 0 corresponding to ln Œª is shown at the bottom right of figure 1 4 may wish to use relatively complex and Ô¨Çexible models one technique that is often used to control the over Ô¨Åtting phenomenon in such cases is that ofregularization which involves adding a penalty term to the error function 1 2 in order to discourage the coefÔ¨Åcients from reaching large values the simplest such penalty term takes the form of a sum of squares of all of the coefÔ¨Åcients leading to a modiÔ¨Åed error function of the form e w 1 2 n n 1 y xn w tn 2 Œª 2 w 2 1 4 where w 2 wtw w2 0 w2 1 w2 m and the coefÔ¨ÅcientŒª governs the rel ative importance of the regularization term compared with the sum of squares error term',\n",
       " 'one technique that is often used to control the over Ô¨Åtting phenomenon in such cases is that ofregularization which involves adding a penalty term to the error function 1 2 in order to discourage the coefÔ¨Åcients from reaching large values the simplest such penalty term takes the form of a sum of squares of all of the coefÔ¨Åcients leading to a modiÔ¨Åed error function of the form e w 1 2 n n 1 y xn w tn 2 Œª 2 w 2 1 4 where w 2 wtw w2 0 w2 1 w2 m and the coefÔ¨ÅcientŒª governs the rel ative importance of the regularization term compared with the sum of squares error term note that often the coefÔ¨Åcientw0 is omitted from the regularizer because its inclusion causes the results to depend on the choice of origin for the target variable hastie et al 2001 or it may be included but with its own regularization coefÔ¨Åcient we shall discuss this topic in more detail in section 5 5 1',\n",
       " 'the simplest such penalty term takes the form of a sum of squares of all of the coefÔ¨Åcients leading to a modiÔ¨Åed error function of the form e w 1 2 n n 1 y xn w tn 2 Œª 2 w 2 1 4 where w 2 wtw w2 0 w2 1 w2 m and the coefÔ¨ÅcientŒª governs the rel ative importance of the regularization term compared with the sum of squares error term note that often the coefÔ¨Åcientw0 is omitted from the regularizer because its inclusion causes the results to depend on the choice of origin for the target variable hastie et al 2001 or it may be included but with its own regularization coefÔ¨Åcient we shall discuss this topic in more detail in section 5 5 1 again the error function in 1 4 can be minimized exactly in closed form',\n",
       " 'note that often the coefÔ¨Åcientw0 is omitted from the regularizer because its inclusion causes the results to depend on the choice of origin for the target variable hastie et al 2001 or it may be included but with its own regularization coefÔ¨Åcient we shall discuss this topic in more detail in section 5 5 1 again the error function in 1 4 can be minimized exactly in closed form techniques such as this are knownexercise 1 2 in the statistics literature asshrinkage methods because they reduce the value of the coefÔ¨Åcients the particular case of a quadratic regularizer is called ridge regres sion hoerl and kennard 1970 in the context of neural networks this approach is known asweight decay',\n",
       " 'the particular case of a quadratic regularizer is called ridge regres sion hoerl and kennard 1970 in the context of neural networks this approach is known asweight decay figure 1 7 shows the results of Ô¨Åtting the polynomial of orderm 9 to the same data set as before but now using the regularized error function given by 1 4 we see that for a value oflnŒª 18 the over Ô¨Åtting has been suppressed and we now obtain a much closer representation of the underlying functionsin 2œÄx if however we use too large a value forŒª then we again obtain a poor Ô¨Åt as shown in figure 1 7 forlnŒª 0 the corresponding coefÔ¨Åcients from the Ô¨Åtted polynomials are given in table 1 2 showing that regularization has the desired effect of reducing 1 1',\n",
       " 'if however we use too large a value forŒª then we again obtain a poor Ô¨Åt as shown in figure 1 7 forlnŒª 0 the corresponding coefÔ¨Åcients from the Ô¨Åtted polynomials are given in table 1 2 showing that regularization has the desired effect of reducing 1 1 example polynomial curve fitting 11 table 1 2 table of the coefÔ¨Åcients w for m 9 polynomials with various values for the regularization parameterŒª note that ln Œª corresponds to a model with no regularization i e to the graph at the bottom right in fig ure 1 4 we see that as the value of Œª increases the typical magnitude of the coefÔ¨Åcients gets smaller',\n",
       " 'note that ln Œª corresponds to a model with no regularization i e to the graph at the bottom right in fig ure 1 4 we see that as the value of Œª increases the typical magnitude of the coefÔ¨Åcients gets smaller ln Œª lnŒª 18 ln Œª 0 w 0 0 35 0 35 0 13 w 1 232 37 4 74 0 05 w 2 5321 83 0 77 0 06 w 3 48568 31 31 97 0 05 w 4 231639 30 3 89 0 03 w 5 640042 26 55 28 0 02 w 6 1061800 52 41 32 0 01 w 7 1042400 18 45 95 0 00 w 8 557682 99 91 53 0 00 w 9 125201 43 72 68 0 01 the magnitude of the coefÔ¨Åcients the impact of the regularization term on the generalization error can be seen by plotting the value of the rms error 1 3 for both training and test sets againstlnŒª as shown in figure 1 8',\n",
       " 'ln Œª lnŒª 18 ln Œª 0 w 0 0 35 0 35 0 13 w 1 232 37 4 74 0 05 w 2 5321 83 0 77 0 06 w 3 48568 31 31 97 0 05 w 4 231639 30 3 89 0 03 w 5 640042 26 55 28 0 02 w 6 1061800 52 41 32 0 01 w 7 1042400 18 45 95 0 00 w 8 557682 99 91 53 0 00 w 9 125201 43 72 68 0 01 the magnitude of the coefÔ¨Åcients the impact of the regularization term on the generalization error can be seen by plotting the value of the rms error 1 3 for both training and test sets againstlnŒª as shown in figure 1 8 we see that in effectŒª now controls the effective complexity of the model and hence determines the degree of over Ô¨Åtting the issue of model complexity is an important one and will be discussed at length in section 1 3',\n",
       " 'we see that in effectŒª now controls the effective complexity of the model and hence determines the degree of over Ô¨Åtting the issue of model complexity is an important one and will be discussed at length in section 1 3 here we simply note that if we were trying to solve a practical application using this approach of minimizing an error function we would have to Ô¨Ånd a way to determine a suitable value for the model complexity the results above suggest a simple way of achieving this namely by taking the available data and partitioning it into a training set used to determine the coefÔ¨Åcientsw and a separate validation set also called a hold out set used to optimize the model complexity either m or Œª',\n",
       " 'here we simply note that if we were trying to solve a practical application using this approach of minimizing an error function we would have to Ô¨Ånd a way to determine a suitable value for the model complexity the results above suggest a simple way of achieving this namely by taking the available data and partitioning it into a training set used to determine the coefÔ¨Åcientsw and a separate validation set also called a hold out set used to optimize the model complexity either m or Œª in many cases however this will prove to be too wasteful of valuable training data and we have to seek more sophisticated approaches section 1 3 so far our discussion of polynomial curve Ô¨Åtting has appealed largely to in tuition',\n",
       " 'the results above suggest a simple way of achieving this namely by taking the available data and partitioning it into a training set used to determine the coefÔ¨Åcientsw and a separate validation set also called a hold out set used to optimize the model complexity either m or Œª in many cases however this will prove to be too wasteful of valuable training data and we have to seek more sophisticated approaches section 1 3 so far our discussion of polynomial curve Ô¨Åtting has appealed largely to in tuition we now seek a more principled approach to solving problems in pattern recognition by turning to a discussion of probability theory',\n",
       " 'in many cases however this will prove to be too wasteful of valuable training data and we have to seek more sophisticated approaches section 1 3 so far our discussion of polynomial curve Ô¨Åtting has appealed largely to in tuition we now seek a more principled approach to solving problems in pattern recognition by turning to a discussion of probability theory as well as providing the foundation for nearly all of the subsequent developments in this book it will also figure 1 8 graph of the root mean square er ror 1 3 versus ln Œª for the m 9 polynomial erms lnŒª 35 30 25 20 0 0 5 1 training test 12 1',\n",
       " 'as well as providing the foundation for nearly all of the subsequent developments in this book it will also figure 1 8 graph of the root mean square er ror 1 3 versus ln Œª for the m 9 polynomial erms lnŒª 35 30 25 20 0 0 5 1 training test 12 1 introduction give us some important insights into the concepts we have introduced in the con text of polynomial curve Ô¨Åtting and will allow us to extend these to more complex situations 1 2 probability theory a key concept in the Ô¨Åeld of pattern recognition is that of uncertainty it arises both through noise on measurements as well as through the Ô¨Ånite size of data sets prob ability theory provides a consistent framework for the quantiÔ¨Åcation and manipula tion of uncertainty and forms one of the central foundations for pattern recognition',\n",
       " 'it arises both through noise on measurements as well as through the Ô¨Ånite size of data sets prob ability theory provides a consistent framework for the quantiÔ¨Åcation and manipula tion of uncertainty and forms one of the central foundations for pattern recognition when combined with decision theory discussed in section 1 5 it allows us to make optimal predictions given all the information available to us even though that infor mation may be incomplete or ambiguous we will introduce the basic concepts of probability theory by considering a sim ple example imagine we have two boxes one red and one blue and in the red box we have 2 apples and 6 oranges and in the blue box we have 3 apples and 1 orange this is illustrated in figure 1 9',\n",
       " 'imagine we have two boxes one red and one blue and in the red box we have 2 apples and 6 oranges and in the blue box we have 3 apples and 1 orange this is illustrated in figure 1 9 now suppose we randomly pick one of the boxes and from that box we randomly select an item of fruit and having observed which sort of fruit it is we replace it in the box from which it came we could imagine repeating this process many times let us suppose that in so doing we pick the red box 40 of the time and we pick the blue box 60 of the time and that when we remove an item of fruit from a box we are equally likely to select any of the pieces of fruit in the box in this example the identity of the box that will be chosen is a random variable which we shall denote by b',\n",
       " 'let us suppose that in so doing we pick the red box 40 of the time and we pick the blue box 60 of the time and that when we remove an item of fruit from a box we are equally likely to select any of the pieces of fruit in the box in this example the identity of the box that will be chosen is a random variable which we shall denote by b this random variable can take one of two possible values namely r corresponding to the red box or b corresponding to the blue box similarly the identity of the fruit is also a random variable and will be denoted by f it can take either of the valuesa for apple oro for orange',\n",
       " 'this random variable can take one of two possible values namely r corresponding to the red box or b corresponding to the blue box similarly the identity of the fruit is also a random variable and will be denoted by f it can take either of the valuesa for apple oro for orange to begin with we shall deÔ¨Åne the probability of an event to be the fraction of times that event occurs out of the total number of trials in the limit that the total number of trials goes to inÔ¨Ånity thus the probability of selecting the red box is4 10 figure 1 9 we use a simple example of two coloured boxes each containing fruit apples shown in green and or anges shown in orange to intro duce the basic ideas of probability 1 2',\n",
       " 'thus the probability of selecting the red box is4 10 figure 1 9 we use a simple example of two coloured boxes each containing fruit apples shown in green and or anges shown in orange to intro duce the basic ideas of probability 1 2 probability theory 13 figure 1 10 we can derive the sum and product rules of probability by considering two random variables x which takes the values xi where i 1 m and y which takes the values yj where j 1 l in this illustration we have m 5 and l 3 if we consider a total number n of instances of these variables then we denote the number of instances where x xi and y yj by nij which is the number of points in the corresponding cell of the array',\n",
       " 'in this illustration we have m 5 and l 3 if we consider a total number n of instances of these variables then we denote the number of instances where x xi and y yj by nij which is the number of points in the corresponding cell of the array the number of points in column i corresponding to x xi is denoted by ci and the number of points in row j corresponding to y yj is denoted by rj ci rjyj xi nij and the probability of selecting the blue box is6 10 we write these probabilities as p b r 4 10 and p b b 6 10 note that by deÔ¨Ånition probabilities must lie in the interval 0 1',\n",
       " 'we write these probabilities as p b r 4 10 and p b b 6 10 note that by deÔ¨Ånition probabilities must lie in the interval 0 1 also if the events are mutually exclusive and if they include all possible outcomes for instance in this example the box must be either red or blue then we see that the probabilities for those events must sum to one we can now ask questions such as what is the overall probability that the se lection procedure will pick an apple or given that we have chosen an orange what is the probability that the box we chose was the blue one',\n",
       " 'also if the events are mutually exclusive and if they include all possible outcomes for instance in this example the box must be either red or blue then we see that the probabilities for those events must sum to one we can now ask questions such as what is the overall probability that the se lection procedure will pick an apple or given that we have chosen an orange what is the probability that the box we chose was the blue one we can answer questions such as these and indeed much more complex questions associated with problems in pattern recognition once we have equipped ourselves with the two el ementary rules of probability known as thesum rule and the product rule having obtained these rules we shall then return to our boxes of fruit example',\n",
       " 'we can answer questions such as these and indeed much more complex questions associated with problems in pattern recognition once we have equipped ourselves with the two el ementary rules of probability known as thesum rule and the product rule having obtained these rules we shall then return to our boxes of fruit example in order to derive the rules of probability consider the slightly more general ex ample shown in figure 1 10 involving two random variablesx and y which could for instance be the box and fruit variables considered above we shall suppose that x can take any of the valuesxi where i 1 m and y can take the valuesyj where j 1 l',\n",
       " 'in order to derive the rules of probability consider the slightly more general ex ample shown in figure 1 10 involving two random variablesx and y which could for instance be the box and fruit variables considered above we shall suppose that x can take any of the valuesxi where i 1 m and y can take the valuesyj where j 1 l consider a total of n trials in which we sample both of the variables x and y and let the number of such trials in whichx xi and y yj be nij also let the number of trials in which x takes the value xi irrespective of the value thaty takes be denoted byci and similarly let the number of trials in which y takes the valueyj be denoted byrj',\n",
       " 'consider a total of n trials in which we sample both of the variables x and y and let the number of such trials in whichx xi and y yj be nij also let the number of trials in which x takes the value xi irrespective of the value thaty takes be denoted byci and similarly let the number of trials in which y takes the valueyj be denoted byrj the probability that x will take the value xi and y will take the value yj is written p x xi y yj and is called the joint probability of x xi and y yj it is given by the number of points falling in the celli j as a fraction of the total number of points and hence p x xi y yj nij n 1 5 here we are implicitly considering the limitn',\n",
       " 'it is given by the number of points falling in the celli j as a fraction of the total number of points and hence p x xi y yj nij n 1 5 here we are implicitly considering the limitn similarly the probability that x takes the valuexi irrespective of the value ofy is written as p x xi and is given by the fraction of the total number of points that fall in columni so that p x xi ci n 1 6 because the number of instances in columni in figure 1 10 is just the sum of the number of instances in each cell of that column we haveci j nij and therefore 14 1 introduction from 1 5 and 1 6 we have p x xi l j 1 p x xi y yj 1 7 which is thesum rule of probability',\n",
       " '1 6 because the number of instances in columni in figure 1 10 is just the sum of the number of instances in each cell of that column we haveci j nij and therefore 14 1 introduction from 1 5 and 1 6 we have p x xi l j 1 p x xi y yj 1 7 which is thesum rule of probability note thatp x xi is sometimes called the marginal probability because it is obtained by marginalizing or summing out the other variables in this casey if we consider only those instances for which x xi then the fraction of such instances for which y yj is written p y yj x xi and is called the conditional probability of y yj given x xi it is obtained by Ô¨Ånding the fraction of those points in columni that fall in celli j and hence is given by p y yj x xi nij ci',\n",
       " 'if we consider only those instances for which x xi then the fraction of such instances for which y yj is written p y yj x xi and is called the conditional probability of y yj given x xi it is obtained by Ô¨Ånding the fraction of those points in columni that fall in celli j and hence is given by p y yj x xi nij ci 1 8 from 1 5 1 6 and 1 8 we can then derive the following relationship p x xi y yj nij n nij ci ci n p y yj x xi p x xi 1 9 which is theproduct ruleof probability so far we have been quite careful to make a distinction between a random vari able such as the boxb in the fruit example and the values that the random variable can take for exampler if the box were the red one',\n",
       " '1 8 from 1 5 1 6 and 1 8 we can then derive the following relationship p x xi y yj nij n nij ci ci n p y yj x xi p x xi 1 9 which is theproduct ruleof probability so far we have been quite careful to make a distinction between a random vari able such as the boxb in the fruit example and the values that the random variable can take for exampler if the box were the red one thus the probability thatb takes the value r is denoted p b r although this helps to avoid ambiguity it leads to a rather cumbersome notation and in many cases there will be no need for such pedantry',\n",
       " 'thus the probability thatb takes the value r is denoted p b r although this helps to avoid ambiguity it leads to a rather cumbersome notation and in many cases there will be no need for such pedantry instead we may simply writep b to denote a distribution over the ran dom variable b o r p r to denote the distribution evaluated for the particular value r provided that the interpretation is clear from the context with this more compact notation we can write the two fundamental rules of probability theory in the following form the rules of probability sum rule p x y p x y 1 10 product rule p x y p y x p x 1 11 here p x y is a joint probability and is verbalized as the probability ofx and y',\n",
       " 'the rules of probability sum rule p x y p x y 1 10 product rule p x y p y x p x 1 11 here p x y is a joint probability and is verbalized as the probability ofx and y similarly the quantityp y x is a conditional probability and is verbalized as the probability ofy given x whereas the quantityp x is a marginal probability 1 2 probability theory 15 and is simply the probability ofx these two simple rules form the basis for all of the probabilistic machinery that we use throughout this book',\n",
       " 'probability theory 15 and is simply the probability ofx these two simple rules form the basis for all of the probabilistic machinery that we use throughout this book from the product rule together with the symmetry propertyp x y p y x we immediately obtain the following relationship between conditional probabilities p y x p x y p y p x 1 12 which is calledbayes theoremand which plays a central role in pattern recognition and machine learning using the sum rule the denominator in bayes theorem can be expressed in terms of the quantities appearing in the numerator p x y p x y p y',\n",
       " 'from the product rule together with the symmetry propertyp x y p y x we immediately obtain the following relationship between conditional probabilities p y x p x y p y p x 1 12 which is calledbayes theoremand which plays a central role in pattern recognition and machine learning using the sum rule the denominator in bayes theorem can be expressed in terms of the quantities appearing in the numerator p x y p x y p y 1 13 we can view the denominator in bayes theorem as being the normalization constant required to ensure that the sum of the conditional probability on the left hand side of 1 12 over all values ofy equals one',\n",
       " 'using the sum rule the denominator in bayes theorem can be expressed in terms of the quantities appearing in the numerator p x y p x y p y 1 13 we can view the denominator in bayes theorem as being the normalization constant required to ensure that the sum of the conditional probability on the left hand side of 1 12 over all values ofy equals one in figure 1 11 we show a simple example involving a joint distribution over two variables to illustrate the concept of marginal and conditional distributions here a Ô¨Ånite sample of n 6 0 data points has been drawn from the joint distribution and is shown in the top left in the top right is a histogram of the fractions of data points having each of the two values ofy',\n",
       " 'here a Ô¨Ånite sample of n 6 0 data points has been drawn from the joint distribution and is shown in the top left in the top right is a histogram of the fractions of data points having each of the two values ofy from the deÔ¨Ånition of probability these fractions would equal the corresponding probabilitiesp y in the limitn w e can view the histogram as a simple way to model a probability distribution given only a Ô¨Ånite number of points drawn from that distribution modelling distributions from data lies at the heart of statistical pattern recognition and will be explored in great detail in this book the remaining two plots in figure 1 11 show the corresponding histogram estimates ofp x and p x y 1 let us now return to our example involving boxes of fruit',\n",
       " 'the remaining two plots in figure 1 11 show the corresponding histogram estimates ofp x and p x y 1 let us now return to our example involving boxes of fruit for the moment we shall once again be explicit about distinguishing between the random variables and their instantiations we have seen that the probabilities of selecting either the red or the blue boxes are given by p b r 4 10 1 14 p b b 6 10 1 15 respectively note that these satisfyp b r p b b 1 now suppose that we pick a box at random and it turns out to be the blue box then the probability of selecting an apple is just the fraction of apples in the blue box which is3 4 and sop f a b b 3 4',\n",
       " 'now suppose that we pick a box at random and it turns out to be the blue box then the probability of selecting an apple is just the fraction of apples in the blue box which is3 4 and sop f a b b 3 4 in fact we can write out all four conditional probabilities for the type of fruit given the selected box p f a b r 1 4 1 16 p f o b r 3 4 1 17 p f a b b 3 4 1 18 p f o b b 1 4 1 19 16 1 introduction p x y x y 2 y 1 p y p x x x p x y 1 figure 1 11 an illustration of a distribution over two variables x which takes 9 possible values and y which takes two possible values the top left Ô¨Ågure shows a sample of 60 points drawn from a joint probability distri bution over these variables',\n",
       " 'introduction p x y x y 2 y 1 p y p x x x p x y 1 figure 1 11 an illustration of a distribution over two variables x which takes 9 possible values and y which takes two possible values the top left Ô¨Ågure shows a sample of 60 points drawn from a joint probability distri bution over these variables the remaining Ô¨Ågures show histogram estimates of the marginal distributions p x and p y as well as the conditional distribution p x y 1 corresponding to the bottom row in the top left Ô¨Ågure again note that these probabilities are normalized so that p f a b r p f o b r 1 1 20 and similarly p f a b b p f o b b 1',\n",
       " 'the remaining Ô¨Ågures show histogram estimates of the marginal distributions p x and p y as well as the conditional distribution p x y 1 corresponding to the bottom row in the top left Ô¨Ågure again note that these probabilities are normalized so that p f a b r p f o b r 1 1 20 and similarly p f a b b p f o b b 1 1 21 we can now use the sum and product rules of probability to evaluate the overall probability of choosing an apple p f a p f a b r p b r p f a b b p b b 1 4 4 10 3 4 6 10 11 20 1 22 from which it follows using the sum rule thatp f o 1 11 20 9 20 1 2 probability theory 17 suppose instead we are told that a piece of fruit has been selected and it is an orange and we would like to know which box it came from',\n",
       " '1 2 probability theory 17 suppose instead we are told that a piece of fruit has been selected and it is an orange and we would like to know which box it came from this requires that we evaluate the probability distribution over boxes conditioned on the identity of the fruit whereas the probabilities in 1 16 1 19 give the probability distribution over the fruit conditioned on the identity of the box we can solve the problem of reversing the conditional probability by using bayes theorem to give p b r f o p f o b r p b r p f o 3 4 4 10 20 9 2 3 1 23 from the sum rule it then follows thatp b b f o 1 2 3 1 3 we can provide an important interpretation of bayes theorem as follows',\n",
       " '1 23 from the sum rule it then follows thatp b b f o 1 2 3 1 3 we can provide an important interpretation of bayes theorem as follows if we had been asked which box had been chosen before being told the identity of the selected item of fruit then the most complete information we have available is provided by the probabilityp b we call this theprior probabilitybecause it is the probability availablebefore we observe the identity of the fruit once we are told that the fruit is an orange we can then use bayes theorem to compute the probability p b f which we shall call theposterior probability because it is the probability obtained after we have observedf',\n",
       " 'we call this theprior probabilitybecause it is the probability availablebefore we observe the identity of the fruit once we are told that the fruit is an orange we can then use bayes theorem to compute the probability p b f which we shall call theposterior probability because it is the probability obtained after we have observedf note that in this example the prior probability of selecting the red box was4 10 so that we were more likely to select the blue box than the red one however once we have observed that the piece of selected fruit is an orange we Ô¨Ånd that the posterior probability of the red box is now2 3 so that it is now more likely that the box we selected was in fact the red one',\n",
       " 'note that in this example the prior probability of selecting the red box was4 10 so that we were more likely to select the blue box than the red one however once we have observed that the piece of selected fruit is an orange we Ô¨Ånd that the posterior probability of the red box is now2 3 so that it is now more likely that the box we selected was in fact the red one this result accords with our intuition as the proportion of oranges is much higher in the red box than it is in the blue box and so the observation that the fruit was an orange provides signiÔ¨Åcant evidence favouring the red box in fact the evidence is sufÔ¨Åciently strong that it outweighs the prior and makes it more likely that the red box was chosen rather than the blue one',\n",
       " 'this result accords with our intuition as the proportion of oranges is much higher in the red box than it is in the blue box and so the observation that the fruit was an orange provides signiÔ¨Åcant evidence favouring the red box in fact the evidence is sufÔ¨Åciently strong that it outweighs the prior and makes it more likely that the red box was chosen rather than the blue one finally we note that if the joint distribution of two variables factorizes into the product of the marginals so thatp x y p x p y then x and y are said to be independent',\n",
       " 'in fact the evidence is sufÔ¨Åciently strong that it outweighs the prior and makes it more likely that the red box was chosen rather than the blue one finally we note that if the joint distribution of two variables factorizes into the product of the marginals so thatp x y p x p y then x and y are said to be independent from the product rule we see that p y x p y and so the conditional distribution ofy given x is indeed independent of the value ofx f o r instance in our boxes of fruit example if each box contained the same fraction of apples and oranges thenp f b p f so that the probability of selecting say an apple is independent of which box is chosen',\n",
       " 'finally we note that if the joint distribution of two variables factorizes into the product of the marginals so thatp x y p x p y then x and y are said to be independent from the product rule we see that p y x p y and so the conditional distribution ofy given x is indeed independent of the value ofx f o r instance in our boxes of fruit example if each box contained the same fraction of apples and oranges thenp f b p f so that the probability of selecting say an apple is independent of which box is chosen 1 2 1 probability densities as well as considering probabilities deÔ¨Åned over discrete sets of events we also wish to consider probabilities with respect to continuous variables we shall limit ourselves to a relatively informal discussion',\n",
       " '1 2 1 probability densities as well as considering probabilities deÔ¨Åned over discrete sets of events we also wish to consider probabilities with respect to continuous variables we shall limit ourselves to a relatively informal discussion if the probability of a real valued variable x falling in the interval x x Œ¥x is given by p x Œ¥x for Œ¥x 0 then p x is called theprobability density over x this is illustrated in figure 1 12 the probability that x will lie in an interval a b is then given by p x a b b a p x d x 1 24 18 1',\n",
       " 'the probability that x will lie in an interval a b is then given by p x a b b a p x d x 1 24 18 1 introduction figure 1 12 the concept of probability for discrete variables can be ex tended to that of a probability density p x over a continuous variable x and is such that the probability of x lying in the inter val x x Œ¥x is given by p x Œ¥x for Œ¥x 0 the probability density can be expressed as the derivative of a cumulative distri bution function p x xŒ¥x p x p x because probabilities are nonnegative and because the value ofx must lie some where on the real axis the probability densityp x must satisfy the two conditions p x 0 1 25 p x d x 1',\n",
       " 'the probability density can be expressed as the derivative of a cumulative distri bution function p x xŒ¥x p x p x because probabilities are nonnegative and because the value ofx must lie some where on the real axis the probability densityp x must satisfy the two conditions p x 0 1 25 p x d x 1 1 26 under a nonlinear change of variable a probability density transforms differently from a simple function due to the jacobian factor for instance if we consider a change of variables x g y then a function f x becomes f y f g y now consider a probability densitypx x that corresponds to a densitypy y with respect to the new variabley where the sufÔ¨Åces denote the fact thatpx x and py y are different densities',\n",
       " 'for instance if we consider a change of variables x g y then a function f x becomes f y f g y now consider a probability densitypx x that corresponds to a densitypy y with respect to the new variabley where the sufÔ¨Åces denote the fact thatpx x and py y are different densities observations falling in the range x x Œ¥x will for small values of Œ¥x be transformed into the range y y Œ¥y where px x Œ¥x py y Œ¥y and hence py y px x dx dy px g y g y',\n",
       " 'now consider a probability densitypx x that corresponds to a densitypy y with respect to the new variabley where the sufÔ¨Åces denote the fact thatpx x and py y are different densities observations falling in the range x x Œ¥x will for small values of Œ¥x be transformed into the range y y Œ¥y where px x Œ¥x py y Œ¥y and hence py y px x dx dy px g y g y 1 27 one consequence of this property is that the concept of the maximum of a probability density is dependent on the choice of variable exercise 1 4 the probability that x lies in the interval z is given by the cumulative distribution functiondeÔ¨Åned by p z z p x d x 1 28 which satisÔ¨Åes p x p x as shown in figure 1 12',\n",
       " 'observations falling in the range x x Œ¥x will for small values of Œ¥x be transformed into the range y y Œ¥y where px x Œ¥x py y Œ¥y and hence py y px x dx dy px g y g y 1 27 one consequence of this property is that the concept of the maximum of a probability density is dependent on the choice of variable exercise 1 4 the probability that x lies in the interval z is given by the cumulative distribution functiondeÔ¨Åned by p z z p x d x 1 28 which satisÔ¨Åes p x p x as shown in figure 1 12 if we have several continuous variablesx1 x d denoted collectively by the vector x then we can deÔ¨Åne a joint probability densityp x p x1 x d such 1 2',\n",
       " '1 27 one consequence of this property is that the concept of the maximum of a probability density is dependent on the choice of variable exercise 1 4 the probability that x lies in the interval z is given by the cumulative distribution functiondeÔ¨Åned by p z z p x d x 1 28 which satisÔ¨Åes p x p x as shown in figure 1 12 if we have several continuous variablesx1 x d denoted collectively by the vector x then we can deÔ¨Åne a joint probability densityp x p x1 x d such 1 2 probability theory 19 that the probability ofx falling in an inÔ¨Ånitesimal volumeŒ¥x containing the pointx is given byp x Œ¥x this multivariate probability density must satisfy p x 0 1 29 p x dx 1 1 30 in which the integral is taken over the whole ofx space',\n",
       " 'probability theory 19 that the probability ofx falling in an inÔ¨Ånitesimal volumeŒ¥x containing the pointx is given byp x Œ¥x this multivariate probability density must satisfy p x 0 1 29 p x dx 1 1 30 in which the integral is taken over the whole ofx space we can also consider joint probability distributions over a combination of discrete and continuous variables note that if x is a discrete variable thenp x is sometimes called aprobability mass functionbecause it can be regarded as a set of probability masses concentrated at the allowed values ofx the sum and product rules of probability as well as bayes theorem apply equally to the case of probability densities or to combinations of discrete and con tinuous variables',\n",
       " 'note that if x is a discrete variable thenp x is sometimes called aprobability mass functionbecause it can be regarded as a set of probability masses concentrated at the allowed values ofx the sum and product rules of probability as well as bayes theorem apply equally to the case of probability densities or to combinations of discrete and con tinuous variables for instance ifx and y are two real variables then the sum and product rules take the form p x p x y d y 1 31 p x y p y x p x 1 32 a formal justiÔ¨Åcation of the sum and product rules for continuous variables feller 1966 requires a branch of mathematics calledmeasure theory and lies outside the scope of this book',\n",
       " 'for instance ifx and y are two real variables then the sum and product rules take the form p x p x y d y 1 31 p x y p y x p x 1 32 a formal justiÔ¨Åcation of the sum and product rules for continuous variables feller 1966 requires a branch of mathematics calledmeasure theory and lies outside the scope of this book its validity can be seen informally however by dividing each real variable into intervals of width and considering the discrete probability dis tribution over these intervals taking the limit 0 then turns sums into integrals and gives the desired result 1 2 2 expectations and covariances one of the most important operations involving probabilities is that of Ô¨Ånding weighted averages of functions',\n",
       " 'taking the limit 0 then turns sums into integrals and gives the desired result 1 2 2 expectations and covariances one of the most important operations involving probabilities is that of Ô¨Ånding weighted averages of functions the average value of some functionf x under a probability distributionp x is called theexpectation of f x and will be denoted by e f for a discrete distribution it is given by e f x p x f x 1 33 so that the average is weighted by the relative probabilities of the different values of x in the case of continuous variables expectations are expressed in terms of an integration with respect to the corresponding probability density e f p x f x d x',\n",
       " 'for a discrete distribution it is given by e f x p x f x 1 33 so that the average is weighted by the relative probabilities of the different values of x in the case of continuous variables expectations are expressed in terms of an integration with respect to the corresponding probability density e f p x f x d x 1 34 in either case if we are given a Ô¨Ånite numbern of points drawn from the probability distribution or probability density then the expectation can be approximated as a 20 1 introduction Ô¨Ånite sum over these points e f 1 n n n 1 f xn 1 35 we shall make extensive use of this result when we discuss sampling methods in chapter 11 the approximation in 1 35 becomes exact in the limitn',\n",
       " '1 35 we shall make extensive use of this result when we discuss sampling methods in chapter 11 the approximation in 1 35 becomes exact in the limitn sometimes we will be considering expectations of functions of several variables in which case we can use a subscript to indicate which variable is being averaged over so that for instance ex f x y 1 36 denotes the average of the functionf x y with respect to the distribution ofx note that ex f x y will be a function ofy we can also consider a conditional expectation with respect to a conditional distribution so that ex f y x p x y f x 1 37 with an analogous deÔ¨Ånition for continuous variables',\n",
       " 'note that ex f x y will be a function ofy we can also consider a conditional expectation with respect to a conditional distribution so that ex f y x p x y f x 1 37 with an analogous deÔ¨Ånition for continuous variables the variance of f x is deÔ¨Åned by var f e f x e f x 2 1 38 and provides a measure of how much variability there is inf x around its mean value e f x expanding out the square we see that the variance can also be written in terms of the expectations off x and f x 2exercise 1 5 var f e f x 2 e f x 2 1 39 in particular we can consider the variance of the variablex itself which is given by var x e x2 e x 2',\n",
       " 'expanding out the square we see that the variance can also be written in terms of the expectations off x and f x 2exercise 1 5 var f e f x 2 e f x 2 1 39 in particular we can consider the variance of the variablex itself which is given by var x e x2 e x 2 1 40 for two random variablesx and y the covariance is deÔ¨Åned by cov x y ex y x e x y e y ex y xy e x e y 1 41 which expresses the extent to whichx and y vary together if x and y are indepen dent then their covariance vanishes exercise 1 6 in the case of two vectors of random variablesx and y the covariance is a matrix cov x y ex y x e x yt e yt ex y xyt e x e yt',\n",
       " '1 40 for two random variablesx and y the covariance is deÔ¨Åned by cov x y ex y x e x y e y ex y xy e x e y 1 41 which expresses the extent to whichx and y vary together if x and y are indepen dent then their covariance vanishes exercise 1 6 in the case of two vectors of random variablesx and y the covariance is a matrix cov x y ex y x e x yt e yt ex y xyt e x e yt 1 42 if we consider the covariance of the components of a vectorx with each other then we use a slightly simpler notationcov x cov x x 1 2 probability theory 21 1 2 3 bayesian probabilities so far in this chapter we have viewed probabilities in terms of the frequencies of random repeatable events',\n",
       " '1 2 probability theory 21 1 2 3 bayesian probabilities so far in this chapter we have viewed probabilities in terms of the frequencies of random repeatable events we shall refer to this as theclassical or frequentist interpretation of probability now we turn to the more general bayesian view in which probabilities provide a quantiÔ¨Åcation of uncertainty consider an uncertain event for example whether the moon was once in its own orbit around the sun or whether the arctic ice cap will have disappeared by the end of the century these are not events that can be repeated numerous times in order to deÔ¨Åne a notion of probability as we did earlier in the context of boxes of fruit nevertheless we will generally have some idea for example of how quickly we think the polar ice is melting',\n",
       " 'these are not events that can be repeated numerous times in order to deÔ¨Åne a notion of probability as we did earlier in the context of boxes of fruit nevertheless we will generally have some idea for example of how quickly we think the polar ice is melting if we now obtain fresh evidence for instance from a new earth observation satellite gathering novel forms of diagnostic information we may revise our opinion on the rate of ice loss our assessment of such matters will affect the actions we take for instance the extent to which we endeavour to reduce the emission of greenhouse gasses',\n",
       " 'if we now obtain fresh evidence for instance from a new earth observation satellite gathering novel forms of diagnostic information we may revise our opinion on the rate of ice loss our assessment of such matters will affect the actions we take for instance the extent to which we endeavour to reduce the emission of greenhouse gasses in such circumstances we would like to be able to quantify our expression of uncertainty and make precise revisions of uncertainty in the light of new evidence as well as subsequently to be able to take optimal actions or decisions as a consequence this can all be achieved through the elegant and very general bayesian interpretation of probability',\n",
       " 'in such circumstances we would like to be able to quantify our expression of uncertainty and make precise revisions of uncertainty in the light of new evidence as well as subsequently to be able to take optimal actions or decisions as a consequence this can all be achieved through the elegant and very general bayesian interpretation of probability the use of probability to represent uncertainty however is not an ad hoc choice but is inevitable if we are to respect common sense while making rational coherent inferences',\n",
       " 'this can all be achieved through the elegant and very general bayesian interpretation of probability the use of probability to represent uncertainty however is not an ad hoc choice but is inevitable if we are to respect common sense while making rational coherent inferences for instance cox 1946 showed that if numerical values are used to represent degrees of belief then a simple set of axioms encoding common sense properties of such beliefs leads uniquely to a set of rules for manipulating degrees of belief that are equivalent to the sum and product rules of probability this provided the Ô¨Årst rigorous proof that probability theory could be regarded as an extension of boolean logic to situations involving uncertainty jaynes 2003',\n",
       " 'for instance cox 1946 showed that if numerical values are used to represent degrees of belief then a simple set of axioms encoding common sense properties of such beliefs leads uniquely to a set of rules for manipulating degrees of belief that are equivalent to the sum and product rules of probability this provided the Ô¨Årst rigorous proof that probability theory could be regarded as an extension of boolean logic to situations involving uncertainty jaynes 2003 numerous other authors have proposed different sets of properties or axioms that such measures of uncertainty should satisfy ramsey 1931 good 1950 savage 1961 definetti 1970 lindley 1982 in each case the resulting numerical quantities behave pre cisely according to the rules of probability',\n",
       " 'numerous other authors have proposed different sets of properties or axioms that such measures of uncertainty should satisfy ramsey 1931 good 1950 savage 1961 definetti 1970 lindley 1982 in each case the resulting numerical quantities behave pre cisely according to the rules of probability it is therefore natural to refer to these quantities as bayesian probabilities in the Ô¨Åeld of pattern recognition too it is helpful to have a more general no thomas bayes 1701 1761 thomas bayes was born in tun bridge wells and was a clergyman as well as an amateur scientist and a mathematician he studied logic and theology at edinburgh univer sity and was elected fellow of the royal society in 1742',\n",
       " 'in the Ô¨Åeld of pattern recognition too it is helpful to have a more general no thomas bayes 1701 1761 thomas bayes was born in tun bridge wells and was a clergyman as well as an amateur scientist and a mathematician he studied logic and theology at edinburgh univer sity and was elected fellow of the royal society in 1742 during the 18 th century is sues regarding probability arose in connection with gambling and with the new concept of insurance one particularly important problem concerned so called in verse probability a solution was proposed by thomas bayes in his paper essay towards solving a problem in the doctrine of chances which was published in 1764 some three years after his death in the philo sophical transactions of the royal society',\n",
       " 'one particularly important problem concerned so called in verse probability a solution was proposed by thomas bayes in his paper essay towards solving a problem in the doctrine of chances which was published in 1764 some three years after his death in the philo sophical transactions of the royal society in fact bayes only formulated his theory for the case of a uni form prior and it was pierre simon laplace who inde pendently rediscovered the theory in general form and who demonstrated its broad applicability 22 1 introduction tion of probability consider the example of polynomial curve Ô¨Åtting discussed in section 1 1',\n",
       " 'introduction tion of probability consider the example of polynomial curve Ô¨Åtting discussed in section 1 1 it seems reasonable to apply the frequentist notion of probability to the random values of the observed variablest n however we would like to address and quantify the uncertainty that surrounds the appropriate choice for the model param eters w we shall see that from a bayesian perspective we can use the machinery of probability theory to describe the uncertainty in model parameters such asw o r indeed in the choice of model itself bayes theorem now acquires a new signiÔ¨Åcance recall that in the boxes of fruit example the observation of the identity of the fruit provided relevant information that altered the probability that the chosen box was the red one',\n",
       " 'bayes theorem now acquires a new signiÔ¨Åcance recall that in the boxes of fruit example the observation of the identity of the fruit provided relevant information that altered the probability that the chosen box was the red one in that example bayes theorem was used to convert a prior probability into a posterior probability by incorporating the evidence provided by the observed data as we shall see in detail later we can adopt a similar approach when making inferences about quantities such as the parametersw in the polynomial curve Ô¨Åtting example we capture our assumptions about w before observing the data in the form of a prior probability distribution p w',\n",
       " 'as we shall see in detail later we can adopt a similar approach when making inferences about quantities such as the parametersw in the polynomial curve Ô¨Åtting example we capture our assumptions about w before observing the data in the form of a prior probability distribution p w the effect of the observed datad t 1 t n is expressed through the conditional probabilityp d w and we shall see later in section 1 2 5 how this can be represented explicitly bayes theorem which takes the form p w d p d w p w p d 1 43 then allows us to evaluate the uncertainty inw after we have observedd in the form of the posterior probabilityp w d',\n",
       " 'the effect of the observed datad t 1 t n is expressed through the conditional probabilityp d w and we shall see later in section 1 2 5 how this can be represented explicitly bayes theorem which takes the form p w d p d w p w p d 1 43 then allows us to evaluate the uncertainty inw after we have observedd in the form of the posterior probabilityp w d the quantity p d w on the right hand side of bayes theorem is evaluated for the observed data set d and can be viewed as a function of the parameter vector w in which case it is called thelikelihood function it expresses how probable the observed data set is for different settings of the parameter vectorw',\n",
       " 'the quantity p d w on the right hand side of bayes theorem is evaluated for the observed data set d and can be viewed as a function of the parameter vector w in which case it is called thelikelihood function it expresses how probable the observed data set is for different settings of the parameter vectorw note that the likelihood is not a probability distribution overw and its integral with respect tow does not necessarily equal one given this deÔ¨Ånition of likelihood we can state bayes theorem in words posterior likelihood prior 1 44 where all of these quantities are viewed as functions of w the denominator in 1 43 is the normalization constant which ensures that the posterior distribution on the left hand side is a valid probability density and integrates to one',\n",
       " 'note that the likelihood is not a probability distribution overw and its integral with respect tow does not necessarily equal one given this deÔ¨Ånition of likelihood we can state bayes theorem in words posterior likelihood prior 1 44 where all of these quantities are viewed as functions of w the denominator in 1 43 is the normalization constant which ensures that the posterior distribution on the left hand side is a valid probability density and integrates to one indeed integrating both sides of 1 43 with respect tow we can express the denominator in bayes theorem in terms of the prior distribution and the likelihood function p d p d w p w d w 1 45 in both the bayesian and frequentist paradigms the likelihood functionp d w plays a central role',\n",
       " 'given this deÔ¨Ånition of likelihood we can state bayes theorem in words posterior likelihood prior 1 44 where all of these quantities are viewed as functions of w the denominator in 1 43 is the normalization constant which ensures that the posterior distribution on the left hand side is a valid probability density and integrates to one indeed integrating both sides of 1 43 with respect tow we can express the denominator in bayes theorem in terms of the prior distribution and the likelihood function p d p d w p w d w 1 45 in both the bayesian and frequentist paradigms the likelihood functionp d w plays a central role however the manner in which it is used is fundamentally dif ferent in the two approaches',\n",
       " 'indeed integrating both sides of 1 43 with respect tow we can express the denominator in bayes theorem in terms of the prior distribution and the likelihood function p d p d w p w d w 1 45 in both the bayesian and frequentist paradigms the likelihood functionp d w plays a central role however the manner in which it is used is fundamentally dif ferent in the two approaches in a frequentist setting w is considered to be a Ô¨Åxed parameter whose value is determined by some form of estimator and error bars 1 2 probability theory 23 on this estimate are obtained by considering the distribution of possible data setsd',\n",
       " 'in a frequentist setting w is considered to be a Ô¨Åxed parameter whose value is determined by some form of estimator and error bars 1 2 probability theory 23 on this estimate are obtained by considering the distribution of possible data setsd by contrast from the bayesian viewpoint there is only a single data setd namely the one that is actually observed and the uncertainty in the parameters is expressed through a probability distribution overw a widely used frequentist estimator ismaximum likelihood in which w is set to the value that maximizes the likelihood functionp d w this corresponds to choosing the value ofw for which the probability of the observed data set is maxi mized',\n",
       " 'a widely used frequentist estimator ismaximum likelihood in which w is set to the value that maximizes the likelihood functionp d w this corresponds to choosing the value ofw for which the probability of the observed data set is maxi mized in the machine learning literature the negative log of the likelihood function is called an error function because the negative logarithm is a monotonically de creasing function maximizing the likelihood is equivalent to minimizing the error one approach to determining frequentist error bars is thebootstrap efron 1979 hastie et al 2001 in which multiple data sets are created as follows suppose our original data set consists ofn data points x x 1 xn',\n",
       " 'one approach to determining frequentist error bars is thebootstrap efron 1979 hastie et al 2001 in which multiple data sets are created as follows suppose our original data set consists ofn data points x x 1 xn we can create a new data setxb by drawingn points at random fromx with replacement so that some points inx may be replicated inxb whereas other points inx may be absent from xb this process can be repeatedl times to generatel data sets each of sizen and each obtained by sampling from the original data setx the statistical accuracy of parameter estimates can then be evaluated by looking at the variability of predictions between the different bootstrap data sets one advantage of the bayesian viewpoint is that the inclusion of prior knowl edge arises naturally',\n",
       " 'the statistical accuracy of parameter estimates can then be evaluated by looking at the variability of predictions between the different bootstrap data sets one advantage of the bayesian viewpoint is that the inclusion of prior knowl edge arises naturally suppose for instance that a fair looking coin is tossed three times and lands heads each time a classical maximum likelihood estimate of the probability of landing heads would give 1 implying that all future tosses will landsection 2 1 heads by contrast a bayesian approach with any reasonable prior will lead to a much less extreme conclusion',\n",
       " 'a classical maximum likelihood estimate of the probability of landing heads would give 1 implying that all future tosses will landsection 2 1 heads by contrast a bayesian approach with any reasonable prior will lead to a much less extreme conclusion there has been much controversy and debate associated with the relative mer its of the frequentist and bayesian paradigms which have not been helped by the fact that there is no unique frequentist or even bayesian viewpoint for instance one common criticism of the bayesian approach is that the prior distribution is of ten selected on the basis of mathematical convenience rather than as a reÔ¨Çection of any prior beliefs',\n",
       " 'there has been much controversy and debate associated with the relative mer its of the frequentist and bayesian paradigms which have not been helped by the fact that there is no unique frequentist or even bayesian viewpoint for instance one common criticism of the bayesian approach is that the prior distribution is of ten selected on the basis of mathematical convenience rather than as a reÔ¨Çection of any prior beliefs even the subjective nature of the conclusions through their de pendence on the choice of prior is seen by some as a source of difÔ¨Åculty',\n",
       " 'for instance one common criticism of the bayesian approach is that the prior distribution is of ten selected on the basis of mathematical convenience rather than as a reÔ¨Çection of any prior beliefs even the subjective nature of the conclusions through their de pendence on the choice of prior is seen by some as a source of difÔ¨Åculty reducing the dependence on the prior is one motivation for so callednoninformative priors section 2 4 3 however these lead to difÔ¨Åculties when comparing different models and indeed bayesian methods based on poor choices of prior can give poor results with high conÔ¨Ådence frequentist evaluation methods offer some protection from such prob lems and techniques such as cross validation remain useful in areas such as modelsection 1 3 comparison',\n",
       " 'reducing the dependence on the prior is one motivation for so callednoninformative priors section 2 4 3 however these lead to difÔ¨Åculties when comparing different models and indeed bayesian methods based on poor choices of prior can give poor results with high conÔ¨Ådence frequentist evaluation methods offer some protection from such prob lems and techniques such as cross validation remain useful in areas such as modelsection 1 3 comparison this book places a strong emphasis on the bayesian viewpoint reÔ¨Çecting the huge growth in the practical importance of bayesian methods in the past few years while also discussing useful frequentist concepts as required',\n",
       " 'frequentist evaluation methods offer some protection from such prob lems and techniques such as cross validation remain useful in areas such as modelsection 1 3 comparison this book places a strong emphasis on the bayesian viewpoint reÔ¨Çecting the huge growth in the practical importance of bayesian methods in the past few years while also discussing useful frequentist concepts as required although the bayesian framework has its origins in the 18 th century the prac tical application of bayesian methods was for a long time severely limited by the difÔ¨Åculties in carrying through the full bayesian procedure particularly the need to marginalize sum or integrate over the whole of parameter space which as we shall 24 1',\n",
       " 'this book places a strong emphasis on the bayesian viewpoint reÔ¨Çecting the huge growth in the practical importance of bayesian methods in the past few years while also discussing useful frequentist concepts as required although the bayesian framework has its origins in the 18 th century the prac tical application of bayesian methods was for a long time severely limited by the difÔ¨Åculties in carrying through the full bayesian procedure particularly the need to marginalize sum or integrate over the whole of parameter space which as we shall 24 1 introduction see is required in order to make predictions or to compare different models',\n",
       " 'although the bayesian framework has its origins in the 18 th century the prac tical application of bayesian methods was for a long time severely limited by the difÔ¨Åculties in carrying through the full bayesian procedure particularly the need to marginalize sum or integrate over the whole of parameter space which as we shall 24 1 introduction see is required in order to make predictions or to compare different models the development of sampling methods such as markov chain monte carlo discussed in chapter 11 along with dramatic improvements in the speed and memory capacity of computers opened the door to the practical use of bayesian techniques in an im pressive range of problem domains monte carlo methods are very Ô¨Çexible and can be applied to a wide range of models',\n",
       " 'the development of sampling methods such as markov chain monte carlo discussed in chapter 11 along with dramatic improvements in the speed and memory capacity of computers opened the door to the practical use of bayesian techniques in an im pressive range of problem domains monte carlo methods are very Ô¨Çexible and can be applied to a wide range of models however they are computationally intensive and have mainly been used for small scale problems more recently highly efÔ¨Åcient deterministic approximation schemes such as variational bayes and expectation propagation discussed in chapter 10 have been developed these offer a complementary alternative to sampling methods and have allowed bayesian techniques to be used in large scale applications bleiet al 2003',\n",
       " 'more recently highly efÔ¨Åcient deterministic approximation schemes such as variational bayes and expectation propagation discussed in chapter 10 have been developed these offer a complementary alternative to sampling methods and have allowed bayesian techniques to be used in large scale applications bleiet al 2003 1 2 4 the gaussian distribution we shall devote the whole of chapter 2 to a study of various probability dis tributions and their key properties it is convenient however to introduce here one of the most important probability distributions for continuous variables called the normal or gaussian distribution we shall make extensive use of this distribution in the remainder of this chapter and indeed throughout much of the book',\n",
       " 'it is convenient however to introduce here one of the most important probability distributions for continuous variables called the normal or gaussian distribution we shall make extensive use of this distribution in the remainder of this chapter and indeed throughout much of the book for the case of a single real valued variablex the gaussian distribution is de Ô¨Åned by n x ¬µ œÉ2 1 2œÄœÉ2 1 2 exp 1 2œÉ2 x ¬µ 2 1 46 which is governed by two parameters ¬µ called the mean and œÉ2 called the vari ance the square root of the variance given byœÉ is called thestandard deviation and the reciprocal of the variance written asŒ≤ 1 œÉ2 is called theprecision w e shall see the motivation for these terms shortly figure 1 13 shows a plot of the gaussian distribution',\n",
       " 'the square root of the variance given byœÉ is called thestandard deviation and the reciprocal of the variance written asŒ≤ 1 œÉ2 is called theprecision w e shall see the motivation for these terms shortly figure 1 13 shows a plot of the gaussian distribution from the form of 1 46 we see that the gaussian distribution satisÔ¨Åes n x ¬µ œÉ2 0 1 47 also it is straightforward to show that the gaussian is normalized so thatexercise 1 7 pierre simon laplace 1749 1827 it is said that laplace was seri ously lacking in modesty and at one point declared himself to be the best mathematician in france at the time a claim that was arguably true',\n",
       " 'from the form of 1 46 we see that the gaussian distribution satisÔ¨Åes n x ¬µ œÉ2 0 1 47 also it is straightforward to show that the gaussian is normalized so thatexercise 1 7 pierre simon laplace 1749 1827 it is said that laplace was seri ously lacking in modesty and at one point declared himself to be the best mathematician in france at the time a claim that was arguably true as well as being proliÔ¨Åc in mathe matics he also made numerous contributions to as tronomy including the nebular hypothesis by which the earth is thought to have formed from the condensa tion and cooling of a large rotating disk of gas and dust',\n",
       " '1 47 also it is straightforward to show that the gaussian is normalized so thatexercise 1 7 pierre simon laplace 1749 1827 it is said that laplace was seri ously lacking in modesty and at one point declared himself to be the best mathematician in france at the time a claim that was arguably true as well as being proliÔ¨Åc in mathe matics he also made numerous contributions to as tronomy including the nebular hypothesis by which the earth is thought to have formed from the condensa tion and cooling of a large rotating disk of gas and dust in 1812 he published the Ô¨Årst edition of th eorie analytique des probabilit es in which laplace states that probability theory is nothing but common sense reduced to calculation',\n",
       " 'as well as being proliÔ¨Åc in mathe matics he also made numerous contributions to as tronomy including the nebular hypothesis by which the earth is thought to have formed from the condensa tion and cooling of a large rotating disk of gas and dust in 1812 he published the Ô¨Årst edition of th eorie analytique des probabilit es in which laplace states that probability theory is nothing but common sense reduced to calculation this work included a discus sion of the inverse probability calculation later termed bayes theorem by poincar e which he used to solve problems in life expectancy jurisprudence planetary masses triangulation and error estimation 1 2',\n",
       " 'this work included a discus sion of the inverse probability calculation later termed bayes theorem by poincar e which he used to solve problems in life expectancy jurisprudence planetary masses triangulation and error estimation 1 2 probability theory 25 figure 1 13 plot of the univariate gaussian showing the mean ¬µ and the standard deviation œÉ n x ¬µ œÉ2 x 2œÉ ¬µ n x ¬µ œÉ2 dx 1 1 48 thus 1 46 satisÔ¨Åes the two requirements for a valid probability density we can readily Ô¨Ånd expectations of functions ofx under the gaussian distribu tion in particular the average value ofx is given byexercise 1 8 e x n x ¬µ œÉ2 xdx ¬µ 1 49 because the parameter¬µ represents the average value ofx under the distribution it is referred to as the mean',\n",
       " 'in particular the average value ofx is given byexercise 1 8 e x n x ¬µ œÉ2 xdx ¬µ 1 49 because the parameter¬µ represents the average value ofx under the distribution it is referred to as the mean similarly for the second order moment e x2 n x ¬µ œÉ2 x2 dx ¬µ2 œÉ2 1 50 from 1 49 and 1 50 it follows that the variance ofx is given by var x e x2 e x 2 œÉ2 1 51 and henceœÉ2 is referred to as the variance parameter the maximum of a distribution is known as its mode',\n",
       " '1 50 from 1 49 and 1 50 it follows that the variance ofx is given by var x e x2 e x 2 œÉ2 1 51 and henceœÉ2 is referred to as the variance parameter the maximum of a distribution is known as its mode for a gaussian the mode coincides with the mean exercise 1 9 we are also interested in the gaussian distribution deÔ¨Åned over ad dimensional vector x of continuous variables which is given by n x ¬µ œÉ 1 2œÄ d 2 1 œÉ 1 2 exp 1 2 x ¬µ tœÇ 1 x ¬µ 1 52 where thed dimensional vector¬µ is called the mean thed d matrix œÉ is called the covariance and œÉ denotes the determinant of œÉ we shall make use of the multivariate gaussian distribution brieÔ¨Çy in this chapter although its properties will be studied in detail in section 2 3 26 1',\n",
       " 'we shall make use of the multivariate gaussian distribution brieÔ¨Çy in this chapter although its properties will be studied in detail in section 2 3 26 1 introduction figure 1 14 illustration of the likelihood function for a gaussian distribution shown by the red curve here the black points de note a data set of values xn and the likelihood function given by 1 53 corresponds to the product of the blue values maximizing the likelihood in volves adjusting the mean and vari ance of the gaussian so as to maxi mize this product x p x xn n xn ¬µ œÉ2 now suppose that we have a data set of observationsx x1 x n t rep resenting n observations of the scalar variablex',\n",
       " 'maximizing the likelihood in volves adjusting the mean and vari ance of the gaussian so as to maxi mize this product x p x xn n xn ¬µ œÉ2 now suppose that we have a data set of observationsx x1 x n t rep resenting n observations of the scalar variablex note that we are using the type face x to distinguish this from a single observation of the vector valued variable x1 x d t which we denote byx we shall suppose that the observations are drawn independently from a gaussian distribution whose mean¬µ and variance œÉ2 are unknown and we would like to determine these parameters from the data set data points that are drawn independently from the same distribution are said to be independent and identically distributed which is often abbreviated to i i d',\n",
       " 'we shall suppose that the observations are drawn independently from a gaussian distribution whose mean¬µ and variance œÉ2 are unknown and we would like to determine these parameters from the data set data points that are drawn independently from the same distribution are said to be independent and identically distributed which is often abbreviated to i i d we have seen that the joint probability of two independent events is given by the product of the marginal probabilities for each event separately because our data setx is i i d we can therefore write the probability of the data set given¬µ and œÉ2 in the form p x ¬µ œÉ2 n n 1 n xn ¬µ œÉ2',\n",
       " 'we have seen that the joint probability of two independent events is given by the product of the marginal probabilities for each event separately because our data setx is i i d we can therefore write the probability of the data set given¬µ and œÉ2 in the form p x ¬µ œÉ2 n n 1 n xn ¬µ œÉ2 1 53 when viewed as a function of¬µ and œÉ2 this is the likelihood function for the gaus sian and is interpreted diagrammatically in figure 1 14 one common criterion for determining the parameters in a probability distribu tion using an observed data set is to Ô¨Ånd the parameter values that maximize the likelihood function',\n",
       " '1 53 when viewed as a function of¬µ and œÉ2 this is the likelihood function for the gaus sian and is interpreted diagrammatically in figure 1 14 one common criterion for determining the parameters in a probability distribu tion using an observed data set is to Ô¨Ånd the parameter values that maximize the likelihood function this might seem like a strange criterion because from our fore going discussion of probability theory it would seem more natural to maximize the probability of the parameters given the data not the probability of the data given the parameters',\n",
       " 'one common criterion for determining the parameters in a probability distribu tion using an observed data set is to Ô¨Ånd the parameter values that maximize the likelihood function this might seem like a strange criterion because from our fore going discussion of probability theory it would seem more natural to maximize the probability of the parameters given the data not the probability of the data given the parameters in fact these two criteria are related as we shall discuss in the context of curve Ô¨Åtting section 1 2 5 for the moment however we shall determine values for the unknown parame ters ¬µ and œÉ2 in the gaussian by maximizing the likelihood function 1 53 in prac tice it is more convenient to maximize the log of the likelihood function',\n",
       " 'in fact these two criteria are related as we shall discuss in the context of curve Ô¨Åtting section 1 2 5 for the moment however we shall determine values for the unknown parame ters ¬µ and œÉ2 in the gaussian by maximizing the likelihood function 1 53 in prac tice it is more convenient to maximize the log of the likelihood function because the logarithm is a monotonically increasing function of its argument maximization of the log of a function is equivalent to maximization of the function itself',\n",
       " 'in prac tice it is more convenient to maximize the log of the likelihood function because the logarithm is a monotonically increasing function of its argument maximization of the log of a function is equivalent to maximization of the function itself taking the log not only simpliÔ¨Åes the subsequent mathematical analysis but it also helps numerically because the product of a large number of small probabilities can easily underÔ¨Çow the numerical precision of the computer and this is resolved by computing instead the sum of the log probabilities from 1 46 and 1 53 the log likelihood 1 2 probability theory 27 function can be written in the form lnp x ¬µ œÉ2 1 2œÉ2 n n 1 xn ¬µ 2 n 2 lnœÉ2 n 2 ln 2œÄ',\n",
       " 'from 1 46 and 1 53 the log likelihood 1 2 probability theory 27 function can be written in the form lnp x ¬µ œÉ2 1 2œÉ2 n n 1 xn ¬µ 2 n 2 lnœÉ2 n 2 ln 2œÄ 1 54 maximizing 1 54 with respect to ¬µ we obtain the maximum likelihood solution given byexercise 1 11 ¬µml 1 n n n 1 xn 1 55 which is the sample mean i e the mean of the observed values xn similarly maximizing 1 54 with respect toœÉ2 we obtain the maximum likelihood solution for the variance in the form œÉ2 ml 1 n n n 1 xn ¬µml 2 1 56 which is thesample variance measured with respect to the sample mean¬µml',\n",
       " '1 54 maximizing 1 54 with respect to ¬µ we obtain the maximum likelihood solution given byexercise 1 11 ¬µml 1 n n n 1 xn 1 55 which is the sample mean i e the mean of the observed values xn similarly maximizing 1 54 with respect toœÉ2 we obtain the maximum likelihood solution for the variance in the form œÉ2 ml 1 n n n 1 xn ¬µml 2 1 56 which is thesample variance measured with respect to the sample mean¬µml note that we are performing a joint maximization of 1 54 with respect to¬µ and œÉ2 b u t in the case of the gaussian distribution the solution for¬µ decouples from that forœÉ2 so that we can Ô¨Årst evaluate 1 55 and then subsequently use this result to evaluate 1 56',\n",
       " 'similarly maximizing 1 54 with respect toœÉ2 we obtain the maximum likelihood solution for the variance in the form œÉ2 ml 1 n n n 1 xn ¬µml 2 1 56 which is thesample variance measured with respect to the sample mean¬µml note that we are performing a joint maximization of 1 54 with respect to¬µ and œÉ2 b u t in the case of the gaussian distribution the solution for¬µ decouples from that forœÉ2 so that we can Ô¨Årst evaluate 1 55 and then subsequently use this result to evaluate 1 56 later in this chapter and also in subsequent chapters we shall highlight the sig niÔ¨Åcant limitations of the maximum likelihood approach here we give an indication of the problem in the context of our solutions for the maximum likelihood param eter settings for the univariate gaussian distribution',\n",
       " 'later in this chapter and also in subsequent chapters we shall highlight the sig niÔ¨Åcant limitations of the maximum likelihood approach here we give an indication of the problem in the context of our solutions for the maximum likelihood param eter settings for the univariate gaussian distribution in particular we shall show that the maximum likelihood approach systematically underestimates the variance of the distribution this is an example of a phenomenon calledbias and is related to the problem of over Ô¨Åtting encountered in the context of polynomial curve Ô¨Åtting section 1 1 we Ô¨Årst note that the maximum likelihood solutions¬µ ml and œÉ2 ml are functions of the data set values x1 x n',\n",
       " 'in particular we shall show that the maximum likelihood approach systematically underestimates the variance of the distribution this is an example of a phenomenon calledbias and is related to the problem of over Ô¨Åtting encountered in the context of polynomial curve Ô¨Åtting section 1 1 we Ô¨Årst note that the maximum likelihood solutions¬µ ml and œÉ2 ml are functions of the data set values x1 x n consider the expectations of these quantities with respect to the data set values which themselves come from a gaussian distribution with parameters ¬µ and œÉ2 it is straightforward to show thatexercise 1 12 e ¬µml ¬µ 1 57 e œÉ2 ml n 1 n œÉ2 1 58 so that on average the maximum likelihood estimate will obtain the correct mean but will underestimate the true variance by a factor n 1 n',\n",
       " 'consider the expectations of these quantities with respect to the data set values which themselves come from a gaussian distribution with parameters ¬µ and œÉ2 it is straightforward to show thatexercise 1 12 e ¬µml ¬µ 1 57 e œÉ2 ml n 1 n œÉ2 1 58 so that on average the maximum likelihood estimate will obtain the correct mean but will underestimate the true variance by a factor n 1 n the intuition behind this result is given by figure 1 15 from 1 58 it follows that the following estimate for the variance parameter is unbiased œÉ2 n n 1œÉ2 ml 1 n 1 n n 1 xn ¬µml 2 1 59 28 1 introduction figure 1 15 illustration of how bias arises in using max imum likelihood to determine the variance of a gaussian',\n",
       " '1 59 28 1 introduction figure 1 15 illustration of how bias arises in using max imum likelihood to determine the variance of a gaussian the green curve shows the true gaussian distribution from which data is generated and the three red curves show the gaussian distributions obtained by Ô¨Åtting to three data sets each consist ing of two data points shown in blue us ing the maximum likelihood results 1 55 and 1 56 averaged across the three data sets the mean is correct but the variance is systematically under estimated because it is measured relative to the sample mean and not relative to the true mean a b c in section 10 1 3 we shall see how this result arises automatically when we adopt a bayesian approach',\n",
       " 'averaged across the three data sets the mean is correct but the variance is systematically under estimated because it is measured relative to the sample mean and not relative to the true mean a b c in section 10 1 3 we shall see how this result arises automatically when we adopt a bayesian approach note that the bias of the maximum likelihood solution becomes less signiÔ¨Åcant as the number n of data points increases and in the limitn the maximum likelihood solution for the variance equals the true variance of the distribution that generated the data in practice for anything other than smalln this bias will not prove to be a serious problem',\n",
       " 'note that the bias of the maximum likelihood solution becomes less signiÔ¨Åcant as the number n of data points increases and in the limitn the maximum likelihood solution for the variance equals the true variance of the distribution that generated the data in practice for anything other than smalln this bias will not prove to be a serious problem however throughout this book we shall be interested in more complex models with many parameters for which the bias problems asso ciated with maximum likelihood will be much more severe in fact as we shall see the issue of bias in maximum likelihood lies at the root of the over Ô¨Åtting problem that we encountered earlier in the context of polynomial curve Ô¨Åtting',\n",
       " 'however throughout this book we shall be interested in more complex models with many parameters for which the bias problems asso ciated with maximum likelihood will be much more severe in fact as we shall see the issue of bias in maximum likelihood lies at the root of the over Ô¨Åtting problem that we encountered earlier in the context of polynomial curve Ô¨Åtting 1 2 5 curve Ô¨Åtting re visited we have seen how the problem of polynomial curve Ô¨Åtting can be expressed in terms of error minimization here we return to the curve Ô¨Åtting example and view itsection 1 1 from a probabilistic perspective thereby gaining some insights into error functions and regularization as well as taking us towards a full bayesian treatment',\n",
       " '1 2 5 curve Ô¨Åtting re visited we have seen how the problem of polynomial curve Ô¨Åtting can be expressed in terms of error minimization here we return to the curve Ô¨Åtting example and view itsection 1 1 from a probabilistic perspective thereby gaining some insights into error functions and regularization as well as taking us towards a full bayesian treatment the goal in the curve Ô¨Åtting problem is to be able to make predictions for the target variablet given some new value of the input variablex on the basis of a set of training data comprisingn input valuesx x1 x n t and their corresponding target values t t1 t n t we can express our uncertainty over the value of the target variable using a probability distribution',\n",
       " 'here we return to the curve Ô¨Åtting example and view itsection 1 1 from a probabilistic perspective thereby gaining some insights into error functions and regularization as well as taking us towards a full bayesian treatment the goal in the curve Ô¨Åtting problem is to be able to make predictions for the target variablet given some new value of the input variablex on the basis of a set of training data comprisingn input valuesx x1 x n t and their corresponding target values t t1 t n t we can express our uncertainty over the value of the target variable using a probability distribution for this purpose we shall assume that given the value ofx the corresponding value oft has a gaussian distribution with a mean equal to the valuey x w of the polynomial curve given by 1 1',\n",
       " 'the goal in the curve Ô¨Åtting problem is to be able to make predictions for the target variablet given some new value of the input variablex on the basis of a set of training data comprisingn input valuesx x1 x n t and their corresponding target values t t1 t n t we can express our uncertainty over the value of the target variable using a probability distribution for this purpose we shall assume that given the value ofx the corresponding value oft has a gaussian distribution with a mean equal to the valuey x w of the polynomial curve given by 1 1 thus we have p t x w Œ≤ n t y x w Œ≤ 1 1 60 where for consistency with the notation in later chapters we have deÔ¨Åned a preci sion parameter Œ≤ corresponding to the inverse variance of the distribution',\n",
       " 'for this purpose we shall assume that given the value ofx the corresponding value oft has a gaussian distribution with a mean equal to the valuey x w of the polynomial curve given by 1 1 thus we have p t x w Œ≤ n t y x w Œ≤ 1 1 60 where for consistency with the notation in later chapters we have deÔ¨Åned a preci sion parameter Œ≤ corresponding to the inverse variance of the distribution this is illustrated schematically in figure 1 16 1 2 probability theory 29 figure 1 16 schematic illustration of a gaus sian conditional distribution for t given x given by 1 60 in which the mean is given by the polyno mial function y x w and the precision is given by the parameter Œ≤ which is related to the vari ance by Œ≤ 1 œÉ2',\n",
       " '1 2 probability theory 29 figure 1 16 schematic illustration of a gaus sian conditional distribution for t given x given by 1 60 in which the mean is given by the polyno mial function y x w and the precision is given by the parameter Œ≤ which is related to the vari ance by Œ≤ 1 œÉ2 t xx0 2œÉy x0 w y x w p t x0 w Œ≤ we now use the training data x t to determine the values of the unknown parameters w and Œ≤ by maximum likelihood if the data are assumed to be drawn independently from the distribution 1 60 then the likelihood function is given by p t x w Œ≤ n n 1 n tn y xn w Œ≤ 1 1 61 as we did in the case of the simple gaussian distribution earlier it is convenient to maximize the logarithm of the likelihood function',\n",
       " 'if the data are assumed to be drawn independently from the distribution 1 60 then the likelihood function is given by p t x w Œ≤ n n 1 n tn y xn w Œ≤ 1 1 61 as we did in the case of the simple gaussian distribution earlier it is convenient to maximize the logarithm of the likelihood function substituting for the form of the gaussian distribution given by 1 46 we obtain the log likelihood function in the form lnp t x w Œ≤ Œ≤ 2 n n 1 y xn w tn 2 n 2 ln Œ≤ n 2 ln 2œÄ 1 62 consider Ô¨Årst the determination of the maximum likelihood solution for the polyno mial coefÔ¨Åcients which will be denoted bywml these are determined by maxi mizing 1 62 with respect tow',\n",
       " '1 62 consider Ô¨Årst the determination of the maximum likelihood solution for the polyno mial coefÔ¨Åcients which will be denoted bywml these are determined by maxi mizing 1 62 with respect tow for this purpose we can omit the last two terms on the right hand side of 1 62 because they do not depend onw also we note that scaling the log likelihood by a positive constant coefÔ¨Åcient does not alter the location of the maximum with respect tow and so we can replace the coefÔ¨Åcient Œ≤ 2 with 1 2 finally instead of maximizing the log likelihood we can equivalently minimize the negative log likelihood we therefore see that maximizing likelihood is equivalent so far as determiningw is concerned to minimizing thesum of squares error functiondeÔ¨Åned by 1 2',\n",
       " 'finally instead of maximizing the log likelihood we can equivalently minimize the negative log likelihood we therefore see that maximizing likelihood is equivalent so far as determiningw is concerned to minimizing thesum of squares error functiondeÔ¨Åned by 1 2 thus the sum of squares error function has arisen as a consequence of maximizing likelihood under the assumption of a gaussian noise distribution we can also use maximum likelihood to determine the precision parameterŒ≤ of the gaussian conditional distribution maximizing 1 62 with respect toŒ≤ gives 1 Œ≤ml 1 n n n 1 y xn wml tn 2 1 63 30 1',\n",
       " 'maximizing 1 62 with respect toŒ≤ gives 1 Œ≤ml 1 n n n 1 y xn wml tn 2 1 63 30 1 introduction again we can Ô¨Årst determine the parameter vectorwml governing the mean and sub sequently use this to Ô¨Ånd the precisionŒ≤ml as was the case for the simple gaussian distribution section 1 2 4 having determined the parametersw and Œ≤ we can now make predictions for new values ofx because we now have a probabilistic model these are expressed in terms of thepredictive distribution that gives the probability distribution overt rather than simply a point estimate and is obtained by substituting the maximum likelihood parameters into 1 60 to give p t x wml Œ≤ml n t y x wml Œ≤ 1 ml',\n",
       " 'introduction again we can Ô¨Årst determine the parameter vectorwml governing the mean and sub sequently use this to Ô¨Ånd the precisionŒ≤ml as was the case for the simple gaussian distribution section 1 2 4 having determined the parametersw and Œ≤ we can now make predictions for new values ofx because we now have a probabilistic model these are expressed in terms of thepredictive distribution that gives the probability distribution overt rather than simply a point estimate and is obtained by substituting the maximum likelihood parameters into 1 60 to give p t x wml Œ≤ml n t y x wml Œ≤ 1 ml 1 64 now let us take a step towards a more bayesian approach and introduce a prior distribution over the polynomial coefÔ¨Åcients w for simplicity let us consider a gaussian distribution of the form p w Œ± n w 0 Œ± 1i Œ± 2œÄ m 1 2 exp Œ± 2 wtw 1 65 where Œ± is the precision of the distribution andm 1 is the total number of elements in the vector w for an mth order polynomial',\n",
       " 'because we now have a probabilistic model these are expressed in terms of thepredictive distribution that gives the probability distribution overt rather than simply a point estimate and is obtained by substituting the maximum likelihood parameters into 1 60 to give p t x wml Œ≤ml n t y x wml Œ≤ 1 ml 1 64 now let us take a step towards a more bayesian approach and introduce a prior distribution over the polynomial coefÔ¨Åcients w for simplicity let us consider a gaussian distribution of the form p w Œ± n w 0 Œ± 1i Œ± 2œÄ m 1 2 exp Œ± 2 wtw 1 65 where Œ± is the precision of the distribution andm 1 is the total number of elements in the vector w for an mth order polynomial v ariables such asŒ± which control the distribution of model parameters are called hyperparameters',\n",
       " '1 64 now let us take a step towards a more bayesian approach and introduce a prior distribution over the polynomial coefÔ¨Åcients w for simplicity let us consider a gaussian distribution of the form p w Œ± n w 0 Œ± 1i Œ± 2œÄ m 1 2 exp Œ± 2 wtw 1 65 where Œ± is the precision of the distribution andm 1 is the total number of elements in the vector w for an mth order polynomial v ariables such asŒ± which control the distribution of model parameters are called hyperparameters using bayes theorem the posterior distribution forw is proportional to the product of the prior distribution and the likelihood function p w x t Œ± Œ≤ p t x w Œ≤ p w Œ±',\n",
       " 'v ariables such asŒ± which control the distribution of model parameters are called hyperparameters using bayes theorem the posterior distribution forw is proportional to the product of the prior distribution and the likelihood function p w x t Œ± Œ≤ p t x w Œ≤ p w Œ± 1 66 we can now determinew by Ô¨Ånding the most probable value ofw given the data in other words by maximizing the posterior distribution this technique is called maximum posterior or simply map taking the negative logarithm of 1 66 and combining with 1 62 and 1 65 we Ô¨Ånd that the maximum of the posterior is given by the minimum of Œ≤ 2 n n 1 y xn w tn 2 Œ± 2 wtw',\n",
       " 'this technique is called maximum posterior or simply map taking the negative logarithm of 1 66 and combining with 1 62 and 1 65 we Ô¨Ånd that the maximum of the posterior is given by the minimum of Œ≤ 2 n n 1 y xn w tn 2 Œ± 2 wtw 1 67 thus we see that maximizing the posterior distribution is equivalent to minimizing the regularized sum of squares error function encountered earlier in the form 1 4 with a regularization parameter given byŒª Œ± Œ≤ 1 2 6 bayesian curve Ô¨Åtting although we have included a prior distributionp w Œ± we are so far still mak ing a point estimate ofw and so this does not yet amount to a bayesian treatment',\n",
       " '1 67 thus we see that maximizing the posterior distribution is equivalent to minimizing the regularized sum of squares error function encountered earlier in the form 1 4 with a regularization parameter given byŒª Œ± Œ≤ 1 2 6 bayesian curve Ô¨Åtting although we have included a prior distributionp w Œ± we are so far still mak ing a point estimate ofw and so this does not yet amount to a bayesian treatment in a fully bayesian approach we should consistently apply the sum and product rules of probability which requires as we shall see shortly that we integrate over all val ues of w such marginalizations lie at the heart of bayesian methods for pattern recognition 1 2',\n",
       " 'in a fully bayesian approach we should consistently apply the sum and product rules of probability which requires as we shall see shortly that we integrate over all val ues of w such marginalizations lie at the heart of bayesian methods for pattern recognition 1 2 probability theory 31 in the curve Ô¨Åtting problem we are given the training datax and t along with a new test point x and our goal is to predict the value of t we therefore wish to evaluate the predictive distribution p t x x t here we shall assume that the parameters Œ± and Œ≤ are Ô¨Åxed and known in advance in later chapters we shall discuss how such parameters can be inferred from data in a bayesian setting',\n",
       " 'probability theory 31 in the curve Ô¨Åtting problem we are given the training datax and t along with a new test point x and our goal is to predict the value of t we therefore wish to evaluate the predictive distribution p t x x t here we shall assume that the parameters Œ± and Œ≤ are Ô¨Åxed and known in advance in later chapters we shall discuss how such parameters can be inferred from data in a bayesian setting a bayesian treatment simply corresponds to a consistent application of the sum and product rules of probability which allow the predictive distribution to be written in the form p t x x t p t x w p w x t d w 1 68 here p t x w is given by 1 60 and we have omitted the dependence onŒ± and Œ≤ to simplify the notation',\n",
       " 'here we shall assume that the parameters Œ± and Œ≤ are Ô¨Åxed and known in advance in later chapters we shall discuss how such parameters can be inferred from data in a bayesian setting a bayesian treatment simply corresponds to a consistent application of the sum and product rules of probability which allow the predictive distribution to be written in the form p t x x t p t x w p w x t d w 1 68 here p t x w is given by 1 60 and we have omitted the dependence onŒ± and Œ≤ to simplify the notation herep w x t is the posterior distribution over param eters and can be found by normalizing the right hand side of 1 66 we shall see in section 3 3 that for problems such as the curve Ô¨Åtting example this posterior distribution is a gaussian and can be evaluated analytically',\n",
       " 'herep w x t is the posterior distribution over param eters and can be found by normalizing the right hand side of 1 66 we shall see in section 3 3 that for problems such as the curve Ô¨Åtting example this posterior distribution is a gaussian and can be evaluated analytically similarly the integra tion in 1 68 can also be performed analytically with the result that the predictive distribution is given by a gaussian of the form p t x x t n t m x s 2 x 1 69 where the mean and variance are given by m x Œ≤œÜ x ts n n 1 œÜ xn tn 1 70 s2 x Œ≤ 1 œÜ x tsœÜ x 1 71 here the matrixs is given by s 1 Œ±i Œ≤ n n 1 œÜ xn œÜ x t 1 72 where i is the unit matrix and we have deÔ¨Åned the vector œÜ x with elements œÜi x xi for i 0 m',\n",
       " 'similarly the integra tion in 1 68 can also be performed analytically with the result that the predictive distribution is given by a gaussian of the form p t x x t n t m x s 2 x 1 69 where the mean and variance are given by m x Œ≤œÜ x ts n n 1 œÜ xn tn 1 70 s2 x Œ≤ 1 œÜ x tsœÜ x 1 71 here the matrixs is given by s 1 Œ±i Œ≤ n n 1 œÜ xn œÜ x t 1 72 where i is the unit matrix and we have deÔ¨Åned the vector œÜ x with elements œÜi x xi for i 0 m we see that the variance as well as the mean of the predictive distribution in 1 69 is dependent onx the Ô¨Årst term in 1 71 represents the uncertainty in the predicted value oft due to the noise on the target variables and was expressed already in the maximum likelihood predictive distribution 1 64 throughŒ≤ 1 ml',\n",
       " 'we see that the variance as well as the mean of the predictive distribution in 1 69 is dependent onx the Ô¨Årst term in 1 71 represents the uncertainty in the predicted value oft due to the noise on the target variables and was expressed already in the maximum likelihood predictive distribution 1 64 throughŒ≤ 1 ml however the second term arises from the uncertainty in the parametersw and is a consequence of the bayesian treatment the predictive distribution for the synthetic sinusoidal regression problem is illustrated in figure 1 17 32 1',\n",
       " 'the predictive distribution for the synthetic sinusoidal regression problem is illustrated in figure 1 17 32 1 introduction figure 1 17 the predictive distribution result ing from a bayesian treatment of polynomial curve Ô¨Åtting using an m 9 polynomial with the Ô¨Åxed parameters Œ± 5 10 3 and Œ≤ 11 1 corresponding to the known noise variance in which the red curve denotes the mean of the predictive distribution and the red region corresponds to 1 stan dard deviation around the mean x t 0 1 1 0 1 1 3 model selection in our example of polynomial curve Ô¨Åtting using least squares we saw that there was an optimal order of polynomial that gave the best generalization the order of the polynomial controls the number of free parameters in the model and thereby governs the model complexity',\n",
       " 'model selection in our example of polynomial curve Ô¨Åtting using least squares we saw that there was an optimal order of polynomial that gave the best generalization the order of the polynomial controls the number of free parameters in the model and thereby governs the model complexity with regularized least squares the regularization coefÔ¨Åcient Œª also controls the effective complexity of the model whereas for more complex models such as mixture distributions or neural networks there may be multiple pa rameters governing complexity in a practical application we need to determine the values of such parameters and the principal objective in doing so is usually to achieve the best predictive performance on new data',\n",
       " 'with regularized least squares the regularization coefÔ¨Åcient Œª also controls the effective complexity of the model whereas for more complex models such as mixture distributions or neural networks there may be multiple pa rameters governing complexity in a practical application we need to determine the values of such parameters and the principal objective in doing so is usually to achieve the best predictive performance on new data furthermore as well as Ô¨Ånd ing the appropriate values for complexity parameters within a given model we may wish to consider a range of different types of model in order to Ô¨Ånd the best one for our particular application',\n",
       " 'in a practical application we need to determine the values of such parameters and the principal objective in doing so is usually to achieve the best predictive performance on new data furthermore as well as Ô¨Ånd ing the appropriate values for complexity parameters within a given model we may wish to consider a range of different types of model in order to Ô¨Ånd the best one for our particular application we have already seen that in the maximum likelihood approach the perfor mance on the training set is not a good indicator of predictive performance on un seen data due to the problem of over Ô¨Åtting',\n",
       " 'furthermore as well as Ô¨Ånd ing the appropriate values for complexity parameters within a given model we may wish to consider a range of different types of model in order to Ô¨Ånd the best one for our particular application we have already seen that in the maximum likelihood approach the perfor mance on the training set is not a good indicator of predictive performance on un seen data due to the problem of over Ô¨Åtting if data is plentiful then one approach is simply to use some of the available data to train a range of models or a given model with a range of values for its complexity parameters and then to compare them on independent data sometimes called avalidation set and select the one having the best predictive performance',\n",
       " 'we have already seen that in the maximum likelihood approach the perfor mance on the training set is not a good indicator of predictive performance on un seen data due to the problem of over Ô¨Åtting if data is plentiful then one approach is simply to use some of the available data to train a range of models or a given model with a range of values for its complexity parameters and then to compare them on independent data sometimes called avalidation set and select the one having the best predictive performance if the model design is iterated many times using a lim ited size data set then some over Ô¨Åtting to the validation data can occur and so it may be necessary to keep aside a thirdtest set on which the performance of the selected model is Ô¨Ånally evaluated',\n",
       " 'if data is plentiful then one approach is simply to use some of the available data to train a range of models or a given model with a range of values for its complexity parameters and then to compare them on independent data sometimes called avalidation set and select the one having the best predictive performance if the model design is iterated many times using a lim ited size data set then some over Ô¨Åtting to the validation data can occur and so it may be necessary to keep aside a thirdtest set on which the performance of the selected model is Ô¨Ånally evaluated in many applications however the supply of data for training and testing will be limited and in order to build good models we wish to use as much of the available data as possible for training',\n",
       " 'if the model design is iterated many times using a lim ited size data set then some over Ô¨Åtting to the validation data can occur and so it may be necessary to keep aside a thirdtest set on which the performance of the selected model is Ô¨Ånally evaluated in many applications however the supply of data for training and testing will be limited and in order to build good models we wish to use as much of the available data as possible for training however if the validation set is small it will give a relatively noisy estimate of predictive performance one solution to this dilemma is to use cross validation which is illustrated in figure 1 18 this allows a proportion s 1 s of the available data to be used for training while making use of all of the 1 4',\n",
       " 'one solution to this dilemma is to use cross validation which is illustrated in figure 1 18 this allows a proportion s 1 s of the available data to be used for training while making use of all of the 1 4 the curse of dimensionality 33 figure 1 18 the technique of s fold cross validation illus trated here for the case of s 4 involves tak ing the available data and partitioning it into s groups in the simplest case these are of equal size then s 1 of the groups are used to train a set of models that are then evaluated on the re maining group this procedure is then repeated for all s possible choices for the held out group indicated here by the red blocks and the perfor mance scores from the s runs are then averaged run 1 run 2 run 3 run 4 data to assess performance',\n",
       " 'this procedure is then repeated for all s possible choices for the held out group indicated here by the red blocks and the perfor mance scores from the s runs are then averaged run 1 run 2 run 3 run 4 data to assess performance when data is particularly scarce it may be appropriate to consider the cases n wheren is the total number of data points which gives the leave one out technique one major drawback of cross validation is that the number of training runs that must be performed is increased by a factor ofs and this can prove problematic for models in which the training is itself computationally expensive',\n",
       " 'when data is particularly scarce it may be appropriate to consider the cases n wheren is the total number of data points which gives the leave one out technique one major drawback of cross validation is that the number of training runs that must be performed is increased by a factor ofs and this can prove problematic for models in which the training is itself computationally expensive a further problem with techniques such as cross validation that use separate data to assess performance is that we might have multiple complexity parameters for a single model for in stance there might be several regularization parameters exploring combinations of settings for such parameters could in the worst case require a number of training runs that is exponential in the number of parameters',\n",
       " 'a further problem with techniques such as cross validation that use separate data to assess performance is that we might have multiple complexity parameters for a single model for in stance there might be several regularization parameters exploring combinations of settings for such parameters could in the worst case require a number of training runs that is exponential in the number of parameters clearly we need a better ap proach ideally this should rely only on the training data and should allow multiple hyperparameters and model types to be compared in a single training run we there fore need to Ô¨Ånd a measure of performance which depends only on the training data and which does not suffer from bias due to over Ô¨Åtting',\n",
       " 'ideally this should rely only on the training data and should allow multiple hyperparameters and model types to be compared in a single training run we there fore need to Ô¨Ånd a measure of performance which depends only on the training data and which does not suffer from bias due to over Ô¨Åtting historically various information criteria have been proposed that attempt to correct for the bias of maximum likelihood by the addition of a penalty term to compensate for the over Ô¨Åtting of more complex models for example theakaike information criterion or aic akaike 1974 chooses the model for which the quan tity lnp d wml m 1 73 is largest here p d wml is the best Ô¨Åt log likelihood andm is the number of adjustable parameters in the model',\n",
       " 'for example theakaike information criterion or aic akaike 1974 chooses the model for which the quan tity lnp d wml m 1 73 is largest here p d wml is the best Ô¨Åt log likelihood andm is the number of adjustable parameters in the model a variant of this quantity called thebayesian information criterion o r bic will be discussed in section 4 4 1 such criteria do not take account of the uncertainty in the model parameters however and in practice they tend to favour overly simple models we therefore turn in section 3 4 to a fully bayesian approach where we shall see how complexity penalties arise in a natural and principled way 1 4 the curse of dimensionality in the polynomial curve Ô¨Åtting example we had just one input variablex',\n",
       " '1 4 the curse of dimensionality in the polynomial curve Ô¨Åtting example we had just one input variablex for prac tical applications of pattern recognition however we will have to deal with spaces 34 1 introduction figure 1 19 scatter plot of the oil Ô¨Çow data for input variables x6 and x7 i n which red denotes the homoge nous class green denotes the annular class and blue denotes the laminar class our goal is to classify the new test point de noted by x6 x7 0 0 25 0 5 0 75 1 0 0 5 1 1 5 2 of high dimensionality comprising many input variables as we now discuss this poses some serious challenges and is an important factor inÔ¨Çuencing the design of pattern recognition techniques',\n",
       " 'x6 x7 0 0 25 0 5 0 75 1 0 0 5 1 1 5 2 of high dimensionality comprising many input variables as we now discuss this poses some serious challenges and is an important factor inÔ¨Çuencing the design of pattern recognition techniques in order to illustrate the problem we consider a synthetically generated data set representing measurements taken from a pipeline containing a mixture of oil wa ter and gas bishop and james 1993 these three materials can be present in one of three different geometrical conÔ¨Ågurations known as homogenous annular and laminar and the fractions of the three materials can also vary',\n",
       " 'in order to illustrate the problem we consider a synthetically generated data set representing measurements taken from a pipeline containing a mixture of oil wa ter and gas bishop and james 1993 these three materials can be present in one of three different geometrical conÔ¨Ågurations known as homogenous annular and laminar and the fractions of the three materials can also vary each data point com prises a 12 dimensional input vector consisting of measurements taken with gamma ray densitometers that measure the attenuation of gamma rays passing along nar row beams through the pipe this data set is described in detail in appendix a',\n",
       " 'each data point com prises a 12 dimensional input vector consisting of measurements taken with gamma ray densitometers that measure the attenuation of gamma rays passing along nar row beams through the pipe this data set is described in detail in appendix a figure 1 19 shows100 points from this data set on a plot showing two of the mea surements x6 and x7 the remaining ten input values are ignored for the purposes of this illustration each data point is labelled according to which of the three geomet rical classes it belongs to and our goal is to use this data as a training set in order to be able to classify a new observation x6 x7 such as the one denoted by the cross in figure 1 19',\n",
       " 'figure 1 19 shows100 points from this data set on a plot showing two of the mea surements x6 and x7 the remaining ten input values are ignored for the purposes of this illustration each data point is labelled according to which of the three geomet rical classes it belongs to and our goal is to use this data as a training set in order to be able to classify a new observation x6 x7 such as the one denoted by the cross in figure 1 19 we observe that the cross is surrounded by numerous red points and so we might suppose that it belongs to the red class however there are also plenty of green points nearby so we might think that it could instead belong to the green class it seems unlikely that it belongs to the blue class',\n",
       " 'however there are also plenty of green points nearby so we might think that it could instead belong to the green class it seems unlikely that it belongs to the blue class the intuition here is that the identity of the cross should be determined more strongly by nearby points from the training set and less strongly by more distant points in fact this intuition turns out to be reasonable and will be discussed more fully in later chapters how can we turn this intuition into a learning algorithm one very simple ap proach would be to divide the input space into regular cells as indicated in fig ure 1 20 when we are given a test point and we wish to predict its class we Ô¨Årst decide which cell it belongs to and we then Ô¨Ånd all of the training data points that 1 4',\n",
       " 'one very simple ap proach would be to divide the input space into regular cells as indicated in fig ure 1 20 when we are given a test point and we wish to predict its class we Ô¨Årst decide which cell it belongs to and we then Ô¨Ånd all of the training data points that 1 4 the curse of dimensionality 35 figure 1 20 illustration of a simple approach to the solution of a classiÔ¨Åcation problem in which the input space is divided into cells and any new test point is assigned to the class that has a majority number of rep resentatives in the same cell as the test point as we shall see shortly this simplistic approach has some severe shortcomings x6 x7 0 0 25 0 5 0 75 1 0 0 5 1 1 5 2 fall in the same cell',\n",
       " 'as we shall see shortly this simplistic approach has some severe shortcomings x6 x7 0 0 25 0 5 0 75 1 0 0 5 1 1 5 2 fall in the same cell the identity of the test point is predicted as being the same as the class having the largest number of training points in the same cell as the test point with ties being broken at random there are numerous problems with this naive approach but one of the most se vere becomes apparent when we consider its extension to problems having larger numbers of input variables corresponding to input spaces of higher dimensionality the origin of the problem is illustrated in figure 1 21 which shows that if we divide a region of a space into regular cells then the number of such cells grows exponen tially with the dimensionality of the space',\n",
       " 'there are numerous problems with this naive approach but one of the most se vere becomes apparent when we consider its extension to problems having larger numbers of input variables corresponding to input spaces of higher dimensionality the origin of the problem is illustrated in figure 1 21 which shows that if we divide a region of a space into regular cells then the number of such cells grows exponen tially with the dimensionality of the space the problem with an exponentially large number of cells is that we would need an exponentially large quantity of training data in order to ensure that the cells are not empty clearly we have no hope of applying such a technique in a space of more than a few variables and so we need to Ô¨Ånd a more sophisticated approach',\n",
       " 'the problem with an exponentially large number of cells is that we would need an exponentially large quantity of training data in order to ensure that the cells are not empty clearly we have no hope of applying such a technique in a space of more than a few variables and so we need to Ô¨Ånd a more sophisticated approach we can gain further insight into the problems of high dimensional spaces by returning to the example of polynomial curve Ô¨Åtting and considering how we wouldsection 1 1 figure 1 21 illustration of the curse of dimensionality showing how the number of regions of a regular grid grows exponentially with the dimensionality d of the space for clarity only a subset of the cubical regions are shown for d 3 x1 d 1 x1 x2 d 2 x1 x2 x3 d 3 36 1',\n",
       " 'for clarity only a subset of the cubical regions are shown for d 3 x1 d 1 x1 x2 d 2 x1 x2 x3 d 3 36 1 introduction extend this approach to deal with input spaces having several variables if we have d input variables then a general polynomial with coefÔ¨Åcients up to order3 would take the form y x w w0 d i 1 wixi d i 1 d j 1 wijxixj d i 1 d j 1 d k 1 wijkxixjxk 1 74 as d increases so the number of independent coefÔ¨Åcients not all of the coefÔ¨Åcients are independent due to interchange symmetries amongst thex variables grows pro portionally tod3 in practice to capture complex dependencies in the data we may need to use a higher order polynomial for a polynomial of orderm the growth in the number of coefÔ¨Åcients is likedm',\n",
       " 'in practice to capture complex dependencies in the data we may need to use a higher order polynomial for a polynomial of orderm the growth in the number of coefÔ¨Åcients is likedm although this is now a power law growth exercise 1 16 rather than an exponential growth it still points to the method becoming rapidly unwieldy and of limited practical utility our geometrical intuitions formed through a life spent in a space of three di mensions can fail badly when we consider spaces of higher dimensionality as a simple example consider a sphere of radiusr 1 in a space ofd dimensions and ask what is the fraction of the volume of the sphere that lies between radiusr 1 œµ and r 1',\n",
       " 'our geometrical intuitions formed through a life spent in a space of three di mensions can fail badly when we consider spaces of higher dimensionality as a simple example consider a sphere of radiusr 1 in a space ofd dimensions and ask what is the fraction of the volume of the sphere that lies between radiusr 1 œµ and r 1 we can evaluate this fraction by noting that the volume of a sphere of radius r in d dimensions must scale asr d and so we write vd r kdrd 1 75 where the constantkd depends only ond thus the required fraction is given byexercise 1 18 vd 1 vd 1 œµ vd 1 1 1 œµ d 1 76 which is plotted as a function ofœµ for various values ofd in figure 1 22 we see that for larged this fraction tends to1 even for small values ofœµ',\n",
       " 'thus the required fraction is given byexercise 1 18 vd 1 vd 1 œµ vd 1 1 1 œµ d 1 76 which is plotted as a function ofœµ for various values ofd in figure 1 22 we see that for larged this fraction tends to1 even for small values ofœµ thus in spaces of high dimensionality most of the volume of a sphere is concentrated in a thin shell near the surface as a further example of direct relevance to pattern recognition consider the behaviour of a gaussian distribution in a high dimensional space',\n",
       " 'thus in spaces of high dimensionality most of the volume of a sphere is concentrated in a thin shell near the surface as a further example of direct relevance to pattern recognition consider the behaviour of a gaussian distribution in a high dimensional space if we transform from cartesian to polar coordinates and then integrate out the directional variables we obtain an expression for the densityp r as a function of radiusr from the origin exercise 1 20 thus p r Œ¥r is the probability mass inside a thin shell of thicknessŒ¥r located at radius r this distribution is plotted for various values ofd in figure 1 23 and we see that for larged the probability mass of the gaussian is concentrated in a thin shell',\n",
       " 'as a further example of direct relevance to pattern recognition consider the behaviour of a gaussian distribution in a high dimensional space if we transform from cartesian to polar coordinates and then integrate out the directional variables we obtain an expression for the densityp r as a function of radiusr from the origin exercise 1 20 thus p r Œ¥r is the probability mass inside a thin shell of thicknessŒ¥r located at radius r this distribution is plotted for various values ofd in figure 1 23 and we see that for larged the probability mass of the gaussian is concentrated in a thin shell the severe difÔ¨Åculty that can arise in spaces of many dimensions is sometimes called the curse of dimensionality bellman 1961',\n",
       " 'if we transform from cartesian to polar coordinates and then integrate out the directional variables we obtain an expression for the densityp r as a function of radiusr from the origin exercise 1 20 thus p r Œ¥r is the probability mass inside a thin shell of thicknessŒ¥r located at radius r this distribution is plotted for various values ofd in figure 1 23 and we see that for larged the probability mass of the gaussian is concentrated in a thin shell the severe difÔ¨Åculty that can arise in spaces of many dimensions is sometimes called the curse of dimensionality bellman 1961 in this book we shall make ex tensive use of illustrative examples involving input spaces of one or two dimensions because this makes it particularly easy to illustrate the techniques graphically',\n",
       " 'the severe difÔ¨Åculty that can arise in spaces of many dimensions is sometimes called the curse of dimensionality bellman 1961 in this book we shall make ex tensive use of illustrative examples involving input spaces of one or two dimensions because this makes it particularly easy to illustrate the techniques graphically the reader should be warned however that not all intuitions developed in spaces of low dimensionality will generalize to spaces of many dimensions 1 4',\n",
       " 'the reader should be warned however that not all intuitions developed in spaces of low dimensionality will generalize to spaces of many dimensions 1 4 the curse of dimensionality 37 figure 1 22 plot of the fraction of the volume of a sphere lying in the ranger 1 œµ to r 1 for various values of the dimensionality d œµ volume fraction d 1 d 2 d 5 d 2 0 0 0 2 0 4 0 6 0 8 1 0 0 2 0 4 0 6 0 8 1 although the curse of dimensionality certainly raises important issues for pat tern recognition applications it does not prevent us from Ô¨Ånding effective techniques applicable to high dimensional spaces the reasons for this are twofold',\n",
       " 'the curse of dimensionality 37 figure 1 22 plot of the fraction of the volume of a sphere lying in the ranger 1 œµ to r 1 for various values of the dimensionality d œµ volume fraction d 1 d 2 d 5 d 2 0 0 0 2 0 4 0 6 0 8 1 0 0 2 0 4 0 6 0 8 1 although the curse of dimensionality certainly raises important issues for pat tern recognition applications it does not prevent us from Ô¨Ånding effective techniques applicable to high dimensional spaces the reasons for this are twofold first real data will often be conÔ¨Åned to a region of the space having lower effective dimension ality and in particular the directions over which important variations in the target variables occur may be so conÔ¨Åned',\n",
       " 'the reasons for this are twofold first real data will often be conÔ¨Åned to a region of the space having lower effective dimension ality and in particular the directions over which important variations in the target variables occur may be so conÔ¨Åned second real data will typically exhibit some smoothness properties at least locally so that for the most part small changes in the input variables will produce small changes in the target variables and so we can ex ploit local interpolation like techniques to allow us to make predictions of the target variables for new values of the input variables successful pattern recognition tech niques exploit one or both of these properties',\n",
       " 'second real data will typically exhibit some smoothness properties at least locally so that for the most part small changes in the input variables will produce small changes in the target variables and so we can ex ploit local interpolation like techniques to allow us to make predictions of the target variables for new values of the input variables successful pattern recognition tech niques exploit one or both of these properties consider for example an application in manufacturing in which images are captured of identical planar objects on a con veyor belt in which the goal is to determine their orientation',\n",
       " 'successful pattern recognition tech niques exploit one or both of these properties consider for example an application in manufacturing in which images are captured of identical planar objects on a con veyor belt in which the goal is to determine their orientation each image is a point figure 1 23 plot of the probability density with respect to radius r of a gaus sian distribution for various values of the dimensionality d i n a high dimensional space most of the probability mass of a gaussian is lo cated within a thin shell at a speciÔ¨Åc radius d 1 d 2 d 2 0 r p r 0 2 4 0 1 2 38 1 introduction in a high dimensional space whose dimensionality is determined by the number of pixels',\n",
       " 'd 1 d 2 d 2 0 r p r 0 2 4 0 1 2 38 1 introduction in a high dimensional space whose dimensionality is determined by the number of pixels because the objects can occur at different positions within the image and in different orientations there are three degrees of freedom of variability between images and a set of images will live on a three dimensionalmanifold embedded within the high dimensional space due to the complex relationships between the object position or orientation and the pixel intensities this manifold will be highly nonlinear if the goal is to learn a model that can take an input image and output the orientation of the object irrespective of its position then there is only one degree of freedom of variability within the manifold that is signiÔ¨Åcant 1 5',\n",
       " 'if the goal is to learn a model that can take an input image and output the orientation of the object irrespective of its position then there is only one degree of freedom of variability within the manifold that is signiÔ¨Åcant 1 5 decision theory we have seen in section 1 2 how probability theory provides us with a consistent mathematical framework for quantifying and manipulating uncertainty here we turn to a discussion of decision theory that when combined with probability theory allows us to make optimal decisions in situations involving uncertainty such as those encountered in pattern recognition suppose we have an input vectorx together with a corresponding vectort of target variables and our goal is to predictt given a new value forx',\n",
       " 'here we turn to a discussion of decision theory that when combined with probability theory allows us to make optimal decisions in situations involving uncertainty such as those encountered in pattern recognition suppose we have an input vectorx together with a corresponding vectort of target variables and our goal is to predictt given a new value forx for regression problems t will comprise continuous variables whereas for classiÔ¨Åcation problems t will represent class labels the joint probability distribution p x t provides a complete summary of the uncertainty associated with these variables determination of p x t from a set of training data is an example ofinference and is typically a very difÔ¨Åcult problem whose solution forms the subject of much of this book',\n",
       " 'the joint probability distribution p x t provides a complete summary of the uncertainty associated with these variables determination of p x t from a set of training data is an example ofinference and is typically a very difÔ¨Åcult problem whose solution forms the subject of much of this book in a practical application however we must often make a speciÔ¨Åc prediction for the value oft or more generally take a speciÔ¨Åc action based on our understanding of the values t is likely to take and this aspect is the subject of decision theory consider for example a medical diagnosis problem in which we have taken an x ray image of a patient and we wish to determine whether the patient has cancer or not',\n",
       " 'in a practical application however we must often make a speciÔ¨Åc prediction for the value oft or more generally take a speciÔ¨Åc action based on our understanding of the values t is likely to take and this aspect is the subject of decision theory consider for example a medical diagnosis problem in which we have taken an x ray image of a patient and we wish to determine whether the patient has cancer or not in this case the input vector x is the set of pixel intensities in the image and output variablet will represent the presence of cancer which we denote by the class c 1 or the absence of cancer which we denote by the classc2 we might for instance chooset to be a binary variable such thatt 0 corresponds to classc1 and t 1 corresponds to class c2',\n",
       " 'in this case the input vector x is the set of pixel intensities in the image and output variablet will represent the presence of cancer which we denote by the class c 1 or the absence of cancer which we denote by the classc2 we might for instance chooset to be a binary variable such thatt 0 corresponds to classc1 and t 1 corresponds to class c2 we shall see later that this choice of label values is particularly convenient for probabilistic models the general inference problem then involves determining the joint distributionp x ck or equivalently p x t which gives us the most complete probabilistic description of the situation',\n",
       " 'we shall see later that this choice of label values is particularly convenient for probabilistic models the general inference problem then involves determining the joint distributionp x ck or equivalently p x t which gives us the most complete probabilistic description of the situation although this can be a very useful and informative quantity in the end we must decide either to give treatment to the patient or not and we would like this choice to be optimal in some appropriate sense duda and hart 1973 this is the decision step and it is the subject of decision theory to tell us how to make optimal decisions given the appropriate probabilities we shall see that the decision stage is generally very simple even trivial once we have solved the inference problem',\n",
       " 'this is the decision step and it is the subject of decision theory to tell us how to make optimal decisions given the appropriate probabilities we shall see that the decision stage is generally very simple even trivial once we have solved the inference problem here we give an introduction to the key ideas of decision theory as required for 1 5 decision theory 39 the rest of the book further background as well as more detailed accounts can be found in berger 1985 and bather 2000 before giving a more detailed analysis let us Ô¨Årst consider informally how we might expect probabilities to play a role in making decisions when we obtain the x ray image x for a new patient our goal is to decide which of the two classes to assign to the image',\n",
       " 'before giving a more detailed analysis let us Ô¨Årst consider informally how we might expect probabilities to play a role in making decisions when we obtain the x ray image x for a new patient our goal is to decide which of the two classes to assign to the image we are interested in the probabilities of the two classes given the image which are given byp ck x using bayes theorem these probabilities can be expressed in the form p ck x p x ck p ck p x 1 77 note that any of the quantities appearing in bayes theorem can be obtained from the joint distributionp x ck by either marginalizing or conditioning with respect to the appropriate variables we can now interpretp ck as the prior probability for the class ck and p ck x as the corresponding posterior probability',\n",
       " '1 77 note that any of the quantities appearing in bayes theorem can be obtained from the joint distributionp x ck by either marginalizing or conditioning with respect to the appropriate variables we can now interpretp ck as the prior probability for the class ck and p ck x as the corresponding posterior probability thusp c1 repre sents the probability that a person has cancer before we take the x ray measurement similarly p c1 x is the corresponding probability revised using bayes theorem in light of the information contained in the x ray if our aim is to minimize the chance of assigning x to the wrong class then intuitively we would choose the class having the higher posterior probability',\n",
       " 'similarly p c1 x is the corresponding probability revised using bayes theorem in light of the information contained in the x ray if our aim is to minimize the chance of assigning x to the wrong class then intuitively we would choose the class having the higher posterior probability we now show that this intuition is correct and we also discuss more general criteria for making decisions 1 5 1 minimizing the misclassiÔ¨Åcation rate suppose that our goal is simply to make as few misclassiÔ¨Åcations as possible we need a rule that assigns each value ofx to one of the available classes such a rule will divide the input space into regionsrk called decision regions one for each class such that all points inrk are assigned to classck',\n",
       " 'we need a rule that assigns each value ofx to one of the available classes such a rule will divide the input space into regionsrk called decision regions one for each class such that all points inrk are assigned to classck the boundaries between decision regions are calleddecision boundaries or decision surfaces note that each decision region need not be contiguous but could comprise some number of disjoint regions we shall encounter examples of decision boundaries and decision regions in later chapters in order to Ô¨Ånd the optimal decision rule consider Ô¨Årst of all the case of two classes as in the cancer problem for instance a mistake occurs when an input vector belonging to classc 1 is assigned to classc2 or vice versa',\n",
       " 'in order to Ô¨Ånd the optimal decision rule consider Ô¨Årst of all the case of two classes as in the cancer problem for instance a mistake occurs when an input vector belonging to classc 1 is assigned to classc2 or vice versa the probability of this occurring is given by p mistake p x r 1 c2 p x r 2 c1 r1 p x c2 d x r2 p x c1 d x 1 78 we are free to choose the decision rule that assigns each pointx to one of the two classes clearly to minimizep mistake we should arrange that eachx is assigned to whichever class has the smaller value of the integrand in 1 78 thus ifp x c1 p x c2 for a given value ofx then we should assign thatx to class c1 from the product rule of probability we havep x ck p ck x p x',\n",
       " 'thus ifp x c1 p x c2 for a given value ofx then we should assign thatx to class c1 from the product rule of probability we havep x ck p ck x p x because the factor p x is common to both terms we can restate this result as saying that the minimum 40 1 introduction r1 r2 x0 ÀÜx p x c1 p x c2 x figure 1 24 schematic illustration of the joint probabilities p x ck for each of two classes plotted against x together with the decision boundary x bx values of x bx are classiÔ¨Åed as class c2 and hence belong to decision region r2 whereas points x bx are classiÔ¨Åed as c1 and belong to r1',\n",
       " 'introduction r1 r2 x0 ÀÜx p x c1 p x c2 x figure 1 24 schematic illustration of the joint probabilities p x ck for each of two classes plotted against x together with the decision boundary x bx values of x bx are classiÔ¨Åed as class c2 and hence belong to decision region r2 whereas points x bx are classiÔ¨Åed as c1 and belong to r1 errors arise from the blue green and red regions so that for x bx the errors are due to points from classc2 being misclassiÔ¨Åed as c1 represented by the sum of the red and green regions and conversely for points in the region x bx the errors are due to points from class c1 being misclassiÔ¨Åed as c2 represented by the blue region',\n",
       " 'values of x bx are classiÔ¨Åed as class c2 and hence belong to decision region r2 whereas points x bx are classiÔ¨Åed as c1 and belong to r1 errors arise from the blue green and red regions so that for x bx the errors are due to points from classc2 being misclassiÔ¨Åed as c1 represented by the sum of the red and green regions and conversely for points in the region x bx the errors are due to points from class c1 being misclassiÔ¨Åed as c2 represented by the blue region as we vary the location bx of the decision boundary the combined areas of the blue and green regions remains constant whereas the size of the red region varies the optimal choice for bx is where the curves for p x c1 and p x c2 cross corresponding to bx x0 because in this case the red region disappears',\n",
       " 'as we vary the location bx of the decision boundary the combined areas of the blue and green regions remains constant whereas the size of the red region varies the optimal choice for bx is where the curves for p x c1 and p x c2 cross corresponding to bx x0 because in this case the red region disappears this is equivalent to the minimum misclassiÔ¨Åcation rate decision rule which assigns each value of x to the class having the higher posterior probability p ck x probability of making a mistake is obtained if each value ofx is assigned to the class for which the posterior probabilityp ck x is largest this result is illustrated for two classes and a single input variablex in figure 1 24',\n",
       " 'probability of making a mistake is obtained if each value ofx is assigned to the class for which the posterior probabilityp ck x is largest this result is illustrated for two classes and a single input variablex in figure 1 24 for the more general case of k classes it is slightly easier to maximize the probability of being correct which is given by p correct k k 1 p x r k ck k k 1 rk p x ck d x 1 79 which is maximized when the regionsrk are chosen such that eachx is assigned to the class for whichp x ck is largest again using the product rulep x ck p ck x p x and noting that the factor of p x is common to all terms we see that each x should be assigned to the class having the largest posterior probability p ck x 1 5',\n",
       " 'again using the product rulep x ck p ck x p x and noting that the factor of p x is common to all terms we see that each x should be assigned to the class having the largest posterior probability p ck x 1 5 decision theory 41 figure 1 25 an example of a loss matrix with ele ments lkj for the cancer treatment problem the rows correspond to the true class whereas the columns cor respond to the assignment of class made by our deci sion criterion cancer normal cancer 0 1000 normal 10 1 5 2 minimizing the expected loss for many applications our objective will be more complex than simply mini mizing the number of misclassiÔ¨Åcations let us consider again the medical diagnosis problem',\n",
       " 'cancer normal cancer 0 1000 normal 10 1 5 2 minimizing the expected loss for many applications our objective will be more complex than simply mini mizing the number of misclassiÔ¨Åcations let us consider again the medical diagnosis problem we note that if a patient who does not have cancer is incorrectly diagnosed as having cancer the consequences may be some patient distress plus the need for further investigations conversely if a patient with cancer is diagnosed as healthy the result may be premature death due to lack of treatment thus the consequences of these two types of mistake can be dramatically different it would clearly be better to make fewer mistakes of the second kind even if this was at the expense of making more mistakes of the Ô¨Årst kind',\n",
       " 'thus the consequences of these two types of mistake can be dramatically different it would clearly be better to make fewer mistakes of the second kind even if this was at the expense of making more mistakes of the Ô¨Årst kind we can formalize such issues through the introduction of aloss function also called a cost function which is a single overall measure of loss incurred in taking any of the available decisions or actions our goal is then to minimize the total loss incurred note that some authors consider instead a utility function whose value they aim to maximize these are equivalent concepts if we take the utility to be simply the negative of the loss and throughout this text we shall use the loss function convention',\n",
       " 'note that some authors consider instead a utility function whose value they aim to maximize these are equivalent concepts if we take the utility to be simply the negative of the loss and throughout this text we shall use the loss function convention suppose that for a new value ofx the true class isc k and that we assign x to class cj where j may or may not be equal tok in so doing we incur some level of loss that we denote bylkj which we can view as thek j element of aloss matrix for instance in our cancer example we might have a loss matrix of the form shown in figure 1 25',\n",
       " 'in so doing we incur some level of loss that we denote bylkj which we can view as thek j element of aloss matrix for instance in our cancer example we might have a loss matrix of the form shown in figure 1 25 this particular loss matrix says that there is no loss incurred if the correct decision is made there is a loss of1 if a healthy patient is diagnosed as having cancer whereas there is a loss of1000 if a patient having cancer is diagnosed as healthy the optimal solution is the one which minimizes the loss function however the loss function depends on the true class which is unknown',\n",
       " 'the optimal solution is the one which minimizes the loss function however the loss function depends on the true class which is unknown for a given input vector x our uncertainty in the true class is expressed through the joint probability distribution p x ck and so we seek instead to minimize the average loss where the average is computed with respect to this distribution which is given by e l k j rj lkjp x ck d x 1 80 each x can be assigned independently to one of the decision regionsrj our goal is to choose the regions rj in order to minimize the expected loss 1 80 which implies that for eachx we should minimize k lkjp x ck as before we can use the product rule p x ck p ck x p x to eliminate the common factor ofp x',\n",
       " 'our goal is to choose the regions rj in order to minimize the expected loss 1 80 which implies that for eachx we should minimize k lkjp x ck as before we can use the product rule p x ck p ck x p x to eliminate the common factor ofp x thus the decision rule that minimizes the expected loss is the one that assigns each 42 1 introduction figure 1 26 illustration of the reject option inputs x such that the larger of the two poste rior probabilities is less than or equal to some threshold Œ∏ will be rejected x p c1 x p c2 x 0 0 1 0 Œ∏ reject region new x to the classj for which the quantity k lkjp ck x 1 81 is a minimum this is clearly trivial to do once we know the posterior class proba bilities p ck x',\n",
       " 'x p c1 x p c2 x 0 0 1 0 Œ∏ reject region new x to the classj for which the quantity k lkjp ck x 1 81 is a minimum this is clearly trivial to do once we know the posterior class proba bilities p ck x 1 5 3 the reject option we have seen that classiÔ¨Åcation errors arise from the regions of input space where the largest of the posterior probabilitiesp ck x is signiÔ¨Åcantly less than unity or equivalently where the joint distributionsp x ck have comparable values these are the regions where we are relatively uncertain about class membership in some applications it will be appropriate to avoid making decisions on the difÔ¨Åcult cases in anticipation of a lower error rate on those examples for which a classiÔ¨Åcation de cision is made this is known as thereject option',\n",
       " 'in some applications it will be appropriate to avoid making decisions on the difÔ¨Åcult cases in anticipation of a lower error rate on those examples for which a classiÔ¨Åcation de cision is made this is known as thereject option for example in our hypothetical medical illustration it may be appropriate to use an automatic system to classify those x ray images for which there is little doubt as to the correct class while leav ing a human expert to classify the more ambiguous cases we can achieve this by introducing a threshold Œ∏ and rejecting those inputs x for which the largest of the posterior probabilities p ck x is less than or equal toŒ∏ this is illustrated for the case of two classes and a single continuous input variablex in figure 1 26',\n",
       " 'we can achieve this by introducing a threshold Œ∏ and rejecting those inputs x for which the largest of the posterior probabilities p ck x is less than or equal toŒ∏ this is illustrated for the case of two classes and a single continuous input variablex in figure 1 26 note that setting Œ∏ 1 will ensure that all examples are rejected whereas if there arek classes then setting Œ∏ 1 k will ensure that no examples are rejected thus the fraction of examples that get rejected is controlled by the value ofŒ∏',\n",
       " 'note that setting Œ∏ 1 will ensure that all examples are rejected whereas if there arek classes then setting Œ∏ 1 k will ensure that no examples are rejected thus the fraction of examples that get rejected is controlled by the value ofŒ∏ we can easily extend the reject criterion to minimize the expected loss when a loss matrix is given taking account of the loss incurred when a reject decision is made exercise 1 24 1 5 4 inference and decision we have broken the classiÔ¨Åcation problem down into two separate stages the inference stage in which we use training data to learn a model forp ck x and the 1 5 decision theory 43 subsequent decision stage in which we use these posterior probabilities to make op timal class assignments',\n",
       " 'we can easily extend the reject criterion to minimize the expected loss when a loss matrix is given taking account of the loss incurred when a reject decision is made exercise 1 24 1 5 4 inference and decision we have broken the classiÔ¨Åcation problem down into two separate stages the inference stage in which we use training data to learn a model forp ck x and the 1 5 decision theory 43 subsequent decision stage in which we use these posterior probabilities to make op timal class assignments an alternative possibility would be to solve both problems together and simply learn a function that maps inputsx directly into decisions such a function is called adiscriminant function',\n",
       " 'an alternative possibility would be to solve both problems together and simply learn a function that maps inputsx directly into decisions such a function is called adiscriminant function in fact we can identify three distinct approaches to solving decision problems all of which have been used in practical applications these are given in decreasing order of complexity by a first solve the inference problem of determining the class conditional densities p x c k for each class ck individually also separately infer the prior class probabilities p ck then use bayes theorem in the form p ck x p x ck p ck p x 1 82 to Ô¨Ånd the posterior class probabilities p ck x',\n",
       " 'also separately infer the prior class probabilities p ck then use bayes theorem in the form p ck x p x ck p ck p x 1 82 to Ô¨Ånd the posterior class probabilities p ck x as usual the denominator in bayes theorem can be found in terms of the quantities appearing in the numerator because p x k p x ck p ck 1 83 equivalently we can model the joint distributionp x ck directly and then normalize to obtain the posterior probabilities having found the posterior probabilities we use decision theory to determine class membership for each new inputx approaches that explicitly or implicitly model the distribution of inputs as well as outputs are known asgenerative models because by sampling from them it is possible to generate synthetic data points in the input space',\n",
       " 'having found the posterior probabilities we use decision theory to determine class membership for each new inputx approaches that explicitly or implicitly model the distribution of inputs as well as outputs are known asgenerative models because by sampling from them it is possible to generate synthetic data points in the input space b first solve the inference problem of determining the posterior class probabilities p ck x and then subsequently use decision theory to assign each newx to one of the classes approaches that model the posterior probabilities directly are called discriminative models c find a function f x called a discriminant function which maps each inputx directly onto a class label',\n",
       " 'approaches that model the posterior probabilities directly are called discriminative models c find a function f x called a discriminant function which maps each inputx directly onto a class label for instance in the case of two class problems f might be binary valued and such thatf 0 represents classc1 and f 1 represents class c2 in this case probabilities play no role let us consider the relative merits of these three alternatives approach a is the most demanding because it involves Ô¨Ånding the joint distribution over bothx and ck for many applications x will have high dimensionality and consequently we may need a large training set in order to be able to determine the class conditional densities to reasonable accuracy',\n",
       " 'approach a is the most demanding because it involves Ô¨Ånding the joint distribution over bothx and ck for many applications x will have high dimensionality and consequently we may need a large training set in order to be able to determine the class conditional densities to reasonable accuracy note that the class priorsp c k can often be esti mated simply from the fractions of the training set data points in each of the classes one advantage of approach a however is that it also allows the marginal density of datap x to be determined from 1 83 this can be useful for detecting new data points that have low probability under the model and for which the predictions may 44 1',\n",
       " 'one advantage of approach a however is that it also allows the marginal density of datap x to be determined from 1 83 this can be useful for detecting new data points that have low probability under the model and for which the predictions may 44 1 introduction p x c1 p x c2 x class densities 0 0 2 0 4 0 6 0 8 1 0 1 2 3 4 5 x p c1 x p c2 x 0 0 2 0 4 0 6 0 8 1 0 0 2 0 4 0 6 0 8 1 1 2 figure 1 27 example of the class conditional densities for two classes having a single input variable x left plot together with the corresponding posterior probabilities right plot note that the left hand mode of the class conditional density p x c1 shown in blue on the left plot has no effect on the posterior probabilities',\n",
       " 'introduction p x c1 p x c2 x class densities 0 0 2 0 4 0 6 0 8 1 0 1 2 3 4 5 x p c1 x p c2 x 0 0 2 0 4 0 6 0 8 1 0 0 2 0 4 0 6 0 8 1 1 2 figure 1 27 example of the class conditional densities for two classes having a single input variable x left plot together with the corresponding posterior probabilities right plot note that the left hand mode of the class conditional density p x c1 shown in blue on the left plot has no effect on the posterior probabilities the vertical green line in the right plot shows the decision boundary in x that gives the minimum misclassiÔ¨Åcation rate be of low accuracy which is known asoutlier detectionor novelty detection bishop 1994 tarassenko 1995',\n",
       " 'the vertical green line in the right plot shows the decision boundary in x that gives the minimum misclassiÔ¨Åcation rate be of low accuracy which is known asoutlier detectionor novelty detection bishop 1994 tarassenko 1995 however if we only wish to make classiÔ¨Åcation decisions then it can be waste ful of computational resources and excessively demanding of data to Ô¨Ånd the joint distribution p x ck when in fact we only really need the posterior probabilities p ck x which can be obtained directly through approach b indeed the class conditional densities may contain a lot of structure that has little effect on the pos terior probabilities as illustrated in figure 1 27',\n",
       " 'however if we only wish to make classiÔ¨Åcation decisions then it can be waste ful of computational resources and excessively demanding of data to Ô¨Ånd the joint distribution p x ck when in fact we only really need the posterior probabilities p ck x which can be obtained directly through approach b indeed the class conditional densities may contain a lot of structure that has little effect on the pos terior probabilities as illustrated in figure 1 27 there has been much interest in exploring the relative merits of generative and discriminative approaches to machine learning and in Ô¨Ånding ways to combine them jebara 2004 lasserreet al 2006',\n",
       " 'indeed the class conditional densities may contain a lot of structure that has little effect on the pos terior probabilities as illustrated in figure 1 27 there has been much interest in exploring the relative merits of generative and discriminative approaches to machine learning and in Ô¨Ånding ways to combine them jebara 2004 lasserreet al 2006 an even simpler approach is c in which we use the training data to Ô¨Ånd a discriminant function f x that maps each x directly onto a class label thereby combining the inference and decision stages into a single learning problem in the example of figure 1 27 this would correspond to Ô¨Ånding the value ofx shown by the vertical green line because this is the decision boundary giving the minimum probability of misclassiÔ¨Åcation',\n",
       " 'an even simpler approach is c in which we use the training data to Ô¨Ånd a discriminant function f x that maps each x directly onto a class label thereby combining the inference and decision stages into a single learning problem in the example of figure 1 27 this would correspond to Ô¨Ånding the value ofx shown by the vertical green line because this is the decision boundary giving the minimum probability of misclassiÔ¨Åcation with option c however we no longer have access to the posterior probabilities p ck x there are many powerful reasons for wanting to compute the posterior probabilities even if we subsequently use them to make decisions these include minimizing risk',\n",
       " 'there are many powerful reasons for wanting to compute the posterior probabilities even if we subsequently use them to make decisions these include minimizing risk consider a problem in which the elements of the loss matrix are subjected to revision from time to time such as might occur in a Ô¨Ånancial 1 5 decision theory 45 application if we know the posterior probabilities we can trivially revise the minimum risk decision criterion by modifying 1 81 appropriately if we have only a discriminant function then any change to the loss matrix would require that we return to the training data and solve the classiÔ¨Åcation problem afresh reject option',\n",
       " 'if we have only a discriminant function then any change to the loss matrix would require that we return to the training data and solve the classiÔ¨Åcation problem afresh reject option posterior probabilities allow us to determine a rejection criterion that will minimize the misclassiÔ¨Åcation rate or more generally the expected loss for a given fraction of rejected data points compensating for class priors consider our medical x ray problem again and suppose that we have collected a large number of x ray images from the gen eral population for use as training data in order to build an automated screening system because cancer is rare amongst the general population we might Ô¨Ånd that say only 1 in every 1 000 examples corresponds to the presence of can cer',\n",
       " 'compensating for class priors consider our medical x ray problem again and suppose that we have collected a large number of x ray images from the gen eral population for use as training data in order to build an automated screening system because cancer is rare amongst the general population we might Ô¨Ånd that say only 1 in every 1 000 examples corresponds to the presence of can cer if we used such a data set to train an adaptive model we could run into severe difÔ¨Åculties due to the small proportion of the cancer class for instance a classiÔ¨Åer that assigned every point to the normal class would already achieve 99 9 accuracy and it would be difÔ¨Åcult to avoid this trivial solution',\n",
       " 'if we used such a data set to train an adaptive model we could run into severe difÔ¨Åculties due to the small proportion of the cancer class for instance a classiÔ¨Åer that assigned every point to the normal class would already achieve 99 9 accuracy and it would be difÔ¨Åcult to avoid this trivial solution also even a large data set will contain very few examples of x ray images corre sponding to cancer and so the learning algorithm will not be exposed to a broad range of examples of such images and hence is not likely to generalize well a balanced data set in which we have selected equal numbers of exam ples from each of the classes would allow us to Ô¨Ånd a more accurate model however we then have to compensate for the effects of our modiÔ¨Åcations to the training data',\n",
       " 'a balanced data set in which we have selected equal numbers of exam ples from each of the classes would allow us to Ô¨Ånd a more accurate model however we then have to compensate for the effects of our modiÔ¨Åcations to the training data suppose we have used such a modiÔ¨Åed data set and found models for the posterior probabilities from bayes theorem 1 82 we see that the posterior probabilities are proportional to the prior probabilities which we can interpret as the fractions of points in each class we can therefore simply take the posterior probabilities obtained from our artiÔ¨Åcially balanced data set and Ô¨Årst divide by the class fractions in that data set and then multiply by the class fractions in the population to which we wish to apply the model',\n",
       " 'from bayes theorem 1 82 we see that the posterior probabilities are proportional to the prior probabilities which we can interpret as the fractions of points in each class we can therefore simply take the posterior probabilities obtained from our artiÔ¨Åcially balanced data set and Ô¨Årst divide by the class fractions in that data set and then multiply by the class fractions in the population to which we wish to apply the model finally we need to normalize to ensure that the new posterior probabilities sum to one note that this procedure cannot be applied if we have learned a discriminant function directly instead of determining posterior probabilities combining models',\n",
       " 'note that this procedure cannot be applied if we have learned a discriminant function directly instead of determining posterior probabilities combining models for complex applications we may wish to break the problem into a number of smaller subproblems each of which can be tackled by a sep arate module for example in our hypothetical medical diagnosis problem we may have information available from say blood tests as well as x ray im ages rather than combine all of this heterogeneous information into one huge input space it may be more effective to build one system to interpret the x ray images and a different one to interpret the blood data',\n",
       " 'for example in our hypothetical medical diagnosis problem we may have information available from say blood tests as well as x ray im ages rather than combine all of this heterogeneous information into one huge input space it may be more effective to build one system to interpret the x ray images and a different one to interpret the blood data as long as each of the two models gives posterior probabilities for the classes we can combine the outputs systematically using the rules of probability one simple way to do this is to assume that for each class separately the distributions of inputs for the x ray images denoted byx i and the blood data denoted byxb a r e 46 1 introduction independent so that p xi xb ck p xi ck p xb ck',\n",
       " 'one simple way to do this is to assume that for each class separately the distributions of inputs for the x ray images denoted byx i and the blood data denoted byxb a r e 46 1 introduction independent so that p xi xb ck p xi ck p xb ck 1 84 this is an example ofconditional independenceproperty because the indepen section 8 2 dence holds when the distribution is conditioned on the classck the posterior probability given both the x ray and blood data is then given by p ck xi xb p xi xb ck p ck p xi ck p xb ck p ck p ck xi p ck xb p ck 1 85 thus we need the class prior probabilitiesp ck which we can easily estimate from the fractions of data points in each class and then we need to normalize the resulting posterior probabilities so they sum to one',\n",
       " '1 84 this is an example ofconditional independenceproperty because the indepen section 8 2 dence holds when the distribution is conditioned on the classck the posterior probability given both the x ray and blood data is then given by p ck xi xb p xi xb ck p ck p xi ck p xb ck p ck p ck xi p ck xb p ck 1 85 thus we need the class prior probabilitiesp ck which we can easily estimate from the fractions of data points in each class and then we need to normalize the resulting posterior probabilities so they sum to one the particular condi tional independence assumption 1 84 is an example of thenaive bayes model section 8 2 2 note that the joint marginal distributionp x i xb will typically not factorize under this model',\n",
       " 'the posterior probability given both the x ray and blood data is then given by p ck xi xb p xi xb ck p ck p xi ck p xb ck p ck p ck xi p ck xb p ck 1 85 thus we need the class prior probabilitiesp ck which we can easily estimate from the fractions of data points in each class and then we need to normalize the resulting posterior probabilities so they sum to one the particular condi tional independence assumption 1 84 is an example of thenaive bayes model section 8 2 2 note that the joint marginal distributionp x i xb will typically not factorize under this model we shall see in later chapters how to construct models for combining data that do not require the conditional independence assumption 1 84',\n",
       " 'the particular condi tional independence assumption 1 84 is an example of thenaive bayes model section 8 2 2 note that the joint marginal distributionp x i xb will typically not factorize under this model we shall see in later chapters how to construct models for combining data that do not require the conditional independence assumption 1 84 1 5 5 loss functions for regression so far we have discussed decision theory in the context of classiÔ¨Åcation prob lems we now turn to the case of regression problems such as the curve Ô¨Åtting example discussed earlier the decision stage consists of choosing a speciÔ¨Åc esti section 1 1 mate y x of the value oft for each input x suppose that in doing so we incur a loss l t y x',\n",
       " 'the decision stage consists of choosing a speciÔ¨Åc esti section 1 1 mate y x of the value oft for each input x suppose that in doing so we incur a loss l t y x the average or expected loss is then given by e l l t y x p x t d xdt 1 86 a common choice of loss function in regression problems is the squared loss given by l t y x y x t 2 in this case the expected loss can be written e l y x t 2p x t d xdt 1 87 our goal is to choose y x so as to minimize e l if we assume a completely Ô¨Çexible function y x we can do this formally using the calculus of variations toappendix d give Œ¥e l Œ¥y x 2 y x t p x t d t 0',\n",
       " '1 87 our goal is to choose y x so as to minimize e l if we assume a completely Ô¨Çexible function y x we can do this formally using the calculus of variations toappendix d give Œ¥e l Œ¥y x 2 y x t p x t d t 0 1 88 solving for y x and using the sum and product rules of probability we obtain y x tp x t d t p x tp t x d t et t x 1 89 1 5 decision theory 47 figure 1 28 the regression function y x which minimizes the expected squared loss is given by the mean of the conditional distri bution p t x t xx0 y x0 y x p t x0 which is the conditional average oft conditioned onx and is known as theregression function this result is illustrated in figure 1 28',\n",
       " 't xx0 y x0 y x p t x0 which is the conditional average oft conditioned onx and is known as theregression function this result is illustrated in figure 1 28 it can readily be extended to mul tiple target variables represented by the vectort in which case the optimal solution is the conditional averagey x et t x exercise 1 25 we can also derive this result in a slightly different way which will also shed light on the nature of the regression problem armed with the knowledge that the optimal solution is the conditional expectation we can expand the square term as follows y x t 2 y x e t x e t x t 2 y x e t x 2 2 y x e t x e t x t e t x t 2 where to keep the notation uncluttered we usee t x to denoteet t x',\n",
       " 'it can readily be extended to mul tiple target variables represented by the vectort in which case the optimal solution is the conditional averagey x et t x exercise 1 25 we can also derive this result in a slightly different way which will also shed light on the nature of the regression problem armed with the knowledge that the optimal solution is the conditional expectation we can expand the square term as follows y x t 2 y x e t x e t x t 2 y x e t x 2 2 y x e t x e t x t e t x t 2 where to keep the notation uncluttered we usee t x to denoteet t x substituting into the loss function and performing the integral overt we see that the cross term vanishes and we obtain an expression for the loss function in the form e l y x e t x 2 p x dx e t x t 2p x dx',\n",
       " 'armed with the knowledge that the optimal solution is the conditional expectation we can expand the square term as follows y x t 2 y x e t x e t x t 2 y x e t x 2 2 y x e t x e t x t e t x t 2 where to keep the notation uncluttered we usee t x to denoteet t x substituting into the loss function and performing the integral overt we see that the cross term vanishes and we obtain an expression for the loss function in the form e l y x e t x 2 p x dx e t x t 2p x dx 1 90 the function y x we seek to determine enters only in the Ô¨Årst term which will be minimized when y x is equal to e t x in which case this term will vanish',\n",
       " 'substituting into the loss function and performing the integral overt we see that the cross term vanishes and we obtain an expression for the loss function in the form e l y x e t x 2 p x dx e t x t 2p x dx 1 90 the function y x we seek to determine enters only in the Ô¨Årst term which will be minimized when y x is equal to e t x in which case this term will vanish this is simply the result that we derived previously and that shows that the optimal least squares predictor is given by the conditional mean the second term is the variance of the distribution of t averaged over x it represents the intrinsic variability of the target data and can be regarded as noise because it is independent ofy x i t represents the irreducible minimum value of the loss function',\n",
       " 'it represents the intrinsic variability of the target data and can be regarded as noise because it is independent ofy x i t represents the irreducible minimum value of the loss function as with the classiÔ¨Åcation problem we can either determine the appropriate prob abilities and then use these to make optimal decisions or we can build models that make decisions directly indeed we can identify three distinct approaches to solving regression problems given in order of decreasing complexity by a first solve the inference problem of determining the joint densityp x t then normalize to Ô¨Ånd the conditional densityp t x and Ô¨Ånally marginalize to Ô¨Ånd the conditional mean given by 1 89 48 1',\n",
       " 'then normalize to Ô¨Ånd the conditional densityp t x and Ô¨Ånally marginalize to Ô¨Ånd the conditional mean given by 1 89 48 1 introduction b first solve the inference problem of determining the conditional densityp t x and then subsequently marginalize to Ô¨Ånd the conditional mean given by 1 89 c find a regression functiony x directly from the training data the relative merits of these three approaches follow the same lines as for classiÔ¨Åca tion problems above the squared loss is not the only possible choice of loss function for regression indeed there are situations in which squared loss can lead to very poor results and where we need to develop more sophisticated approaches',\n",
       " 'the squared loss is not the only possible choice of loss function for regression indeed there are situations in which squared loss can lead to very poor results and where we need to develop more sophisticated approaches an important example concerns situations in which the conditional distribution p t x is multimodal as often arises in the solution of inverse problems here we consider brieÔ¨Çy one simplesection 5 6 generalization of the squared loss called theminkowski loss whose expectation is given by e lq y x t qp x t d xdt 1 91 which reduces to the expected squared loss for q 2 the function y t q is plotted against y t for various values ofq in figure 1 29',\n",
       " 'here we consider brieÔ¨Çy one simplesection 5 6 generalization of the squared loss called theminkowski loss whose expectation is given by e lq y x t qp x t d xdt 1 91 which reduces to the expected squared loss for q 2 the function y t q is plotted against y t for various values ofq in figure 1 29 the minimum ofe lq is given by the conditional mean forq 2 the conditional median forq 1 and the conditional mode forq 0 exercise 1 27 1 6 information theory in this chapter we have discussed a variety of concepts from probability theory and decision theory that will form the foundations for much of the subsequent discussion in this book',\n",
       " 'the minimum ofe lq is given by the conditional mean forq 2 the conditional median forq 1 and the conditional mode forq 0 exercise 1 27 1 6 information theory in this chapter we have discussed a variety of concepts from probability theory and decision theory that will form the foundations for much of the subsequent discussion in this book we close this chapter by introducing some additional concepts from the Ô¨Åeld of information theory which will also prove useful in our development of pattern recognition and machine learning techniques again we shall focus only on the key concepts and we refer the reader elsewhere for more detailed discussions viterbi and omura 1979 cover and thomas 1991 mackay 2003',\n",
       " 'we close this chapter by introducing some additional concepts from the Ô¨Åeld of information theory which will also prove useful in our development of pattern recognition and machine learning techniques again we shall focus only on the key concepts and we refer the reader elsewhere for more detailed discussions viterbi and omura 1979 cover and thomas 1991 mackay 2003 we begin by considering a discrete random variablex and we ask how much information is received when we observe a speciÔ¨Åc value for this variable the amount of information can be viewed as the degree of surprise on learning the value of x',\n",
       " 'we begin by considering a discrete random variablex and we ask how much information is received when we observe a speciÔ¨Åc value for this variable the amount of information can be viewed as the degree of surprise on learning the value of x if we are told that a highly improbable event has just occurred we will have received more information than if we were told that some very likely event has just occurred and if we knew that the event was certain to happen we would receive no information our measure of information content will therefore depend on the probability distributionp x and we therefore look for a quantityh x that is a monotonic function of the probabilityp x and that expresses the information content',\n",
       " 'if we are told that a highly improbable event has just occurred we will have received more information than if we were told that some very likely event has just occurred and if we knew that the event was certain to happen we would receive no information our measure of information content will therefore depend on the probability distributionp x and we therefore look for a quantityh x that is a monotonic function of the probabilityp x and that expresses the information content the form of h can be found by noting that if we have two events x and y that are unrelated then the information gain from observing both of them should be the sum of the information gained from each of them separately so that h x y h x h y',\n",
       " 'our measure of information content will therefore depend on the probability distributionp x and we therefore look for a quantityh x that is a monotonic function of the probabilityp x and that expresses the information content the form of h can be found by noting that if we have two events x and y that are unrelated then the information gain from observing both of them should be the sum of the information gained from each of them separately so that h x y h x h y two unrelated events will be statistically independent and so p x y p x p y from these two relationships it is easily shown thath x must be given by the logarithm ofp x and so we haveexercise 1 28 1 6',\n",
       " 'two unrelated events will be statistically independent and so p x y p x p y from these two relationships it is easily shown thath x must be given by the logarithm ofp x and so we haveexercise 1 28 1 6 information theory 49 y t y t q q 0 3 2 1 0 1 2 0 1 2 y t y t q q 1 2 1 0 1 2 0 1 2 y t y t q q 2 2 1 0 1 2 0 1 2 y t y t q q 1 0 2 1 0 1 2 0 1 2 figure 1 29 plots of the quantity lq y t q for various values of q h x log2 p x 1 92 where the negative sign ensures that information is positive or zero note that low probability events x correspond to high information content the choice of basis for the logarithm is arbitrary and for the moment we shall adopt the convention prevalent in information theory of using logarithms to the base of2',\n",
       " 'note that low probability events x correspond to high information content the choice of basis for the logarithm is arbitrary and for the moment we shall adopt the convention prevalent in information theory of using logarithms to the base of2 in this case as we shall see shortly the units ofh x are bits binary digits now suppose that a sender wishes to transmit the value of a random variable to a receiver the average amount of information that they transmit in the process is obtained by taking the expectation of 1 92 with respect to the distributionp x and is given by h x x p x log2 p x 1 93 this important quantity is called theentropy of the random variable x',\n",
       " 'the average amount of information that they transmit in the process is obtained by taking the expectation of 1 92 with respect to the distributionp x and is given by h x x p x log2 p x 1 93 this important quantity is called theentropy of the random variable x note that limp 0 p lnp 0 and so we shall takep x l np x 0 whenever we encounter a value forx such that p x 0 so far we have given a rather heuristic motivation for the deÔ¨Ånition of informa 50 1 introduction tion 1 92 and the corresponding entropy 1 93 we now show that these deÔ¨Ånitions indeed possess useful properties consider a random variablex having 8 possible states each of which is equally likely in order to communicate the value ofx to a receiver we would need to transmit a message of length 3 bits',\n",
       " 'consider a random variablex having 8 possible states each of which is equally likely in order to communicate the value ofx to a receiver we would need to transmit a message of length 3 bits notice that the entropy of this variable is given by h x 8 1 8 log2 1 8 3 bits now consider an example cover and thomas 1991 of a variable having8 pos sible states a b c d e f g h for which the respective probabilities are given by 1 2 1 4 1 8 1 16 1 64 1 64 1 64 1 64 the entropy in this case is given by h x 1 2 log2 1 2 1 4 log2 1 4 1 8 log2 1 8 1 16 log2 1 16 4 64 log2 1 64 2 bits',\n",
       " 'now consider an example cover and thomas 1991 of a variable having8 pos sible states a b c d e f g h for which the respective probabilities are given by 1 2 1 4 1 8 1 16 1 64 1 64 1 64 1 64 the entropy in this case is given by h x 1 2 log2 1 2 1 4 log2 1 4 1 8 log2 1 8 1 16 log2 1 16 4 64 log2 1 64 2 bits we see that the nonuniform distribution has a smaller entropy than the uniform one and we shall gain some insight into this shortly when we discuss the interpretation of entropy in terms of disorder for the moment let us consider how we would transmit the identity of the variable s state to a receiver we could do this as before using a 3 bit number',\n",
       " 'for the moment let us consider how we would transmit the identity of the variable s state to a receiver we could do this as before using a 3 bit number however we can take advantage of the nonuniform distribution by using shorter codes for the more probable events at the expense of longer codes for the less probable events in the hope of getting a shorter average code length this can be done by representing the states a b c d e f g h using for instance the following set of code strings 0 10 110 1110 111100 111101 111110 111111 the average length of the code that has to be transmitted is then average code length 1 2 1 1 4 2 1 8 3 1 16 4 4 1 64 6 2 bits which again is the same as the entropy of the random variable',\n",
       " 'this can be done by representing the states a b c d e f g h using for instance the following set of code strings 0 10 110 1110 111100 111101 111110 111111 the average length of the code that has to be transmitted is then average code length 1 2 1 1 4 2 1 8 3 1 16 4 4 1 64 6 2 bits which again is the same as the entropy of the random variable note that shorter code strings cannot be used because it must be possible to disambiguate a concatenation of such strings into its component parts for instance 11001110 decodes uniquely into the state sequencec a d this relation between entropy and shortest coding length is a general one',\n",
       " 'note that shorter code strings cannot be used because it must be possible to disambiguate a concatenation of such strings into its component parts for instance 11001110 decodes uniquely into the state sequencec a d this relation between entropy and shortest coding length is a general one the noiseless coding theorem shannon 1948 states that the entropy is a lower bound on the number of bits needed to transmit the state of a random variable from now on we shall switch to the use of natural logarithms in deÔ¨Åning en tropy as this will provide a more convenient link with ideas elsewhere in this book in this case the entropy is measured in units of nats instead of bits which differ simply by a factor ofln 2',\n",
       " 'from now on we shall switch to the use of natural logarithms in deÔ¨Åning en tropy as this will provide a more convenient link with ideas elsewhere in this book in this case the entropy is measured in units of nats instead of bits which differ simply by a factor ofln 2 we have introduced the concept of entropy in terms of the average amount of information needed to specify the state of a random variable in fact the concept of entropy has much earlier origins in physics where it was introduced in the context of equilibrium thermodynamics and later given a deeper interpretation as a measure of disorder through developments in statistical mechanics',\n",
       " 'we have introduced the concept of entropy in terms of the average amount of information needed to specify the state of a random variable in fact the concept of entropy has much earlier origins in physics where it was introduced in the context of equilibrium thermodynamics and later given a deeper interpretation as a measure of disorder through developments in statistical mechanics we can understand this alternative view of entropy by considering a set ofn identical objects that are to be divided amongst a set of bins such that there aren i objects in theith bin consider 1 6 information theory 51 the number of different ways of allocating the objects to the bins there are n ways to choose the Ô¨Årst object n 1 ways to choose the second object and so on leading to a total ofn',\n",
       " 'information theory 51 the number of different ways of allocating the objects to the bins there are n ways to choose the Ô¨Årst object n 1 ways to choose the second object and so on leading to a total ofn ways to allocate alln objects to the bins wheren pronounced factorialn denotes the productn n 1 2 1 however we don t wish to distinguish between rearrangements of objects within each bin in the i th bin there are ni ways of reordering the objects and so the total number of ways of allocating then objects to the bins is given by w n i ni 1 94 which is called themultiplicity the entropy is then deÔ¨Åned as the logarithm of the multiplicity scaled by an appropriate constant h 1 n lnw 1 n lnn 1 n i lnni',\n",
       " 'the entropy is then deÔ¨Åned as the logarithm of the multiplicity scaled by an appropriate constant h 1 n lnw 1 n lnn 1 n i lnni 1 95 we now consider the limitn in which the fractionsni n are held Ô¨Åxed and apply stirling s approximation lnn n lnn n 1 96 which gives h lim n i ni n ln ni n i pi lnpi 1 97 where we have used i ni n here pi limn ni n is the probability of an object being assigned to theith bin in physics terminology the speciÔ¨Åc ar rangements of objects in the bins is called amicrostate and the overall distribution of occupation numbers expressed through the ratiosni n is called amacrostate the multiplicity w is also known as theweight of the macrostate',\n",
       " 'in physics terminology the speciÔ¨Åc ar rangements of objects in the bins is called amicrostate and the overall distribution of occupation numbers expressed through the ratiosni n is called amacrostate the multiplicity w is also known as theweight of the macrostate we can interpret the bins as the statesxi of a discrete random variablex where p x xi pi the entropy of the random variablex is then h p i p xi l np xi 1 98 distributions p xi that are sharply peaked around a few values will have a relatively low entropy whereas those that are spread more evenly across many values will have higher entropy as illustrated in figure 1 30 because0 pi 1 the entropy is nonnegative and it will equal its minimum value of 0 when one of the pi 1 and all other pj i 0',\n",
       " '1 98 distributions p xi that are sharply peaked around a few values will have a relatively low entropy whereas those that are spread more evenly across many values will have higher entropy as illustrated in figure 1 30 because0 pi 1 the entropy is nonnegative and it will equal its minimum value of 0 when one of the pi 1 and all other pj i 0 the maximum entropy conÔ¨Åguration can be found by maximizing h using a lagrange multiplier to enforce the normalization constraintappendix e on the probabilities thus we maximize h i p xi l np xi Œª i p xi 1 1 99 52 1',\n",
       " 'the maximum entropy conÔ¨Åguration can be found by maximizing h using a lagrange multiplier to enforce the normalization constraintappendix e on the probabilities thus we maximize h i p xi l np xi Œª i p xi 1 1 99 52 1 introduction probabilities h 1 77 0 0 25 0 5 probabilities h 3 09 0 0 25 0 5 figure 1 30 histograms of two probability distributions over 30 bins illustrating the higher value of the entropy h for the broader distribution the largest entropy would arise from a uniform distribution that would give h ln 1 30 3 40 from which we Ô¨Ånd that all of thep xi are equal and are given byp xi 1 m where m is the total number of statesxi',\n",
       " 'introduction probabilities h 1 77 0 0 25 0 5 probabilities h 3 09 0 0 25 0 5 figure 1 30 histograms of two probability distributions over 30 bins illustrating the higher value of the entropy h for the broader distribution the largest entropy would arise from a uniform distribution that would give h ln 1 30 3 40 from which we Ô¨Ånd that all of thep xi are equal and are given byp xi 1 m where m is the total number of statesxi the corresponding value of the entropy is then h l n m this result can also be derived from jensen s inequality to be discussed shortly',\n",
       " 'the largest entropy would arise from a uniform distribution that would give h ln 1 30 3 40 from which we Ô¨Ånd that all of thep xi are equal and are given byp xi 1 m where m is the total number of statesxi the corresponding value of the entropy is then h l n m this result can also be derived from jensen s inequality to be discussed shortly to verify that the stationary point is indeed a maximum we canexercise 1 29 evaluate the second derivative of the entropy which gives h p xi p xj iij 1 pi 1 100 where iij are the elements of the identity matrix we can extend the deÔ¨Ånition of entropy to include distributionsp x over con tinuous variables x as follows first dividex into bins of width',\n",
       " 'we can extend the deÔ¨Ånition of entropy to include distributionsp x over con tinuous variables x as follows first dividex into bins of width then assuming p x is continuous themean value theorem weisstein 1999 tells us that for each such bin there must exist a valuexi such that i 1 i p x d x p xi 1 101 we can now quantize the continuous variablex by assigning any valuex to the value xi whenever x falls in theith bin the probability of observing the valuexi is then p xi this gives a discrete distribution for which the entropy takes the form h i p xi ln p xi i p xi lnp xi ln 1 102 where we have used i p xi 1 which follows from 1 101 we now omit the second term ln on the right hand side of 1 102 and then consider the limit 1 6',\n",
       " 'this gives a discrete distribution for which the entropy takes the form h i p xi ln p xi i p xi lnp xi ln 1 102 where we have used i p xi 1 which follows from 1 101 we now omit the second term ln on the right hand side of 1 102 and then consider the limit 1 6 information theory 53 0 the Ô¨Årst term on the right hand side of 1 102 will approach the integral of p x l np x in this limit so that lim 0 i p xi lnp xi p x l np x d x 1 103 where the quantity on the right hand side is called thedifferential entropy we see that the discrete and continuous forms of the entropy differ by a quantityln which diverges in the limit 0',\n",
       " 'the Ô¨Årst term on the right hand side of 1 102 will approach the integral of p x l np x in this limit so that lim 0 i p xi lnp xi p x l np x d x 1 103 where the quantity on the right hand side is called thedifferential entropy we see that the discrete and continuous forms of the entropy differ by a quantityln which diverges in the limit 0 this reÔ¨Çects the fact that to specify a continuous variable very precisely requires a large number of bits for a density deÔ¨Åned over multiple continuous variables denoted collectively by the vectorx the differential entropy is given by h x p x l np x dx',\n",
       " 'this reÔ¨Çects the fact that to specify a continuous variable very precisely requires a large number of bits for a density deÔ¨Åned over multiple continuous variables denoted collectively by the vectorx the differential entropy is given by h x p x l np x dx 1 104 in the case of discrete distributions we saw that the maximum entropy con Ô¨Åguration corresponded to an equal distribution of probabilities across the possible states of the variable let us now consider the maximum entropy conÔ¨Åguration for a continuous variable in order for this maximum to be well deÔ¨Åned it will be nec essary to constrain the Ô¨Årst and second moments ofp x as well as preserving the normalization constraint',\n",
       " 'let us now consider the maximum entropy conÔ¨Åguration for a continuous variable in order for this maximum to be well deÔ¨Åned it will be nec essary to constrain the Ô¨Årst and second moments ofp x as well as preserving the normalization constraint we therefore maximize the differential entropy with the ludwig boltzmann 1844 1906 ludwig eduard boltzmann was an austrian physicist who created the Ô¨Åeld of statistical mechanics prior to boltzmann the concept of en tropy was already known from classical thermodynamics where it quantiÔ¨Åes the fact that when we take energy from a system not all of that energy is typically available to do useful work',\n",
       " 'we therefore maximize the differential entropy with the ludwig boltzmann 1844 1906 ludwig eduard boltzmann was an austrian physicist who created the Ô¨Åeld of statistical mechanics prior to boltzmann the concept of en tropy was already known from classical thermodynamics where it quantiÔ¨Åes the fact that when we take energy from a system not all of that energy is typically available to do useful work boltzmann showed that the ther modynamic entropy s a macroscopic quantity could be related to the statistical properties at the micro scopic level this is expressed through the famous equation s k ln w in which w represents the number of possible microstates in a macrostate and k 1 38 10 23 in units of joules per kelvin is known as boltzmann s constant',\n",
       " 'boltzmann showed that the ther modynamic entropy s a macroscopic quantity could be related to the statistical properties at the micro scopic level this is expressed through the famous equation s k ln w in which w represents the number of possible microstates in a macrostate and k 1 38 10 23 in units of joules per kelvin is known as boltzmann s constant boltzmann s ideas were disputed by many scientists of they day one dif Ô¨Åculty they saw arose from the second law of thermo dynamics which states that the entropy of a closed system tends to increase with time by contrast at the microscopic level the classical newtonian equa tions of physics are reversible and so they found it difÔ¨Åcult to see how the latter could explain the for mer',\n",
       " 'one dif Ô¨Åculty they saw arose from the second law of thermo dynamics which states that the entropy of a closed system tends to increase with time by contrast at the microscopic level the classical newtonian equa tions of physics are reversible and so they found it difÔ¨Åcult to see how the latter could explain the for mer they didn t fully appreciate boltzmann s argu ments which were statistical in nature and which con cluded not that entropy could never decrease over time but simply that with overwhelming probability it would generally increase boltzmann even had a long running dispute with the editor of the leading german physics journal who refused to let him refer to atoms and molecules as anything other than convenient the oretical constructs',\n",
       " 'they didn t fully appreciate boltzmann s argu ments which were statistical in nature and which con cluded not that entropy could never decrease over time but simply that with overwhelming probability it would generally increase boltzmann even had a long running dispute with the editor of the leading german physics journal who refused to let him refer to atoms and molecules as anything other than convenient the oretical constructs the continued attacks on his work lead to bouts of depression and eventually he com mitted suicide shortly after boltzmann s death new experiments by perrin on colloidal suspensions veri Ô¨Åed his theories and conÔ¨Årmed the value of the boltz mann constant the equation s k ln w is carved on boltzmann s tombstone 54 1',\n",
       " 'the equation s k ln w is carved on boltzmann s tombstone 54 1 introduction three constraints p x d x 1 1 105 xp x d x ¬µ 1 106 x ¬µ 2p x d x œÉ2 1 107 the constrained maximization can be performed using lagrange multipliers so thatappendix e we maximize the following functional with respect top x p x l np x d x Œª1 p x d x 1 Œª2 xp x d x ¬µ Œª3 x ¬µ 2p x d x œÉ2 using the calculus of variations we set the derivative of this functional to zero givingappendix d p x e x p 1 Œª1 Œª2x Œª3 x ¬µ 2',\n",
       " '1 107 the constrained maximization can be performed using lagrange multipliers so thatappendix e we maximize the following functional with respect top x p x l np x d x Œª1 p x d x 1 Œª2 xp x d x ¬µ Œª3 x ¬µ 2p x d x œÉ2 using the calculus of variations we set the derivative of this functional to zero givingappendix d p x e x p 1 Œª1 Œª2x Œª3 x ¬µ 2 1 108 the lagrange multipliers can be found by back substitution of this result into the three constraint equations leading Ô¨Ånally to the resultexercise 1 34 p x 1 2œÄœÉ2 1 2 exp x ¬µ 2 2œÉ2 1 109 and so the distribution that maximizes the differential entropy is the gaussian note that we did not constrain the distribution to be nonnegative when we maximized the entropy',\n",
       " '1 108 the lagrange multipliers can be found by back substitution of this result into the three constraint equations leading Ô¨Ånally to the resultexercise 1 34 p x 1 2œÄœÉ2 1 2 exp x ¬µ 2 2œÉ2 1 109 and so the distribution that maximizes the differential entropy is the gaussian note that we did not constrain the distribution to be nonnegative when we maximized the entropy however because the resulting distribution is indeed nonnegative we see with hindsight that such a constraint is not necessary if we evaluate the differential entropy of the gaussian we obtainexercise 1 35 h x 1 2 1 l n 2œÄœÉ2 1 110 thus we see again that the entropy increases as the distribution becomes broader i e as œÉ2 increases',\n",
       " 'if we evaluate the differential entropy of the gaussian we obtainexercise 1 35 h x 1 2 1 l n 2œÄœÉ2 1 110 thus we see again that the entropy increases as the distribution becomes broader i e as œÉ2 increases this result also shows that the differential entropy unlike the discrete entropy can be negative becauseh x 0 in 1 110 forœÉ2 1 2œÄe suppose we have a joint distributionp x y from which we draw pairs of values of x and y if a value ofx is already known then the additional information needed to specify the corresponding value ofy is given by lnp y x thus the average additional information needed to specifyy can be written as h y x p y x l np y x d ydx 1 111 1 6 information theory 55 which is called the conditional entropy of y given x',\n",
       " 'thus the average additional information needed to specifyy can be written as h y x p y x l np y x d ydx 1 111 1 6 information theory 55 which is called the conditional entropy of y given x it is easily seen using the product rule that the conditional entropy satisÔ¨Åes the relationexercise 1 37 h x y h y x h x 1 112 where h x y is the differential entropy ofp x y and h x is the differential en tropy of the marginal distributionp x thus the information needed to describex and y is given by the sum of the information needed to describex alone plus the additional information required to specifyy given x',\n",
       " 'it is easily seen using the product rule that the conditional entropy satisÔ¨Åes the relationexercise 1 37 h x y h y x h x 1 112 where h x y is the differential entropy ofp x y and h x is the differential en tropy of the marginal distributionp x thus the information needed to describex and y is given by the sum of the information needed to describex alone plus the additional information required to specifyy given x 1 6 1 relative entropy and mutual information so far in this section we have introduced a number of concepts from information theory including the key notion of entropy we now start to relate these ideas to pattern recognition consider some unknown distribution p x and suppose that we have modelled this using an approximating distributionq x',\n",
       " 'we now start to relate these ideas to pattern recognition consider some unknown distribution p x and suppose that we have modelled this using an approximating distributionq x i fw eu s eq x to construct a coding scheme for the purpose of transmitting values ofx to a receiver then the averageadditional amount of information in nats required to specify the value ofx assuming we choose an efÔ¨Åcient coding scheme as a result of usingq x instead of the true distributionp x is given by kl p q p x l nq x dx p x l np x d x p x l n q x p x dx 1 113 this is known as therelative entropy or kullback leibler divergence o r kl diver gence kullback and leibler 1951 between the distributionsp x and q x',\n",
       " 'i fw eu s eq x to construct a coding scheme for the purpose of transmitting values ofx to a receiver then the averageadditional amount of information in nats required to specify the value ofx assuming we choose an efÔ¨Åcient coding scheme as a result of usingq x instead of the true distributionp x is given by kl p q p x l nq x dx p x l np x d x p x l n q x p x dx 1 113 this is known as therelative entropy or kullback leibler divergence o r kl diver gence kullback and leibler 1951 between the distributionsp x and q x note that it is not a symmetrical quantity that is to saykl p q kl q p we now show that the kullback leibler divergence satisÔ¨Åeskl p q 0 with equality if and only if p x q x',\n",
       " 'note that it is not a symmetrical quantity that is to saykl p q kl q p we now show that the kullback leibler divergence satisÔ¨Åeskl p q 0 with equality if and only if p x q x to do this we Ô¨Årst introduce the concept of convex functions a function f x is said to be convex if it has the property that every chord lies on or above the function as shown in figure 1 31 any value ofx in the interval fromx a to x b can be written in the formŒªa 1 Œª b where 0 Œª 1 the corresponding point on the chord is given byŒªf a 1 Œª f b claude shannon 1916 2001 after graduating from michigan and mit shannon joined the at t bell telephone laboratories in 1941',\n",
       " 'any value ofx in the interval fromx a to x b can be written in the formŒªa 1 Œª b where 0 Œª 1 the corresponding point on the chord is given byŒªf a 1 Œª f b claude shannon 1916 2001 after graduating from michigan and mit shannon joined the at t bell telephone laboratories in 1941 his paper a mathematical theory of communication published in the bell system technical journal in 1948 laid the foundations for modern information the ory this paper introduced the word bit and his con cept that information could be sent as a stream of 1s and 0s paved the way for the communications revo lution',\n",
       " 'his paper a mathematical theory of communication published in the bell system technical journal in 1948 laid the foundations for modern information the ory this paper introduced the word bit and his con cept that information could be sent as a stream of 1s and 0s paved the way for the communications revo lution it is said that von neumann recommended to shannon that he use the term entropy not only be cause of its similarity to the quantity used in physics but also because nobody knows what entropy really is so in any discussion you will always have an advan tage 56 1 introduction figure 1 31 a convex functionf x is one for which ev ery chord shown in blue lies on or above the function shown in red',\n",
       " '56 1 introduction figure 1 31 a convex functionf x is one for which ev ery chord shown in blue lies on or above the function shown in red xa bxŒª chord xŒª f x and the corresponding value of the function isf Œªa 1 Œª b convexity then implies f Œªa 1 Œª b Œªf a 1 Œª f b 1 114 this is equivalent to the requirement that the second derivative of the function be everywhere positive examples of convex functions arexln x for x 0 andx2 aexercise 1 36 function is calledstrictly convexif the equality is satisÔ¨Åed only forŒª 0 and Œª 1 if a function has the opposite property namely that every chord lies on or below the function it is calledconcave with a corresponding deÔ¨Ånition forstrictly concave i f a function f x is convex then f x will be concave',\n",
       " 'examples of convex functions arexln x for x 0 andx2 aexercise 1 36 function is calledstrictly convexif the equality is satisÔ¨Åed only forŒª 0 and Œª 1 if a function has the opposite property namely that every chord lies on or below the function it is calledconcave with a corresponding deÔ¨Ånition forstrictly concave i f a function f x is convex then f x will be concave using the technique of proof by induction we can show from 1 114 that aexercise 1 38 convex functionf x satisÔ¨Åes f m i 1 Œªixi m i 1 Œªif xi 1 115 where Œªi 0 and i Œªi 1 for any set of points xi the result 1 115 is known as jensen s inequality',\n",
       " 'using the technique of proof by induction we can show from 1 114 that aexercise 1 38 convex functionf x satisÔ¨Åes f m i 1 Œªixi m i 1 Œªif xi 1 115 where Œªi 0 and i Œªi 1 for any set of points xi the result 1 115 is known as jensen s inequality if we interpret the Œªi as the probability distribution over a discrete variablex taking the values xi then 1 115 can be written f e x e f x 1 116 where e denotes the expectation for continuous variables jensen s inequality takes the form f xp x dx f x p x dx 1 117 we can apply jensen s inequality in the form 1 117 to the kullback leibler divergence 1 113 to give kl p q p x l n q x p x dx ln q x dx 0 1 118 1 6',\n",
       " 'for continuous variables jensen s inequality takes the form f xp x dx f x p x dx 1 117 we can apply jensen s inequality in the form 1 117 to the kullback leibler divergence 1 113 to give kl p q p x l n q x p x dx ln q x dx 0 1 118 1 6 information theory 57 where we have used the fact that lnx is a convex function together with the nor malization condition q x dx 1 in fact lnx is a strictly convex function so the equality will hold if and only if q x p x for all x thus we can in terpret the kullback leibler divergence as a measure of the dissimilarity of the two distributions p x and q x',\n",
       " 'in fact lnx is a strictly convex function so the equality will hold if and only if q x p x for all x thus we can in terpret the kullback leibler divergence as a measure of the dissimilarity of the two distributions p x and q x we see that there is an intimate relationship between data compression and den sity estimation i e the problem of modelling an unknown probability distribution because the most efÔ¨Åcient compression is achieved when we know the true distri bution if we use a distribution that is different from the true one then we must necessarily have a less efÔ¨Åcient coding and on average the additional information that must be transmitted is at least equal to the kullback leibler divergence be tween the two distributions',\n",
       " 'we see that there is an intimate relationship between data compression and den sity estimation i e the problem of modelling an unknown probability distribution because the most efÔ¨Åcient compression is achieved when we know the true distri bution if we use a distribution that is different from the true one then we must necessarily have a less efÔ¨Åcient coding and on average the additional information that must be transmitted is at least equal to the kullback leibler divergence be tween the two distributions suppose that data is being generated from an unknown distributionp x that we wish to model we can try to approximate this distribution using some parametric distribution q x Œ∏ governed by a set of adjustable parameters Œ∏ for example a multivariate gaussian',\n",
       " 'suppose that data is being generated from an unknown distributionp x that we wish to model we can try to approximate this distribution using some parametric distribution q x Œ∏ governed by a set of adjustable parameters Œ∏ for example a multivariate gaussian one way to determineŒ∏ is to minimize the kullback leibler divergence between p x and q x Œ∏ with respect to Œ∏ we cannot do this directly because we don t knowp x suppose however that we have observed a Ô¨Ånite set of training points x n for n 1 n drawn from p x then the expectation with respect to p x can be approximated by a Ô¨Ånite sum over these points using 1 35 so that kl p q n n 1 lnq xn Œ∏ l n p xn',\n",
       " 'suppose however that we have observed a Ô¨Ånite set of training points x n for n 1 n drawn from p x then the expectation with respect to p x can be approximated by a Ô¨Ånite sum over these points using 1 35 so that kl p q n n 1 lnq xn Œ∏ l n p xn 1 119 the second term on the right hand side of 1 119 is independent ofŒ∏ and the Ô¨Årst term is the negative log likelihood function forŒ∏ under the distributionq x Œ∏ eval uated using the training set thus we see that minimizing this kullback leibler divergence is equivalent to maximizing the likelihood function now consider the joint distribution between two sets of variablesx and y given by p x y',\n",
       " 'thus we see that minimizing this kullback leibler divergence is equivalent to maximizing the likelihood function now consider the joint distribution between two sets of variablesx and y given by p x y if the sets of variables are independent then their joint distribution will factorize into the product of their marginalsp x y p x p y if the variables are not independent we can gain some idea of whether they are close to being indepen dent by considering the kullback leibler divergence between the joint distribution and the product of the marginals given by i x y kl p x y p x p y p x y l n p x p y p x y dxdy 1 120 which is called the mutual information between the variables x and y',\n",
       " 'if the sets of variables are independent then their joint distribution will factorize into the product of their marginalsp x y p x p y if the variables are not independent we can gain some idea of whether they are close to being indepen dent by considering the kullback leibler divergence between the joint distribution and the product of the marginals given by i x y kl p x y p x p y p x y l n p x p y p x y dxdy 1 120 which is called the mutual information between the variables x and y from the properties of the kullback leibler divergence we see thati x y 0 with equal ity if and only if x and y are independent',\n",
       " 'if the variables are not independent we can gain some idea of whether they are close to being indepen dent by considering the kullback leibler divergence between the joint distribution and the product of the marginals given by i x y kl p x y p x p y p x y l n p x p y p x y dxdy 1 120 which is called the mutual information between the variables x and y from the properties of the kullback leibler divergence we see thati x y 0 with equal ity if and only if x and y are independent using the sum and product rules of probability we see that the mutual information is related to the conditional entropy throughexercise 1 41 i x y h x h x y h y h y x 1 121 58 1',\n",
       " 'using the sum and product rules of probability we see that the mutual information is related to the conditional entropy throughexercise 1 41 i x y h x h x y h y h y x 1 121 58 1 introduction thus we can view the mutual information as the reduction in the uncertainty aboutx by virtue of being told the value ofy or vice versa from a bayesian perspective we can viewp x as the prior distribution forx and p x y as the posterior distribu tion after we have observed new datay the mutual information therefore represents the reduction in uncertainty aboutx as a consequence of the new observationy exercises 1 1 www consider the sum of squares error function given by 1 2 in which the function y x w is given by the polynomial 1 1',\n",
       " 'the mutual information therefore represents the reduction in uncertainty aboutx as a consequence of the new observationy exercises 1 1 www consider the sum of squares error function given by 1 2 in which the function y x w is given by the polynomial 1 1 show that the coefÔ¨Åcients w wi that minimize this error function are given by the solution to the following set of linear equations m j 0 aijwj ti 1 122 where aij n n 1 xn i j t i n n 1 xn itn 1 123 here a sufÔ¨Åxi or j denotes the index of a component whereas x i denotes x raised to the power ofi 1 2 write down the set of coupled linear equations analogous to 1 122 satisÔ¨Åed by the coefÔ¨Åcientswi which minimize the regularized sum of squares error function given by 1 4',\n",
       " '1 123 here a sufÔ¨Åxi or j denotes the index of a component whereas x i denotes x raised to the power ofi 1 2 write down the set of coupled linear equations analogous to 1 122 satisÔ¨Åed by the coefÔ¨Åcientswi which minimize the regularized sum of squares error function given by 1 4 1 3 suppose that we have three coloured boxesr red b blue and g green box r contains 3 apples 4 oranges and 3 limes boxb contains 1 apple 1 orange and 0 limes and boxg contains 3 apples 3 oranges and 4 limes if a box is chosen at random with probabilities p r 0 2 p b 0 2 p g 0 6 and a piece of fruit is removed from the box with equal probability of selecting any of the items in the box then what is the probability of selecting an apple',\n",
       " 'box r contains 3 apples 4 oranges and 3 limes boxb contains 1 apple 1 orange and 0 limes and boxg contains 3 apples 3 oranges and 4 limes if a box is chosen at random with probabilities p r 0 2 p b 0 2 p g 0 6 and a piece of fruit is removed from the box with equal probability of selecting any of the items in the box then what is the probability of selecting an apple if we observe that the selected fruit is in fact an orange what is the probability that it came from the green box 1 4 www consider a probability densitypx x deÔ¨Åned over a continuous vari able x and suppose that we make a nonlinear change of variable usingx g y so that the density transforms according to 1 27',\n",
       " 'if we observe that the selected fruit is in fact an orange what is the probability that it came from the green box 1 4 www consider a probability densitypx x deÔ¨Åned over a continuous vari able x and suppose that we make a nonlinear change of variable usingx g y so that the density transforms according to 1 27 by differentiating 1 27 show that the locationÀÜy of the maximum of the density iny is not in general related to the location ÀÜx of the maximum of the density overx by the simple functional relation ÀÜx g ÀÜy as a consequence of the jacobian factor this shows that the maximum of a probability density in contrast to a simple function is dependent on the choice of variable',\n",
       " 'by differentiating 1 27 show that the locationÀÜy of the maximum of the density iny is not in general related to the location ÀÜx of the maximum of the density overx by the simple functional relation ÀÜx g ÀÜy as a consequence of the jacobian factor this shows that the maximum of a probability density in contrast to a simple function is dependent on the choice of variable v erify that in the case of a linear transformation the location of the maximum transforms in the same way as the variable itself 1 5 using the deÔ¨Ånition 1 38 show thatvar f x satisÔ¨Åes 1 39 exercises 59 1 6 show that if two variablesx and y are independent then their covariance is zero 1 7 www in this exercise we prove the normalization condition 1 48 for the univariate gaussian',\n",
       " 'exercises 59 1 6 show that if two variablesx and y are independent then their covariance is zero 1 7 www in this exercise we prove the normalization condition 1 48 for the univariate gaussian to do this consider the integral i exp 1 2œÉ2 x2 dx 1 124 which we can evaluate by Ô¨Årst writing its square in the form i2 exp 1 2œÉ2 x2 1 2œÉ2 y2 dxdy 1 125 now make the transformation from cartesian coordinates x y to polar coordinates r Œ∏ and then substituteu r2 show that by performing the integrals overŒ∏ and u and then taking the square root of both sides we obtain i 2œÄœÉ2 1 2 1 126 finally use this result to show that the gaussian distributionn x ¬µ œÉ2 is normal ized',\n",
       " 'show that by performing the integrals overŒ∏ and u and then taking the square root of both sides we obtain i 2œÄœÉ2 1 2 1 126 finally use this result to show that the gaussian distributionn x ¬µ œÉ2 is normal ized 1 8 www by using a change of variables verify that the univariate gaussian distribution given by 1 46 satisÔ¨Åes 1 49 next by differentiating both sides of the normalization condition n x ¬µ œÉ2 dx 1 1 127 with respect toœÉ2 verify that the gaussian satisÔ¨Åes 1 50 finally show that 1 51 holds 1 9 www show that the mode i e the maximum of the gaussian distribution 1 46 is given by ¬µ similarly show that the mode of the multivariate gaussian 1 52 is given by¬µ',\n",
       " 'the maximum of the gaussian distribution 1 46 is given by ¬µ similarly show that the mode of the multivariate gaussian 1 52 is given by¬µ 1 10 www suppose that the two variables x and z are statistically independent show that the mean and variance of their sum satisÔ¨Åes e x z e x e z 1 128 var x z v a r x v a r z 1 129 1 11 by setting the derivatives of the log likelihood function 1 54 with respect to¬µ and œÉ2 equal to zero verify the results 1 55 and 1 56 60 1 introduction 1 12 www using the results 1 49 and 1 50 show that e xnxm ¬µ2 inmœÉ2 1 130 where xn and xm denote data points sampled from a gaussian distribution with mean ¬µ and variance œÉ2 and inm satisÔ¨Åes inm 1 if n m and inm 0 otherwise',\n",
       " '60 1 introduction 1 12 www using the results 1 49 and 1 50 show that e xnxm ¬µ2 inmœÉ2 1 130 where xn and xm denote data points sampled from a gaussian distribution with mean ¬µ and variance œÉ2 and inm satisÔ¨Åes inm 1 if n m and inm 0 otherwise hence prove the results 1 57 and 1 58 1 13 suppose that the variance of a gaussian is estimated using the result 1 56 but with the maximum likelihood estimate ¬µml replaced with the true value ¬µ of the mean show that this estimator has the property that its expectation is given by the true varianceœÉ2',\n",
       " '1 13 suppose that the variance of a gaussian is estimated using the result 1 56 but with the maximum likelihood estimate ¬µml replaced with the true value ¬µ of the mean show that this estimator has the property that its expectation is given by the true varianceœÉ2 1 14 show that an arbitrary square matrix with elements wij can be written in the form wij ws ij wa ij where ws ij and wa ij are symmetric and anti symmetric matrices respectively satisfyingws ij ws ji and wa ij wa ji for all i and j n o w consider the second order term in a higher order polynomial ind dimensions given by d i 1 d j 1 wijxixj 1 131 show that d i 1 d j 1 wijxixj d i 1 d j 1 ws ijxixj 1 132 so that the contribution from the anti symmetric matrix vanishes',\n",
       " '1 14 show that an arbitrary square matrix with elements wij can be written in the form wij ws ij wa ij where ws ij and wa ij are symmetric and anti symmetric matrices respectively satisfyingws ij ws ji and wa ij wa ji for all i and j n o w consider the second order term in a higher order polynomial ind dimensions given by d i 1 d j 1 wijxixj 1 131 show that d i 1 d j 1 wijxixj d i 1 d j 1 ws ijxixj 1 132 so that the contribution from the anti symmetric matrix vanishes we therefore see that without loss of generality the matrix of coefÔ¨Åcientswij can be chosen to be symmetric and so not all of thed2 elements of this matrix can be chosen indepen dently show that the number of independent parameters in the matrixws ij is given by d d 1 2',\n",
       " 'we therefore see that without loss of generality the matrix of coefÔ¨Åcientswij can be chosen to be symmetric and so not all of thed2 elements of this matrix can be chosen indepen dently show that the number of independent parameters in the matrixws ij is given by d d 1 2 1 15 www in this exercise and the next we explore how the number of indepen dent parameters in a polynomial grows with the orderm of the polynomial and with the dimensionality d of the input space we start by writing down themth order term for a polynomial ind dimensions in the form d i1 1 d i2 1 d im 1 wi1i2 im xi1 xi2 xim',\n",
       " '1 15 www in this exercise and the next we explore how the number of indepen dent parameters in a polynomial grows with the orderm of the polynomial and with the dimensionality d of the input space we start by writing down themth order term for a polynomial ind dimensions in the form d i1 1 d i2 1 d im 1 wi1i2 im xi1 xi2 xim 1 133 the coefÔ¨Åcients wi1i2 im comprise dm elements but the number of independent parameters is signiÔ¨Åcantly fewer due to the many interchange symmetries of the factor xi1 xi2 xim begin by showing that the redundancy in the coefÔ¨Åcients can be removed by rewriting thismth order term in the form d i1 1 i1 i2 1 im 1 im 1 wi1i2 im xi1 xi2 xim',\n",
       " '1 133 the coefÔ¨Åcients wi1i2 im comprise dm elements but the number of independent parameters is signiÔ¨Åcantly fewer due to the many interchange symmetries of the factor xi1 xi2 xim begin by showing that the redundancy in the coefÔ¨Åcients can be removed by rewriting thismth order term in the form d i1 1 i1 i2 1 im 1 im 1 wi1i2 im xi1 xi2 xim 1 134 exercises 61 note that the precise relationship between the w coefÔ¨Åcients andw coefÔ¨Åcients need not be made explicit use this result to show that the number ofindependent param eters n d m which appear at orderm satisÔ¨Åes the following recursion relation n d m d i 1 n i m 1 1 135 next use proof by induction to show that the following result holds d i 1 i m 2 i 1 m 1 d m 1 d 1 m',\n",
       " 'd 1 m 1 136 which can be done by Ô¨Årst proving the result ford 1 and arbitrary m by making use of the result0 1 then assuming it is correct for dimensiond and verifying that it is correct for dimensiond 1 finally use the two previous results together with proof by induction to show n d m d m 1 d 1 m 1 137 to do this Ô¨Årst show that the result is true form 2 and any value ofd 1 by comparison with the result of exercise 1 14 then make use of 1 135 together with 1 136 to show that if the result holds at orderm 1 then it will also hold at order m 1 16 in exercise 1 15 we proved the result 1 135 for the number of independent parameters in themth order term of ad dimensional polynomial',\n",
       " '1 137 to do this Ô¨Årst show that the result is true form 2 and any value ofd 1 by comparison with the result of exercise 1 14 then make use of 1 135 together with 1 136 to show that if the result holds at orderm 1 then it will also hold at order m 1 16 in exercise 1 15 we proved the result 1 135 for the number of independent parameters in themth order term of ad dimensional polynomial we now Ô¨Ånd an expression for the total numbern d m of independent parameters in all of the terms up to and including them6th order first show thatn d m satisÔ¨Åes n d m m m 0 n d m 1 138 where n d m is the number of independent parameters in the term of orderm now make use of the result 1 137 together with proof by induction to show that n d m d m d m',\n",
       " 'm 1 139 this can be done by Ô¨Årst proving that the result holds for m 0 and arbitrary d 1 then assuming that it holds at orderm and hence showing that it holds at order m 1 finally make use of stirling s approximation in the form n nne n 1 140 for large n to show that for d m the quantity n d m grows like dm and for m d it grows like md consider a cubic m 3 polynomial in d dimensions and evaluate numerically the total number of independent parameters for i d 1 0 and ii d 100 which correspond to typical small scale and medium scale machine learning applications 62 1 introduction 1 17 www the gamma function is deÔ¨Åned by Œ≥ x 0 ux 1e u du 1 141 using integration by parts prove the relationŒ≥ x 1 xŒ≥ x',\n",
       " 'introduction 1 17 www the gamma function is deÔ¨Åned by Œ≥ x 0 ux 1e u du 1 141 using integration by parts prove the relationŒ≥ x 1 xŒ≥ x show also that Œ≥ 1 1and hence thatŒ≥ x 1 x when x is an integer 1 18 www we can use the result 1 126 to derive an expression for the surface area sd and the volumevd of a sphere of unit radius ind dimensions to do this consider the following result which is obtained by transforming from cartesian to polar coordinates d i 1 e x2 i dxi sd 0 e r2 rd 1 dr 1 142 using the deÔ¨Ånition 1 141 of the gamma function together with 1 126 evaluate both sides of this equation and hence show that sd 2œÄd 2 Œ≥ d 2',\n",
       " '1 18 www we can use the result 1 126 to derive an expression for the surface area sd and the volumevd of a sphere of unit radius ind dimensions to do this consider the following result which is obtained by transforming from cartesian to polar coordinates d i 1 e x2 i dxi sd 0 e r2 rd 1 dr 1 142 using the deÔ¨Ånition 1 141 of the gamma function together with 1 126 evaluate both sides of this equation and hence show that sd 2œÄd 2 Œ≥ d 2 1 143 next by integrating with respect to radius from0 to 1 show that the volume of the unit sphere ind dimensions is given by vd sd d 1 144 finally use the results Œ≥ 1 1 and Œ≥ 3 2 œÄ 2 to show that 1 143 and 1 144 reduce to the usual expressions ford 2 and d 3',\n",
       " '1 143 next by integrating with respect to radius from0 to 1 show that the volume of the unit sphere ind dimensions is given by vd sd d 1 144 finally use the results Œ≥ 1 1 and Œ≥ 3 2 œÄ 2 to show that 1 143 and 1 144 reduce to the usual expressions ford 2 and d 3 1 19 consider a sphere of radiusa in d dimensions together with the concentric hypercube of side2a so that the sphere touches the hypercube at the centres of each of its sides by using the results of exercise 1 18 show that the ratio of the volume of the sphere to the volume of the cube is given by volume of sphere volume of cube œÄd 2 d2d 1Œ≥ d 2',\n",
       " '1 19 consider a sphere of radiusa in d dimensions together with the concentric hypercube of side2a so that the sphere touches the hypercube at the centres of each of its sides by using the results of exercise 1 18 show that the ratio of the volume of the sphere to the volume of the cube is given by volume of sphere volume of cube œÄd 2 d2d 1Œ≥ d 2 1 145 now make use of stirling s formula in the form Œ≥ x 1 2œÄ 1 2e xxx 1 2 1 146 which is valid forx 1 to show that asd the ratio 1 145 goes to zero show also that the ratio of the distance from the centre of the hypercube to one of the corners divided by the perpendicular distance to one of the sides is d which therefore goes to as d',\n",
       " '1 145 now make use of stirling s formula in the form Œ≥ x 1 2œÄ 1 2e xxx 1 2 1 146 which is valid forx 1 to show that asd the ratio 1 145 goes to zero show also that the ratio of the distance from the centre of the hypercube to one of the corners divided by the perpendicular distance to one of the sides is d which therefore goes to as d from these results we see that in a space of high dimensionality most of the volume of a cube is concentrated in the large number of corners which themselves become very long spikes exercises 63 1 20 www in this exercise we explore the behaviour of the gaussian distribution in high dimensional spaces consider a gaussian distribution ind dimensions given by p x 1 2œÄœÉ2 d 2 exp x 2 2œÉ2',\n",
       " 'exercises 63 1 20 www in this exercise we explore the behaviour of the gaussian distribution in high dimensional spaces consider a gaussian distribution ind dimensions given by p x 1 2œÄœÉ2 d 2 exp x 2 2œÉ2 1 147 we wish to Ô¨Ånd the density with respect to radius in polar coordinates in which the direction variables have been integrated out to do this show that the integral of the probability density over a thin shell of radiusr and thickness œµ where œµ 1 i s given byp r œµ where p r sdrd 1 2œÄœÉ2 d 2 exp r2 2œÉ2 1 148 where sd is the surface area of a unit sphere ind dimensions show that the function p r has a single stationary point located for larged a tÀÜr dœÉ',\n",
       " 'to do this show that the integral of the probability density over a thin shell of radiusr and thickness œµ where œµ 1 i s given byp r œµ where p r sdrd 1 2œÄœÉ2 d 2 exp r2 2œÉ2 1 148 where sd is the surface area of a unit sphere ind dimensions show that the function p r has a single stationary point located for larged a tÀÜr dœÉ by considering p ÀÜr œµ where œµ ÀÜr show that for larged p ÀÜr œµ p ÀÜr e x p 3œµ2 2œÉ2 1 149 which shows thatÀÜr is a maximum of the radial probability density and also thatp r decays exponentially away from its maximum atÀÜr with length scale œÉ w e h a v e already seen that œÉ ÀÜr for large d and so we see that most of the probability mass is concentrated in a thin shell at large radius',\n",
       " 'show that the function p r has a single stationary point located for larged a tÀÜr dœÉ by considering p ÀÜr œµ where œµ ÀÜr show that for larged p ÀÜr œµ p ÀÜr e x p 3œµ2 2œÉ2 1 149 which shows thatÀÜr is a maximum of the radial probability density and also thatp r decays exponentially away from its maximum atÀÜr with length scale œÉ w e h a v e already seen that œÉ ÀÜr for large d and so we see that most of the probability mass is concentrated in a thin shell at large radius finally show that the probability density p x is larger at the origin than at the radiusÀÜr by a factor of exp d 2 we therefore see that most of the probability mass in a high dimensional gaussian distribution is located at a different radius from the region of high probability density',\n",
       " 'finally show that the probability density p x is larger at the origin than at the radiusÀÜr by a factor of exp d 2 we therefore see that most of the probability mass in a high dimensional gaussian distribution is located at a different radius from the region of high probability density this property of distributions in spaces of high dimensionality will have important consequences when we consider bayesian inference of model parameters in later chapters 1 21 consider two nonnegative numbers a and b and show that if a b then a ab 1 2 use this result to show that if the decision regions of a two class classiÔ¨Åcation problem are chosen to minimize the probability of misclassiÔ¨Åcation this probability will satisfy p mistake p x c1 p x c2 1 2 dx',\n",
       " '1 21 consider two nonnegative numbers a and b and show that if a b then a ab 1 2 use this result to show that if the decision regions of a two class classiÔ¨Åcation problem are chosen to minimize the probability of misclassiÔ¨Åcation this probability will satisfy p mistake p x c1 p x c2 1 2 dx 1 150 1 22 www given a loss matrix with elementslkj the expected risk is minimized if for each x we choose the class that minimizes 1 81 v erify that when the loss matrix is given bylkj 1 ikj where ikj are the elements of the identity matrix this reduces to the criterion of choosing the class having the largest posterior probability what is the interpretation of this form of loss matrix',\n",
       " 'v erify that when the loss matrix is given bylkj 1 ikj where ikj are the elements of the identity matrix this reduces to the criterion of choosing the class having the largest posterior probability what is the interpretation of this form of loss matrix 1 23 derive the criterion for minimizing the expected loss when there is a general loss matrix and general prior probabilities for the classes 64 1 introduction 1 24 www consider a classiÔ¨Åcation problem in which the loss incurred when an input vector from classck is classiÔ¨Åed as belonging to classcj is given by the loss matrix lkj and for which the loss incurred in selecting the reject option isŒª find the decision criterion that will give the minimum expected loss',\n",
       " 'introduction 1 24 www consider a classiÔ¨Åcation problem in which the loss incurred when an input vector from classck is classiÔ¨Åed as belonging to classcj is given by the loss matrix lkj and for which the loss incurred in selecting the reject option isŒª find the decision criterion that will give the minimum expected loss v erify that this reduces to the reject criterion discussed in section 1 5 3 when the loss matrix is given by l kj 1 ikj what is the relationship betweenŒª and the rejection thresholdŒ∏ 1 25 www consider the generalization of the squared loss function 1 87 for a single target variablet to the case of multiple target variables described by the vector t given by e l t y x y x t 2p x t d xdt',\n",
       " 'what is the relationship betweenŒª and the rejection thresholdŒ∏ 1 25 www consider the generalization of the squared loss function 1 87 for a single target variablet to the case of multiple target variables described by the vector t given by e l t y x y x t 2p x t d xdt 1 151 using the calculus of variations show that the functiony x for which this expected loss is minimized is given byy x et t x show that this result reduces to 1 89 for the case of a single target variablet',\n",
       " '1 151 using the calculus of variations show that the functiony x for which this expected loss is minimized is given byy x et t x show that this result reduces to 1 89 for the case of a single target variablet 1 26 by expansion of the square in 1 151 derive a result analogous to 1 90 and hence show that the functiony x that minimizes the expected squared loss for the case of a vectort of target variables is again given by the conditional expectation of t 1 27 www consider the expected loss for regression problems under thelq loss function given by 1 91 write down the condition thaty x must satisfy in order to minimize e lq',\n",
       " '1 26 by expansion of the square in 1 151 derive a result analogous to 1 90 and hence show that the functiony x that minimizes the expected squared loss for the case of a vectort of target variables is again given by the conditional expectation of t 1 27 www consider the expected loss for regression problems under thelq loss function given by 1 91 write down the condition thaty x must satisfy in order to minimize e lq show that for q 1 this solution represents the conditional median i e the function y x such that the probability mass fort y x is the same as fort y x also show that the minimum expectedlq loss for q 0 is given by the conditional mode i e by the functiony x equal to the value oft that maximizes p t x for each x',\n",
       " 'show that for q 1 this solution represents the conditional median i e the function y x such that the probability mass fort y x is the same as fort y x also show that the minimum expectedlq loss for q 0 is given by the conditional mode i e by the functiony x equal to the value oft that maximizes p t x for each x 1 28 in section 1 6 we introduced the idea of entropyh x as the information gained on observing the value of a random variablex having distribution p x w e s a w that for independent variablesx and y for which p x y p x p y the entropy functions are additive so thath x y h x h y in this exercise we derive the relation between h and p in the form of a functionh p',\n",
       " '1 28 in section 1 6 we introduced the idea of entropyh x as the information gained on observing the value of a random variablex having distribution p x w e s a w that for independent variablesx and y for which p x y p x p y the entropy functions are additive so thath x y h x h y in this exercise we derive the relation between h and p in the form of a functionh p first show that h p2 2h p and hence by induction thath pn nh p where n is a positive integer hence show that h pn m n m h p where m is also a positive integer this implies that h px xh p where x is a positive rational number and hence by continuity when it is a positive real number finally show that this impliesh p must take the formh p lnp',\n",
       " 'this implies that h px xh p where x is a positive rational number and hence by continuity when it is a positive real number finally show that this impliesh p must take the formh p lnp 1 29 www consider an m state discrete random variablex and use jensen s in equality in the form 1 115 to show that the entropy of its distributionp x satisÔ¨Åes h x ln m 1 30 evaluate the kullback leibler divergence 1 113 between two gaussians p x n x ¬µ œÉ2 and q x n x m s2 exercises 65 table 1 3 the joint distributionp x y for two binary variables x and y used in exercise 1 39 y 01 x 0 1 3 1 3 1 0 1 3 1 31 www consider two variablesx and y having joint distributionp x y',\n",
       " '1 29 www consider an m state discrete random variablex and use jensen s in equality in the form 1 115 to show that the entropy of its distributionp x satisÔ¨Åes h x ln m 1 30 evaluate the kullback leibler divergence 1 113 between two gaussians p x n x ¬µ œÉ2 and q x n x m s2 exercises 65 table 1 3 the joint distributionp x y for two binary variables x and y used in exercise 1 39 y 01 x 0 1 3 1 3 1 0 1 3 1 31 www consider two variablesx and y having joint distributionp x y show that the differential entropy of this pair of variables satisÔ¨Åes h x y h x h y 1 152 with equality if and only if x and y are statistically independent 1 32 consider a vector x of continuous variables with distributionp x and corre sponding entropy h x',\n",
       " 'show that the differential entropy of this pair of variables satisÔ¨Åes h x y h x h y 1 152 with equality if and only if x and y are statistically independent 1 32 consider a vector x of continuous variables with distributionp x and corre sponding entropy h x suppose that we make a nonsingular linear transformation of x to obtain a new variabley ax show that the corresponding entropy is given by h y h x l n a where a denotes the determinant ofa 1 33 suppose that the conditional entropy h y x between two discrete random variables x and y is zero show that for all values of x such that p x 0 the variable y must be a function ofx in other words for eachx there is only one value of y such that p y x 0',\n",
       " '1 33 suppose that the conditional entropy h y x between two discrete random variables x and y is zero show that for all values of x such that p x 0 the variable y must be a function ofx in other words for eachx there is only one value of y such that p y x 0 1 34 www use the calculus of variations to show that the stationary point of the functional 1 108 is given by 1 108 then use the constraints 1 105 1 106 and 1 107 to eliminate the lagrange multipliers and hence show that the maximum entropy solution is given by the gaussian 1 109 1 35 www use the results 1 106 and 1 107 to show that the entropy of the univariate gaussian 1 109 is given by 1 110',\n",
       " 'then use the constraints 1 105 1 106 and 1 107 to eliminate the lagrange multipliers and hence show that the maximum entropy solution is given by the gaussian 1 109 1 35 www use the results 1 106 and 1 107 to show that the entropy of the univariate gaussian 1 109 is given by 1 110 1 36 a strictly convex function is deÔ¨Åned as one for which every chord lies above the function show that this is equivalent to the condition that the second derivative of the function be positive 1 37 using the deÔ¨Ånition 1 111 together with the product rule of probability prove the result 1 112 1 38 www using proof by induction show that the inequality 1 114 for convex functions implies the result 1 115',\n",
       " '1 37 using the deÔ¨Ånition 1 111 together with the product rule of probability prove the result 1 112 1 38 www using proof by induction show that the inequality 1 114 for convex functions implies the result 1 115 1 39 consider two binary variablesx and y having the joint distribution given in table 1 3 evaluate the following quantities a h x c h y x e h x y b h y d h x y f i x y draw a diagram to show the relationship between these various quantities 66 1 introduction 1 40 by applying jensen s inequality 1 115 withf x l n x show that the arith metic mean of a set of real numbers is never less than their geometrical mean',\n",
       " '66 1 introduction 1 40 by applying jensen s inequality 1 115 withf x l n x show that the arith metic mean of a set of real numbers is never less than their geometrical mean 1 41 www using the sum and product rules of probability show that the mutual information i x y satisÔ¨Åes the relation 1 121 2 probability distributions in chapter 1 we emphasized the central role played by probability theory in the solution of pattern recognition problems we turn now to an exploration of some particular examples of probability distributions and their properties as well as be ing of great interest in their own right these distributions can form building blocks for more complex models and will be used extensively throughout the book',\n",
       " 'we turn now to an exploration of some particular examples of probability distributions and their properties as well as be ing of great interest in their own right these distributions can form building blocks for more complex models and will be used extensively throughout the book the distributions introduced in this chapter will also serve another important purpose namely to provide us with the opportunity to discuss some key statistical concepts such as bayesian inference in the context of simple models before we encounter them in more complex situations in later chapters one role for the distributions discussed in this chapter is to model the prob ability distribution p x of a random variable x given a Ô¨Ånite set x 1 xn of observations',\n",
       " 'the distributions introduced in this chapter will also serve another important purpose namely to provide us with the opportunity to discuss some key statistical concepts such as bayesian inference in the context of simple models before we encounter them in more complex situations in later chapters one role for the distributions discussed in this chapter is to model the prob ability distribution p x of a random variable x given a Ô¨Ånite set x 1 xn of observations this problem is known as density estimation for the purposes of this chapter we shall assume that the data points are independent and identically distributed it should be emphasized that the problem of density estimation is fun 67 68 2',\n",
       " 'for the purposes of this chapter we shall assume that the data points are independent and identically distributed it should be emphasized that the problem of density estimation is fun 67 68 2 probability distributions damentally ill posed because there are inÔ¨Ånitely many probability distributions that could have given rise to the observed Ô¨Ånite data set indeed any distributionp x that is nonzero at each of the data pointsx1 xn is a potential candidate the issue of choosing an appropriate distribution relates to the problem of model selec tion that has already been encountered in the context of polynomial curve Ô¨Åtting in chapter 1 and that is a central issue in pattern recognition',\n",
       " 'indeed any distributionp x that is nonzero at each of the data pointsx1 xn is a potential candidate the issue of choosing an appropriate distribution relates to the problem of model selec tion that has already been encountered in the context of polynomial curve Ô¨Åtting in chapter 1 and that is a central issue in pattern recognition we begin by considering the binomial and multinomial distributions for discrete random variables and the gaussian distribution for continuous random variables these are speciÔ¨Åc examples ofparametric distributions so called because they are governed by a small number of adaptive parameters such as the mean and variance in the case of a gaussian for example',\n",
       " 'we begin by considering the binomial and multinomial distributions for discrete random variables and the gaussian distribution for continuous random variables these are speciÔ¨Åc examples ofparametric distributions so called because they are governed by a small number of adaptive parameters such as the mean and variance in the case of a gaussian for example to apply such models to the problem of density estimation we need a procedure for determining suitable values for the parameters given an observed data set in a frequentist treatment we choose speciÔ¨Åc values for the parameters by optimizing some criterion such as the likelihood function',\n",
       " 'to apply such models to the problem of density estimation we need a procedure for determining suitable values for the parameters given an observed data set in a frequentist treatment we choose speciÔ¨Åc values for the parameters by optimizing some criterion such as the likelihood function by contrast in a bayesian treatment we introduce prior distributions over the parameters and then use bayes theorem to compute the corresponding posterior distribution given the observed data we shall see that an important role is played byconjugate priors that lead to posterior distributions having the same functional form as the prior and that there fore lead to a greatly simpliÔ¨Åed bayesian analysis',\n",
       " 'by contrast in a bayesian treatment we introduce prior distributions over the parameters and then use bayes theorem to compute the corresponding posterior distribution given the observed data we shall see that an important role is played byconjugate priors that lead to posterior distributions having the same functional form as the prior and that there fore lead to a greatly simpliÔ¨Åed bayesian analysis for example the conjugate prior for the parameters of the multinomial distribution is called thedirichlet distribution while the conjugate prior for the mean of a gaussian is another gaussian all of these distributions are examples of theexponential family of distributions which possess a number of important properties and which will be discussed in some detail',\n",
       " 'for example the conjugate prior for the parameters of the multinomial distribution is called thedirichlet distribution while the conjugate prior for the mean of a gaussian is another gaussian all of these distributions are examples of theexponential family of distributions which possess a number of important properties and which will be discussed in some detail one limitation of the parametric approach is that it assumes a speciÔ¨Åc functional form for the distribution which may turn out to be inappropriate for a particular application an alternative approach is given bynonparametric density estimation methods in which the form of the distribution typically depends on the size of the data set',\n",
       " 'one limitation of the parametric approach is that it assumes a speciÔ¨Åc functional form for the distribution which may turn out to be inappropriate for a particular application an alternative approach is given bynonparametric density estimation methods in which the form of the distribution typically depends on the size of the data set such models still contain parameters but these control the model complexity rather than the form of the distribution we end this chapter by considering three nonparametric methods based respectively on histograms nearest neighbours and kernels 2 1 binary variables we begin by considering a single binary random variablex 0 1 for example x might describe the outcome of Ô¨Çipping a coin withx 1 representing heads and x 0 representing tails',\n",
       " 'binary variables we begin by considering a single binary random variablex 0 1 for example x might describe the outcome of Ô¨Çipping a coin withx 1 representing heads and x 0 representing tails we can imagine that this is a damaged coin so that the probability of landing heads is not necessarily the same as that of landing tails the probability ofx 1 will be denoted by the parameter¬µ so that p x 1 ¬µ ¬µ 2 1 2 1 binary variables 69 where 0 ¬µ 1 from which it follows thatp x 0 ¬µ 1 ¬µ the probability distribution overx can therefore be written in the form bern x ¬µ ¬µx 1 ¬µ 1 x 2 2 which is known as thebernoulli distribution',\n",
       " 'binary variables 69 where 0 ¬µ 1 from which it follows thatp x 0 ¬µ 1 ¬µ the probability distribution overx can therefore be written in the form bern x ¬µ ¬µx 1 ¬µ 1 x 2 2 which is known as thebernoulli distribution it is easily veriÔ¨Åed that this distributionexercise 2 1 is normalized and that it has mean and variance given by e x ¬µ 2 3 var x ¬µ 1 ¬µ 2 4 now suppose we have a data setd x1 x n of observed values ofx we can construct the likelihood function which is a function of¬µ on the assumption that the observations are drawn independently fromp x ¬µ so that p d ¬µ n n 1 p xn ¬µ n n 1 ¬µxn 1 ¬µ 1 xn',\n",
       " '2 4 now suppose we have a data setd x1 x n of observed values ofx we can construct the likelihood function which is a function of¬µ on the assumption that the observations are drawn independently fromp x ¬µ so that p d ¬µ n n 1 p xn ¬µ n n 1 ¬µxn 1 ¬µ 1 xn 2 5 in a frequentist setting we can estimate a value for¬µ by maximizing the likelihood function or equivalently by maximizing the logarithm of the likelihood in the case of the bernoulli distribution the log likelihood function is given by lnp d ¬µ n n 1 ln p xn ¬µ n n 1 xn ln ¬µ 1 xn l n 1 ¬µ 2 6 at this point it is worth noting that the log likelihood function depends on then observations xn only through their sum n xn',\n",
       " 'in the case of the bernoulli distribution the log likelihood function is given by lnp d ¬µ n n 1 ln p xn ¬µ n n 1 xn ln ¬µ 1 xn l n 1 ¬µ 2 6 at this point it is worth noting that the log likelihood function depends on then observations xn only through their sum n xn this sum provides an example of a sufÔ¨Åcient statistic for the data under this distribution and we shall study the impor tant role of sufÔ¨Åcient statistics in some detail',\n",
       " '2 6 at this point it is worth noting that the log likelihood function depends on then observations xn only through their sum n xn this sum provides an example of a sufÔ¨Åcient statistic for the data under this distribution and we shall study the impor tant role of sufÔ¨Åcient statistics in some detail if we set the derivative oflnp d ¬µ section 2 4 with respect to¬µ equal to zero we obtain the maximum likelihood estimator ¬µml 1 n n n 1 xn 2 7 jacob bernoulli 1654 1705 jacob bernoulli also known as jacques or james bernoulli was a swiss mathematician and was the Ô¨Årst of many in the bernoulli family to pursue a career in science and mathematics',\n",
       " 'this sum provides an example of a sufÔ¨Åcient statistic for the data under this distribution and we shall study the impor tant role of sufÔ¨Åcient statistics in some detail if we set the derivative oflnp d ¬µ section 2 4 with respect to¬µ equal to zero we obtain the maximum likelihood estimator ¬µml 1 n n n 1 xn 2 7 jacob bernoulli 1654 1705 jacob bernoulli also known as jacques or james bernoulli was a swiss mathematician and was the Ô¨Årst of many in the bernoulli family to pursue a career in science and mathematics although compelled to study philosophy and theology against his will by his parents he travelled extensively after graduating in order to meet with many of the leading scientists of his time including boyle and hooke in england',\n",
       " 'if we set the derivative oflnp d ¬µ section 2 4 with respect to¬µ equal to zero we obtain the maximum likelihood estimator ¬µml 1 n n n 1 xn 2 7 jacob bernoulli 1654 1705 jacob bernoulli also known as jacques or james bernoulli was a swiss mathematician and was the Ô¨Årst of many in the bernoulli family to pursue a career in science and mathematics although compelled to study philosophy and theology against his will by his parents he travelled extensively after graduating in order to meet with many of the leading scientists of his time including boyle and hooke in england when he returned to switzerland he taught mechanics and became professor of mathematics at basel in 1687',\n",
       " 'although compelled to study philosophy and theology against his will by his parents he travelled extensively after graduating in order to meet with many of the leading scientists of his time including boyle and hooke in england when he returned to switzerland he taught mechanics and became professor of mathematics at basel in 1687 unfortunately rivalry between jacob and his younger brother johann turned an initially productive collabora tion into a bitter and public dispute jacob s most sig niÔ¨Åcant contributions to mathematics appeared in the art of conjecture published in 1713 eight years after his death which deals with topics in probability the ory including what has become known as the bernoulli distribution 70 2',\n",
       " 'jacob s most sig niÔ¨Åcant contributions to mathematics appeared in the art of conjecture published in 1713 eight years after his death which deals with topics in probability the ory including what has become known as the bernoulli distribution 70 2 probability distributions figure 2 1 histogram plot of the binomial dis tribution 2 9 as a function of m for n 1 0 and ¬µ 0 25 m 0 1 2 3 4 5 6 7 8 9 10 0 0 1 0 2 0 3 which is also known as thesample mean if we denote the number of observations of x 1 heads within this data set bym then we can write 2 7 in the form ¬µml m n 2 8 so that the probability of landing heads is given in this maximum likelihood frame work by the fraction of observations of heads in the data set',\n",
       " 'probability distributions figure 2 1 histogram plot of the binomial dis tribution 2 9 as a function of m for n 1 0 and ¬µ 0 25 m 0 1 2 3 4 5 6 7 8 9 10 0 0 1 0 2 0 3 which is also known as thesample mean if we denote the number of observations of x 1 heads within this data set bym then we can write 2 7 in the form ¬µml m n 2 8 so that the probability of landing heads is given in this maximum likelihood frame work by the fraction of observations of heads in the data set now suppose we Ô¨Çip a coin say 3 times and happen to observe 3 heads then n m 3 and ¬µml 1 in this case the maximum likelihood result would predict that all future observations should give heads',\n",
       " 'then n m 3 and ¬µml 1 in this case the maximum likelihood result would predict that all future observations should give heads common sense tells us that this is unreasonable and in fact this is an extreme example of the over Ô¨Åtting associ ated with maximum likelihood we shall see shortly how to arrive at more sensible conclusions through the introduction of a prior distribution over¬µ we can also work out the distribution of the numberm of observations ofx 1 given that the data set has size n this is called the binomial distribution and from 2 5 we see that it is proportional to¬µm 1 ¬µ n m',\n",
       " 'we shall see shortly how to arrive at more sensible conclusions through the introduction of a prior distribution over¬µ we can also work out the distribution of the numberm of observations ofx 1 given that the data set has size n this is called the binomial distribution and from 2 5 we see that it is proportional to¬µm 1 ¬µ n m in order to obtain the normalization coefÔ¨Åcient we note that out of n coin Ô¨Çips we have to add up all of the possible ways of obtainingm heads so that the binomial distribution can be written bin m n ¬µ n m ¬µm 1 ¬µ n m 2 9 where n m n n m m 2 10 is the number of ways of choosingm objects out of a total ofn identical objects exercise 2 3 figure 2 1 shows a plot of the binomial distribution forn 1 0 and ¬µ 0 25',\n",
       " 'n m m 2 10 is the number of ways of choosingm objects out of a total ofn identical objects exercise 2 3 figure 2 1 shows a plot of the binomial distribution forn 1 0 and ¬µ 0 25 the mean and variance of the binomial distribution can be found by using the result of exercise 1 10 which shows that for independent events the mean of the sum is the sum of the means and the variance of the sum is the sum of the variances because m x1 xn and for each observation the mean and variance are 2 1 binary variables 71 given by 2 3 and 2 4 respectively we have e m n m 0 mbin m n ¬µ n¬µ 2 11 var m n m 0 m e m 2 bin m n ¬µ n¬µ 1 ¬µ',\n",
       " 'because m x1 xn and for each observation the mean and variance are 2 1 binary variables 71 given by 2 3 and 2 4 respectively we have e m n m 0 mbin m n ¬µ n¬µ 2 11 var m n m 0 m e m 2 bin m n ¬µ n¬µ 1 ¬µ 2 12 these results can also be proved directly using calculus exercise 2 4 2 1 1 the beta distribution we have seen in 2 8 that the maximum likelihood setting for the parameter¬µ in the bernoulli distribution and hence in the binomial distribution is given by the fraction of the observations in the data set havingx 1 as we have already noted this can give severely over Ô¨Åtted results for small data sets in order to develop a bayesian treatment for this problem we need to introduce a prior distributionp ¬µ over the parameter¬µ',\n",
       " 'as we have already noted this can give severely over Ô¨Åtted results for small data sets in order to develop a bayesian treatment for this problem we need to introduce a prior distributionp ¬µ over the parameter¬µ here we consider a form of prior distribution that has a simple interpretation as well as some useful analytical properties to motivate this prior we note that the likelihood function takes the form of the product of factors of the form ¬µ x 1 ¬µ 1 x if we choose a prior to be proportional to powers of ¬µ and 1 ¬µ then the posterior distribution which is proportional to the product of the prior and the likelihood function will have the same functional form as the prior this property is calledconjugacy and we will see several examples of it later in this chapter',\n",
       " 'if we choose a prior to be proportional to powers of ¬µ and 1 ¬µ then the posterior distribution which is proportional to the product of the prior and the likelihood function will have the same functional form as the prior this property is calledconjugacy and we will see several examples of it later in this chapter we therefore choose a prior called thebeta distribution given by beta ¬µ a b Œ≥ a b Œ≥ a Œ≥ b ¬µa 1 1 ¬µ b 1 2 13 where Œ≥ x is the gamma function deÔ¨Åned by 1 141 and the coefÔ¨Åcient in 2 13 ensures that the beta distribution is normalized so thatexercise 2 5 1 0 beta ¬µ a b d ¬µ 1 2 14 the mean and variance of the beta distribution are given byexercise 2 6 e ¬µ a a b 2 15 var ¬µ ab a b 2 a b 1',\n",
       " 'we therefore choose a prior called thebeta distribution given by beta ¬µ a b Œ≥ a b Œ≥ a Œ≥ b ¬µa 1 1 ¬µ b 1 2 13 where Œ≥ x is the gamma function deÔ¨Åned by 1 141 and the coefÔ¨Åcient in 2 13 ensures that the beta distribution is normalized so thatexercise 2 5 1 0 beta ¬µ a b d ¬µ 1 2 14 the mean and variance of the beta distribution are given byexercise 2 6 e ¬µ a a b 2 15 var ¬µ ab a b 2 a b 1 2 16 the parameters a and b are often called hyperparameters because they control the distribution of the parameter¬µ figure 2 2 shows plots of the beta distribution for various values of the hyperparameters the posterior distribution of ¬µ is now obtained by multiplying the beta prior 2 13 by the binomial likelihood function 2 9 and normalizing',\n",
       " 'figure 2 2 shows plots of the beta distribution for various values of the hyperparameters the posterior distribution of ¬µ is now obtained by multiplying the beta prior 2 13 by the binomial likelihood function 2 9 and normalizing keeping only the factors that depend on¬µ we see that this posterior distribution has the form p ¬µ m l a b ¬µ m a 1 1 ¬µ l b 1 2 17 72 2 probability distributions ¬µ a 0 1 b 0 1 0 0 5 1 0 1 2 3 ¬µ a 1 b 1 0 0 5 1 0 1 2 3 ¬µ a 2 b 3 0 0 5 1 0 1 2 3 ¬µ a 8 b 4 0 0 5 1 0 1 2 3 figure 2 2 plots of the beta distribution beta ¬µ a b given by 2 13 as a function of ¬µ for various values of the hyperparameters a and b where l n m and therefore corresponds to the number of tails in the coin example',\n",
       " 'keeping only the factors that depend on¬µ we see that this posterior distribution has the form p ¬µ m l a b ¬µ m a 1 1 ¬µ l b 1 2 17 72 2 probability distributions ¬µ a 0 1 b 0 1 0 0 5 1 0 1 2 3 ¬µ a 1 b 1 0 0 5 1 0 1 2 3 ¬µ a 2 b 3 0 0 5 1 0 1 2 3 ¬µ a 8 b 4 0 0 5 1 0 1 2 3 figure 2 2 plots of the beta distribution beta ¬µ a b given by 2 13 as a function of ¬µ for various values of the hyperparameters a and b where l n m and therefore corresponds to the number of tails in the coin example we see that 2 17 has the same functional dependence on¬µ as the prior distribution reÔ¨Çecting the conjugacy properties of the prior with respect to the like lihood function',\n",
       " 'probability distributions ¬µ a 0 1 b 0 1 0 0 5 1 0 1 2 3 ¬µ a 1 b 1 0 0 5 1 0 1 2 3 ¬µ a 2 b 3 0 0 5 1 0 1 2 3 ¬µ a 8 b 4 0 0 5 1 0 1 2 3 figure 2 2 plots of the beta distribution beta ¬µ a b given by 2 13 as a function of ¬µ for various values of the hyperparameters a and b where l n m and therefore corresponds to the number of tails in the coin example we see that 2 17 has the same functional dependence on¬µ as the prior distribution reÔ¨Çecting the conjugacy properties of the prior with respect to the like lihood function indeed it is simply another beta distribution and its normalization coefÔ¨Åcient can therefore be obtained by comparison with 2 13 to give p ¬µ m l a b Œ≥ m a l b Œ≥ m a Œ≥ l b ¬µm a 1 1 ¬µ l b 1',\n",
       " 'we see that 2 17 has the same functional dependence on¬µ as the prior distribution reÔ¨Çecting the conjugacy properties of the prior with respect to the like lihood function indeed it is simply another beta distribution and its normalization coefÔ¨Åcient can therefore be obtained by comparison with 2 13 to give p ¬µ m l a b Œ≥ m a l b Œ≥ m a Œ≥ l b ¬µm a 1 1 ¬µ l b 1 2 18 we see that the effect of observing a data set ofm observations of x 1 and l observations of x 0 has been to increase the value ofa by m and the value of b by l in going from the prior distribution to the posterior distribution this allows us to provide a simple interpretation of the hyperparametersa and b in the prior as an effective number of observations of x 1 and x 0 respectively',\n",
       " '2 18 we see that the effect of observing a data set ofm observations of x 1 and l observations of x 0 has been to increase the value ofa by m and the value of b by l in going from the prior distribution to the posterior distribution this allows us to provide a simple interpretation of the hyperparametersa and b in the prior as an effective number of observations of x 1 and x 0 respectively note that a and b need not be integers furthermore the posterior distribution can act as the prior if we subsequently observe additional data to see this we can imagine taking observations one at a time and after each observation updating the current posterior 2 1',\n",
       " 'furthermore the posterior distribution can act as the prior if we subsequently observe additional data to see this we can imagine taking observations one at a time and after each observation updating the current posterior 2 1 binary variables 73 ¬µ prior 0 0 5 1 0 1 2 ¬µ likelihood function 0 0 5 1 0 1 2 ¬µ posterior 0 0 5 1 0 1 2 figure 2 3 illustration of one step of sequential bayesian inference the prior is given by a beta distribution with parameters a 2 b 2 and the likelihood function given by 2 9 with n m 1 corresponds to a single observation of x 1 so that the posterior is given by a beta distribution with parameters a 3 b 2',\n",
       " 'binary variables 73 ¬µ prior 0 0 5 1 0 1 2 ¬µ likelihood function 0 0 5 1 0 1 2 ¬µ posterior 0 0 5 1 0 1 2 figure 2 3 illustration of one step of sequential bayesian inference the prior is given by a beta distribution with parameters a 2 b 2 and the likelihood function given by 2 9 with n m 1 corresponds to a single observation of x 1 so that the posterior is given by a beta distribution with parameters a 3 b 2 distribution by multiplying by the likelihood function for the new observation and then normalizing to obtain the new revised posterior distribution',\n",
       " 'the prior is given by a beta distribution with parameters a 2 b 2 and the likelihood function given by 2 9 with n m 1 corresponds to a single observation of x 1 so that the posterior is given by a beta distribution with parameters a 3 b 2 distribution by multiplying by the likelihood function for the new observation and then normalizing to obtain the new revised posterior distribution at each stage the posterior is a beta distribution with some total number of prior and actual observed values for x 1 and x 0 given by the parametersa and b incorporation of an additional observation ofx 1 simply corresponds to incrementing the value ofa by 1 whereas for an observation ofx 0 we incrementb by 1 figure 2 3 illustrates one step in this process',\n",
       " 'at each stage the posterior is a beta distribution with some total number of prior and actual observed values for x 1 and x 0 given by the parametersa and b incorporation of an additional observation ofx 1 simply corresponds to incrementing the value ofa by 1 whereas for an observation ofx 0 we incrementb by 1 figure 2 3 illustrates one step in this process we see that thissequential approach to learning arises naturally when we adopt a bayesian viewpoint it is independent of the choice of prior and of the likelihood function and depends only on the assumption of i i d data sequential methods make use of observations one at a time or in small batches and then discard them before the next observations are used',\n",
       " 'data sequential methods make use of observations one at a time or in small batches and then discard them before the next observations are used they can be used for example in real time learning scenarios where a steady stream of data is arriving and predictions must be made before all of the data is seen because they do not require the whole data set to be stored or loaded into memory sequential methods are also useful for large data sets maximum likelihood methods can also be cast into a sequential framework section 2 3 5 if our goal is to predict as best we can the outcome of the next trial then we must evaluate the predictive distribution ofx given the observed data setd',\n",
       " 'because they do not require the whole data set to be stored or loaded into memory sequential methods are also useful for large data sets maximum likelihood methods can also be cast into a sequential framework section 2 3 5 if our goal is to predict as best we can the outcome of the next trial then we must evaluate the predictive distribution ofx given the observed data setd from the sum and product rules of probability this takes the form p x 1 d 1 0 p x 1 ¬µ p ¬µ d d ¬µ 1 0 ¬µp ¬µ d d ¬µ e ¬µ d',\n",
       " 'maximum likelihood methods can also be cast into a sequential framework section 2 3 5 if our goal is to predict as best we can the outcome of the next trial then we must evaluate the predictive distribution ofx given the observed data setd from the sum and product rules of probability this takes the form p x 1 d 1 0 p x 1 ¬µ p ¬µ d d ¬µ 1 0 ¬µp ¬µ d d ¬µ e ¬µ d 2 19 using the result 2 18 for the posterior distributionp ¬µ d together with the result 2 15 for the mean of the beta distribution we obtain p x 1 d m a m a l b 2 20 which has a simple interpretation as the total fraction of observations both real ob servations and Ô¨Åctitious prior observations that correspond tox 1',\n",
       " 'from the sum and product rules of probability this takes the form p x 1 d 1 0 p x 1 ¬µ p ¬µ d d ¬µ 1 0 ¬µp ¬µ d d ¬µ e ¬µ d 2 19 using the result 2 18 for the posterior distributionp ¬µ d together with the result 2 15 for the mean of the beta distribution we obtain p x 1 d m a m a l b 2 20 which has a simple interpretation as the total fraction of observations both real ob servations and Ô¨Åctitious prior observations that correspond tox 1 note that in the limit of an inÔ¨Ånitely large data setm l the result 2 20 reduces to the maximum likelihood result 2 8 as we shall see it is a very general property that the bayesian and maximum likelihood results will agree in the limit of an inÔ¨Ånitely 74 2 probability distributions large data set',\n",
       " 'as we shall see it is a very general property that the bayesian and maximum likelihood results will agree in the limit of an inÔ¨Ånitely 74 2 probability distributions large data set for a Ô¨Ånite data set the posterior mean for¬µ always lies between the prior mean and the maximum likelihood estimate for¬µ corresponding to the relative frequencies of events given by 2 7 exercise 2 7 from figure 2 2 we see that as the number of observations increases so the posterior distribution becomes more sharply peaked this can also be seen from the result 2 16 for the variance of the beta distribution in which we see that the variance goes to zero fora or b',\n",
       " 'for a Ô¨Ånite data set the posterior mean for¬µ always lies between the prior mean and the maximum likelihood estimate for¬µ corresponding to the relative frequencies of events given by 2 7 exercise 2 7 from figure 2 2 we see that as the number of observations increases so the posterior distribution becomes more sharply peaked this can also be seen from the result 2 16 for the variance of the beta distribution in which we see that the variance goes to zero fora or b in fact we might wonder whether it is a general property of bayesian learning that as we observe more and more data the uncertainty represented by the posterior distribution will steadily decrease',\n",
       " 'this can also be seen from the result 2 16 for the variance of the beta distribution in which we see that the variance goes to zero fora or b in fact we might wonder whether it is a general property of bayesian learning that as we observe more and more data the uncertainty represented by the posterior distribution will steadily decrease to address this we can take a frequentist view of bayesian learning and show that on average such a property does indeed hold consider a general bayesian inference problem for a parameterŒ∏ for which we have observed a data setd de scribed by the joint distributionp Œ∏ d',\n",
       " 'to address this we can take a frequentist view of bayesian learning and show that on average such a property does indeed hold consider a general bayesian inference problem for a parameterŒ∏ for which we have observed a data setd de scribed by the joint distributionp Œ∏ d the following resultexercise 2 8 e Œ∏ Œ∏ ed eŒ∏ Œ∏ d 2 21 where eŒ∏ Œ∏ p Œ∏ Œ∏ dŒ∏ 2 22 ed eŒ∏ Œ∏ d Œ∏p Œ∏ d d Œ∏ p d d d 2 23 says that the posterior mean ofŒ∏ averaged over the distribution generating the data is equal to the prior mean ofŒ∏ similarly we can show that varŒ∏ Œ∏ ed varŒ∏ Œ∏ d vard eŒ∏ Œ∏ d 2 24 the term on the left hand side of 2 24 is the prior variance ofŒ∏',\n",
       " 'similarly we can show that varŒ∏ Œ∏ ed varŒ∏ Œ∏ d vard eŒ∏ Œ∏ d 2 24 the term on the left hand side of 2 24 is the prior variance ofŒ∏ on the right hand side the Ô¨Årst term is the average posterior variance ofŒ∏ and the second term measures the variance in the posterior mean ofŒ∏ because this variance is a positive quantity this result shows that on average the posterior variance ofŒ∏ is smaller than the prior variance the reduction in variance is greater if the variance in the posterior mean is greater note however that this result only holds on average and that for a particular observed data set it is possible for the posterior variance to be larger than the prior variance 2 2',\n",
       " 'note however that this result only holds on average and that for a particular observed data set it is possible for the posterior variance to be larger than the prior variance 2 2 multinomial variables binary variables can be used to describe quantities that can take one of two possible values often however we encounter discrete variables that can take on one ofk possible mutually exclusive states although there are various alternative ways to express such variables we shall see shortly that a particularly convenient represen tation is the1 of k scheme in which the variable is represented by ak dimensional vector x in which one of the elementsx k equals 1 and all remaining elements equal 2 2 multinomial variables 75 0',\n",
       " 'although there are various alternative ways to express such variables we shall see shortly that a particularly convenient represen tation is the1 of k scheme in which the variable is represented by ak dimensional vector x in which one of the elementsx k equals 1 and all remaining elements equal 2 2 multinomial variables 75 0 so for instance if we have a variable that can takek 6 states and a particular observation of the variable happens to correspond to the state wherex3 1 then x will be represented by x 0 0 1 0 0 0 t 2 25 note that such vectors satisfy k k 1 xk 1',\n",
       " 'multinomial variables 75 0 so for instance if we have a variable that can takek 6 states and a particular observation of the variable happens to correspond to the state wherex3 1 then x will be represented by x 0 0 1 0 0 0 t 2 25 note that such vectors satisfy k k 1 xk 1 if we denote the probability ofxk 1 by the parameter¬µk then the distribution ofx is given p x ¬µ k k 1 ¬µxk k 2 26 where ¬µ ¬µ1 ¬µ k t and the parameters¬µk are constrained to satisfy¬µk 0 and k ¬µk 1 because they represent probabilities the distribution 2 26 can be regarded as a generalization of the bernoulli distribution to more than two outcomes it is easily seen that the distribution is normalized x p x ¬µ k k 1 ¬µk 1 2 27 and that e x ¬µ x p x ¬µ x ¬µ1 ¬µ m t ¬µ',\n",
       " 'the distribution 2 26 can be regarded as a generalization of the bernoulli distribution to more than two outcomes it is easily seen that the distribution is normalized x p x ¬µ k k 1 ¬µk 1 2 27 and that e x ¬µ x p x ¬µ x ¬µ1 ¬µ m t ¬µ 2 28 now consider a data set d of n independent observations x1 xn the corresponding likelihood function takes the form p d ¬µ n n 1 k k 1 ¬µxnk k k k 1 ¬µ p n xnk k k k 1 ¬µmk k 2 29 we see that the likelihood function depends on then data points only through the k quantities mk n xnk 2 30 which represent the number of observations ofxk 1',\n",
       " 'the corresponding likelihood function takes the form p d ¬µ n n 1 k k 1 ¬µxnk k k k 1 ¬µ p n xnk k k k 1 ¬µmk k 2 29 we see that the likelihood function depends on then data points only through the k quantities mk n xnk 2 30 which represent the number of observations ofxk 1 these are called thesufÔ¨Åcient statistics for this distribution section 2 4 in order to Ô¨Ånd the maximum likelihood solution for¬µ we need to maximize lnp d ¬µ with respect to¬µk taking account of the constraint that the¬µk must sum to one this can be achieved using a lagrange multiplierŒª and maximizingappendix e k k 1 mk ln¬µk Œª k k 1 ¬µk 1 2 31 setting the derivative of 2 31 with respect to¬µk to zero we obtain ¬µk mk Œª 2 32 76 2',\n",
       " '2 31 setting the derivative of 2 31 with respect to¬µk to zero we obtain ¬µk mk Œª 2 32 76 2 probability distributions we can solve for the lagrange multiplierŒª by substituting 2 32 into the constraint k ¬µk 1 to give Œª n thus we obtain the maximum likelihood solution in the form ¬µml k mk n 2 33 which is the fraction of then observations for whichxk 1 we can consider the joint distribution of the quantitiesm1 m k conditioned on the parameters ¬µ and on the total numbern of observations from 2 29 this takes the form mult m1 m 2 m k ¬µ n n m1m2 m k k k 1 ¬µmk k 2 34 which is known as themultinomial distribution',\n",
       " 'we can consider the joint distribution of the quantitiesm1 m k conditioned on the parameters ¬µ and on the total numbern of observations from 2 29 this takes the form mult m1 m 2 m k ¬µ n n m1m2 m k k k 1 ¬µmk k 2 34 which is known as themultinomial distribution the normalization coefÔ¨Åcient is the number of ways of partitioningn objects into k groups of sizem1 m k and is given by n m1m2 m k n m1 m2 m k 2 35 note that the variablesmk are subject to the constraint k k 1 mk n 2 36 2 2 1 the dirichlet distribution we now introduce a family of prior distributions for the parameters ¬µk of the multinomial distribution 2 34',\n",
       " 'm k 2 35 note that the variablesmk are subject to the constraint k k 1 mk n 2 36 2 2 1 the dirichlet distribution we now introduce a family of prior distributions for the parameters ¬µk of the multinomial distribution 2 34 by inspection of the form of the multinomial distribution we see that the conjugate prior is given by p ¬µ Œ± k k 1 ¬µŒ±k 1 k 2 37 where 0 ¬µk 1 and k ¬µk 1 here Œ±1 Œ± k are the parameters of the distribution and Œ± denotes Œ±1 Œ± k t note that because of the summation constraint the distribution over the space of the ¬µk is conÔ¨Åned to a simplex of dimensionality k 1 as illustrated fork 3 in figure 2 4',\n",
       " 'by inspection of the form of the multinomial distribution we see that the conjugate prior is given by p ¬µ Œ± k k 1 ¬µŒ±k 1 k 2 37 where 0 ¬µk 1 and k ¬µk 1 here Œ±1 Œ± k are the parameters of the distribution and Œ± denotes Œ±1 Œ± k t note that because of the summation constraint the distribution over the space of the ¬µk is conÔ¨Åned to a simplex of dimensionality k 1 as illustrated fork 3 in figure 2 4 the normalized form for this distribution is byexercise 2 9 dir ¬µ Œ± Œ≥ Œ±0 Œ≥ Œ±1 Œ≥ Œ±k k k 1 ¬µŒ±k 1 k 2 38 which is called thedirichlet distribution here Œ≥ x is the gamma function deÔ¨Åned by 1 141 while Œ±0 k k 1 Œ±k 2 39 2 2',\n",
       " 'here Œ≥ x is the gamma function deÔ¨Åned by 1 141 while Œ±0 k k 1 Œ±k 2 39 2 2 multinomial variables 77 figure 2 4 the dirichlet distribution over three variables ¬µ1 ¬µ 2 ¬µ 3 is conÔ¨Åned to a simplex a bounded linear manifold of the form shown as a consequence of the constraints 0 ¬µk 1 and p k ¬µk 1 ¬µ1 ¬µ2 ¬µ3 plots of the dirichlet distribution over the simplex for various settings of the param eters Œ±k are shown in figure 2 5 multiplying the prior 2 38 by the likelihood function 2 34 we obtain the posterior distribution for the parameters ¬µk in the form p ¬µ d Œ± p d ¬µ p ¬µ Œ± k k 1 ¬µŒ±k mk 1 k 2 40 we see that the posterior distribution again takes the form of a dirichlet distribution conÔ¨Årming that the dirichlet is indeed a conjugate prior for the multinomial',\n",
       " 'multiplying the prior 2 38 by the likelihood function 2 34 we obtain the posterior distribution for the parameters ¬µk in the form p ¬µ d Œ± p d ¬µ p ¬µ Œ± k k 1 ¬µŒ±k mk 1 k 2 40 we see that the posterior distribution again takes the form of a dirichlet distribution conÔ¨Årming that the dirichlet is indeed a conjugate prior for the multinomial this allows us to determine the normalization coefÔ¨Åcient by comparison with 2 38 so that p ¬µ d Œ± d i r ¬µ Œ± m Œ≥ Œ±0 n Œ≥ Œ±1 m1 Œ≥ Œ±k mk k k 1 ¬µŒ±k mk 1 k 2 41 where we have denoted m m1 m k t as for the case of the binomial distribution with its beta prior we can interpret the parametersŒ±k of the dirichlet prior as an effective number of observations ofxk 1',\n",
       " '2 40 we see that the posterior distribution again takes the form of a dirichlet distribution conÔ¨Årming that the dirichlet is indeed a conjugate prior for the multinomial this allows us to determine the normalization coefÔ¨Åcient by comparison with 2 38 so that p ¬µ d Œ± d i r ¬µ Œ± m Œ≥ Œ±0 n Œ≥ Œ±1 m1 Œ≥ Œ±k mk k k 1 ¬µŒ±k mk 1 k 2 41 where we have denoted m m1 m k t as for the case of the binomial distribution with its beta prior we can interpret the parametersŒ±k of the dirichlet prior as an effective number of observations ofxk 1 note that two state quantities can either be represented as binary variables and lejeune dirichlet 1805 1859 johann peter gustav lejeune dirichlet was a modest and re served mathematician who made contributions in number theory me chanics and astronomy and who gave the Ô¨Årst rigorous analysis of fourier series',\n",
       " 'this allows us to determine the normalization coefÔ¨Åcient by comparison with 2 38 so that p ¬µ d Œ± d i r ¬µ Œ± m Œ≥ Œ±0 n Œ≥ Œ±1 m1 Œ≥ Œ±k mk k k 1 ¬µŒ±k mk 1 k 2 41 where we have denoted m m1 m k t as for the case of the binomial distribution with its beta prior we can interpret the parametersŒ±k of the dirichlet prior as an effective number of observations ofxk 1 note that two state quantities can either be represented as binary variables and lejeune dirichlet 1805 1859 johann peter gustav lejeune dirichlet was a modest and re served mathematician who made contributions in number theory me chanics and astronomy and who gave the Ô¨Årst rigorous analysis of fourier series his family originated from richelet in belgium and the name lejeune dirichlet comes from le jeune de richelet the young person from richelet',\n",
       " 'note that two state quantities can either be represented as binary variables and lejeune dirichlet 1805 1859 johann peter gustav lejeune dirichlet was a modest and re served mathematician who made contributions in number theory me chanics and astronomy and who gave the Ô¨Årst rigorous analysis of fourier series his family originated from richelet in belgium and the name lejeune dirichlet comes from le jeune de richelet the young person from richelet dirichlet s Ô¨Årst paper which was published in 1825 brought him instant fame it concerned fer mat s last theorem which claims that there are no positive integer solutions to xn yn zn for n 2 dirichlet gave a partial proof for the case n 5 which was sent to legendre for review and who in turn com pleted the proof',\n",
       " 'it concerned fer mat s last theorem which claims that there are no positive integer solutions to xn yn zn for n 2 dirichlet gave a partial proof for the case n 5 which was sent to legendre for review and who in turn com pleted the proof later dirichlet gave a complete proof for n 1 4 although a full proof of fermat s last theo rem for arbitrary n had to wait until the work of andrew wiles in the closing years of the 20th century 78 2 probability distributions figure 2 5 plots of the dirichlet distribution over three variables where the two horizontal axes are coordinates in the plane of the simplex and the vertical axis corresponds to the value of the density here Œ±k 0 1 on the left plot Œ±k 1 in the centre plot and Œ±k 1 0 in the right plot',\n",
       " 'probability distributions figure 2 5 plots of the dirichlet distribution over three variables where the two horizontal axes are coordinates in the plane of the simplex and the vertical axis corresponds to the value of the density here Œ±k 0 1 on the left plot Œ±k 1 in the centre plot and Œ±k 1 0 in the right plot modelled using the binomial distribution 2 9 or as 1 of 2 variables and modelled using the multinomial distribution 2 34 withk 2 2 3 the gaussian distribution the gaussian also known as the normal distribution is a widely used model for the distribution of continuous variables in the case of a single variablex the gaussian distribution can be written in the form n x ¬µ œÉ2 1 2œÄœÉ2 1 2 exp 1 2œÉ2 x ¬µ 2 2 42 where ¬µ is the mean and œÉ2 is the variance',\n",
       " 'the gaussian distribution the gaussian also known as the normal distribution is a widely used model for the distribution of continuous variables in the case of a single variablex the gaussian distribution can be written in the form n x ¬µ œÉ2 1 2œÄœÉ2 1 2 exp 1 2œÉ2 x ¬µ 2 2 42 where ¬µ is the mean and œÉ2 is the variance for a d dimensional vector x t h e multivariate gaussian distribution takes the form n x ¬µ œÉ 1 2œÄ d 2 1 œÉ 1 2 exp 1 2 x ¬µ tœÇ 1 x ¬µ 2 43 where ¬µ is a d dimensional mean vector œÉ is a d d covariance matrix and œÉ denotes the determinant ofœÇ the gaussian distribution arises in many different contexts and can be motivated from a variety of different perspectives',\n",
       " 'for a d dimensional vector x t h e multivariate gaussian distribution takes the form n x ¬µ œÉ 1 2œÄ d 2 1 œÉ 1 2 exp 1 2 x ¬µ tœÇ 1 x ¬µ 2 43 where ¬µ is a d dimensional mean vector œÉ is a d d covariance matrix and œÉ denotes the determinant ofœÇ the gaussian distribution arises in many different contexts and can be motivated from a variety of different perspectives for example we have already seen that forsection 1 6 a single real variable the distribution that maximizes the entropy is the gaussian this property applies also to the multivariate gaussian exercise 2 14 another situation in which the gaussian distribution arises is when we consider the sum of multiple random variables',\n",
       " 'for example we have already seen that forsection 1 6 a single real variable the distribution that maximizes the entropy is the gaussian this property applies also to the multivariate gaussian exercise 2 14 another situation in which the gaussian distribution arises is when we consider the sum of multiple random variables the central limit theorem due to laplace tells us that subject to certain mild conditions the sum of a set of random variables which is of course itself a random variable has a distribution that becomes increas ingly gaussian as the number of terms in the sum increases walker 1969 we can 2 3',\n",
       " 'the central limit theorem due to laplace tells us that subject to certain mild conditions the sum of a set of random variables which is of course itself a random variable has a distribution that becomes increas ingly gaussian as the number of terms in the sum increases walker 1969 we can 2 3 the gaussian distribution 79 n 1 0 0 5 1 0 1 2 3 n 2 0 0 5 1 0 1 2 3 n 1 0 0 0 5 1 0 1 2 3 figure 2 6 histogram plots of the mean of n uniformly distributed numbers for various values of n w e observe that as n increases the distribution tends towards a gaussian illustrate this by considering n variables x1 x n each of which has a uniform distribution over the interval 0 1 and then considering the distribution of the mean x1 xn n',\n",
       " 'the gaussian distribution 79 n 1 0 0 5 1 0 1 2 3 n 2 0 0 5 1 0 1 2 3 n 1 0 0 0 5 1 0 1 2 3 figure 2 6 histogram plots of the mean of n uniformly distributed numbers for various values of n w e observe that as n increases the distribution tends towards a gaussian illustrate this by considering n variables x1 x n each of which has a uniform distribution over the interval 0 1 and then considering the distribution of the mean x1 xn n for large n this distribution tends to a gaussian as illustrated in figure 2 6 in practice the convergence to a gaussian as n increases can be very rapid',\n",
       " 'for large n this distribution tends to a gaussian as illustrated in figure 2 6 in practice the convergence to a gaussian as n increases can be very rapid one consequence of this result is that the binomial distribution 2 9 which is a distribution over m deÔ¨Åned by the sum of n observations of the random binary variable x will tend to a gaussian as n see figure 2 1 for the case of n 1 0 the gaussian distribution has many important analytical properties and we shall consider several of these in detail as a result this section will be rather more tech nically involved than some of the earlier sections and will require familiarity with various matrix identities',\n",
       " 'the gaussian distribution has many important analytical properties and we shall consider several of these in detail as a result this section will be rather more tech nically involved than some of the earlier sections and will require familiarity with various matrix identities however we strongly encourage the reader to become pro appendix c Ô¨Åcient in manipulating gaussian distributions using the techniques presented here as this will prove invaluable in understanding the more complex models presented in later chapters we begin by considering the geometrical form of the gaussian distribution',\n",
       " 'however we strongly encourage the reader to become pro appendix c Ô¨Åcient in manipulating gaussian distributions using the techniques presented here as this will prove invaluable in understanding the more complex models presented in later chapters we begin by considering the geometrical form of the gaussian distribution the carl friedrich gauss 1777 1855 it is said that when gauss went to elementary school at age 7 his teacher b uttner trying to keep the class occupied asked the pupils to sum the integers from 1 to 100 to the teacher s amazement gauss arrived at the answer in a matter of moments by noting that the sum can be represented as 50 pairs 1 100 2 99 etc each of which added to 101 giving the an swer 5 050',\n",
       " 'to the teacher s amazement gauss arrived at the answer in a matter of moments by noting that the sum can be represented as 50 pairs 1 100 2 99 etc each of which added to 101 giving the an swer 5 050 it is now believed that the problem which was actually set was of the same form but somewhat harder in that the sequence had a larger starting value and a larger increment gauss was a german math ematician and scientist with a reputation for being a hard working perfectionist one of his many contribu tions was to show that least squares can be derived under the assumption of normally distributed errors',\n",
       " 'gauss was a german math ematician and scientist with a reputation for being a hard working perfectionist one of his many contribu tions was to show that least squares can be derived under the assumption of normally distributed errors he also created an early formulation of non euclidean geometry a self consistent geometrical theory that vi olates the axioms of euclid but was reluctant to dis cuss it openly for fear that his reputation might suffer if it were seen that he believed in such a geometry at one point gauss was asked to conduct a geodetic survey of the state of hanover which led to his for mulation of the normal distribution now also known as the gaussian',\n",
       " 'he also created an early formulation of non euclidean geometry a self consistent geometrical theory that vi olates the axioms of euclid but was reluctant to dis cuss it openly for fear that his reputation might suffer if it were seen that he believed in such a geometry at one point gauss was asked to conduct a geodetic survey of the state of hanover which led to his for mulation of the normal distribution now also known as the gaussian after his death a study of his di aries revealed that he had discovered several impor tant mathematical results years or even decades be fore they were published by others 80 2 probability distributions functional dependence of the gaussian on x is through the quadratic form 2 x ¬µ tœÇ 1 x ¬µ 2 44 which appears in the exponent',\n",
       " '80 2 probability distributions functional dependence of the gaussian on x is through the quadratic form 2 x ¬µ tœÇ 1 x ¬µ 2 44 which appears in the exponent the quantity is called the mahalanobis distance from ¬µ to x and reduces to the euclidean distance whenœÇ is the identity matrix the gaussian distribution will be constant on surfaces inx space for which this quadratic form is constant first of all we note that the matrix œÉ can be taken to be symmetric without loss of generality because any antisymmetric component would disappear from the exponent now consider the eigenvector equation for the covariance matrixexercise 2 17 œÉui Œªiui 2 45 where i 1 d',\n",
       " 'first of all we note that the matrix œÉ can be taken to be symmetric without loss of generality because any antisymmetric component would disappear from the exponent now consider the eigenvector equation for the covariance matrixexercise 2 17 œÉui Œªiui 2 45 where i 1 d because œÉ is a real symmetric matrix its eigenvalues will be real and its eigenvectors can be chosen to form an orthonormal set so thatexercise 2 18 ut i uj iij 2 46 where iij is the i jelement of the identity matrix and satisÔ¨Åes iij 1 if i j 0 otherwise 2 47 the covariance matrix œÉ can be expressed as an expansion in terms of its eigenvec tors in the formexercise 2 19 œÉ d i 1 Œªiuiut i 2 48 and similarly the inverse covariance matrix œÉ 1 can be expressed as œÉ 1 d i 1 1 Œªi uiut i',\n",
       " 'because œÉ is a real symmetric matrix its eigenvalues will be real and its eigenvectors can be chosen to form an orthonormal set so thatexercise 2 18 ut i uj iij 2 46 where iij is the i jelement of the identity matrix and satisÔ¨Åes iij 1 if i j 0 otherwise 2 47 the covariance matrix œÉ can be expressed as an expansion in terms of its eigenvec tors in the formexercise 2 19 œÉ d i 1 Œªiuiut i 2 48 and similarly the inverse covariance matrix œÉ 1 can be expressed as œÉ 1 d i 1 1 Œªi uiut i 2 49 substituting 2 49 into 2 44 the quadratic form becomes 2 d i 1 y2 i Œªi 2 50 where we have deÔ¨Åned yi ut i x ¬µ',\n",
       " '2 47 the covariance matrix œÉ can be expressed as an expansion in terms of its eigenvec tors in the formexercise 2 19 œÉ d i 1 Œªiuiut i 2 48 and similarly the inverse covariance matrix œÉ 1 can be expressed as œÉ 1 d i 1 1 Œªi uiut i 2 49 substituting 2 49 into 2 44 the quadratic form becomes 2 d i 1 y2 i Œªi 2 50 where we have deÔ¨Åned yi ut i x ¬µ 2 51 we can interpret yi as a new coordinate system deÔ¨Åned by the orthonormal vectors ui that are shifted and rotated with respect to the original xi coordinates forming the vector y y1 y d t w eh a v e y u x ¬µ 2 52 2 3',\n",
       " '2 51 we can interpret yi as a new coordinate system deÔ¨Åned by the orthonormal vectors ui that are shifted and rotated with respect to the original xi coordinates forming the vector y y1 y d t w eh a v e y u x ¬µ 2 52 2 3 the gaussian distribution 81 figure 2 7 the red curve shows the ellip tical surface of constant proba bility density for a gaussian in a two dimensional space x x1 x 2 on which the density is exp 1 2 of its value at x ¬µ the major axes of the ellipse are deÔ¨Åned by the eigenvectors ui of the covari ance matrix with correspond ing eigenvalues Œªi x1 x2 Œª1 2 1 Œª1 2 2 y1 y2 u1 u2 ¬µ where u is a matrix whose rows are given by ut i',\n",
       " 'the major axes of the ellipse are deÔ¨Åned by the eigenvectors ui of the covari ance matrix with correspond ing eigenvalues Œªi x1 x2 Œª1 2 1 Œª1 2 2 y1 y2 u1 u2 ¬µ where u is a matrix whose rows are given by ut i from 2 46 it follows that u is an orthogonal matrix i e it satisÔ¨Åes uut i and hence also utu i where iappendix c is the identity matrix the quadratic form and hence the gaussian density will be constant on surfaces for which 2 51 is constant if all of the eigenvalues Œªi are positive then these surfaces represent ellipsoids with their centres at¬µ and their axes oriented alongui and with scaling factors in the directions of the axes given by Œª1 2 i as illustrated in figure 2 7',\n",
       " 'the quadratic form and hence the gaussian density will be constant on surfaces for which 2 51 is constant if all of the eigenvalues Œªi are positive then these surfaces represent ellipsoids with their centres at¬µ and their axes oriented alongui and with scaling factors in the directions of the axes given by Œª1 2 i as illustrated in figure 2 7 for the gaussian distribution to be well deÔ¨Åned it is necessary for all of the eigenvalues Œªi of the covariance matrix to be strictly positive otherwise the dis tribution cannot be properly normalized a matrix whose eigenvalues are strictly positive is said to be positive deÔ¨Ånite',\n",
       " 'for the gaussian distribution to be well deÔ¨Åned it is necessary for all of the eigenvalues Œªi of the covariance matrix to be strictly positive otherwise the dis tribution cannot be properly normalized a matrix whose eigenvalues are strictly positive is said to be positive deÔ¨Ånite in chapter 12 we will encounter gaussian distributions for which one or more of the eigenvalues are zero in which case the distribution is singular and is conÔ¨Åned to a subspace of lower dimensionality if all of the eigenvalues are nonnegative then the covariance matrix is said to be positive semideÔ¨Ånite now consider the form of the gaussian distribution in the new coordinate system deÔ¨Åned by theyi',\n",
       " 'if all of the eigenvalues are nonnegative then the covariance matrix is said to be positive semideÔ¨Ånite now consider the form of the gaussian distribution in the new coordinate system deÔ¨Åned by theyi in going from thex to the y coordinate system we have a jacobian matrix j with elements given by jij xi yj uji 2 53 where uji are the elements of the matrix ut using the orthonormality property of the matrix u we see that the square of the determinant of the jacobian matrix is j 2 ut 2 ut u utu i 1 2 54 and hence j 1 also the determinant œÉ of the covariance matrix can be written 82 2 probability distributions as the product of its eigenvalues and hence œÉ 1 2 d j 1 Œª1 2 j',\n",
       " 'also the determinant œÉ of the covariance matrix can be written 82 2 probability distributions as the product of its eigenvalues and hence œÉ 1 2 d j 1 Œª1 2 j 2 55 thus in the yj coordinate system the gaussian distribution takes the form p y p x j d j 1 1 2œÄŒªj 1 2 exp y2 j 2Œªj 2 56 which is the product of d independent univariate gaussian distributions the eigen vectors therefore deÔ¨Åne a new set of shifted and rotated coordinates with respect to which the joint probability distribution factorizes into a product of independent distributions the integral of the distribution in the y coordinate system is then p y d y d j 1 1 2œÄŒªj 1 2 exp y2 j 2Œªj dyj 1 2 57 where we have used the result 1 48 for the normalization of the univariate gaussian',\n",
       " 'the eigen vectors therefore deÔ¨Åne a new set of shifted and rotated coordinates with respect to which the joint probability distribution factorizes into a product of independent distributions the integral of the distribution in the y coordinate system is then p y d y d j 1 1 2œÄŒªj 1 2 exp y2 j 2Œªj dyj 1 2 57 where we have used the result 1 48 for the normalization of the univariate gaussian this conÔ¨Årms that the multivariate gaussian 2 43 is indeed normalized we now look at the moments of the gaussian distribution and thereby provide an interpretation of the parameters ¬µ and œÉ',\n",
       " 'this conÔ¨Årms that the multivariate gaussian 2 43 is indeed normalized we now look at the moments of the gaussian distribution and thereby provide an interpretation of the parameters ¬µ and œÉ the expectation of x under the gaussian distribution is given by e x 1 2œÄ d 2 1 œÉ 1 2 exp 1 2 x ¬µ tœÇ 1 x ¬µ xdx 1 2œÄ d 2 1 œÉ 1 2 exp 1 2ztœÇ 1z z ¬µ d z 2 58 where we have changed variables using z x ¬µ we now note that the exponent is an even function of the components of z and because the integrals over these are taken over the range the term in z in the factor z ¬µ will vanish by symmetry thus e x ¬µ 2 59 and so we refer to ¬µ as the mean of the gaussian distribution we now consider second order moments of the gaussian',\n",
       " 'thus e x ¬µ 2 59 and so we refer to ¬µ as the mean of the gaussian distribution we now consider second order moments of the gaussian in the univariate case we considered the second order moment given by e x2 for the multivariate gaus sian there are d2 second order moments given by e xixj which we can group together to form the matrix e xxt this matrix can be written as e xxt 1 2œÄ d 2 1 œÉ 1 2 exp 1 2 x ¬µ tœÇ 1 x ¬µ xxt dx 1 2œÄ d 2 1 œÉ 1 2 exp 1 2ztœÇ 1z z ¬µ z ¬µ t dz 2 3 the gaussian distribution 83 where again we have changed variables using z x ¬µ note that the cross terms involving ¬µzt and ¬µtz will again vanish by symmetry',\n",
       " 'the gaussian distribution 83 where again we have changed variables using z x ¬µ note that the cross terms involving ¬µzt and ¬µtz will again vanish by symmetry the term ¬µ¬µt is constant and can be taken outside the integral which itself is unity because the gaussian distribution is normalized consider the term involving zzt',\n",
       " 'the term ¬µ¬µt is constant and can be taken outside the integral which itself is unity because the gaussian distribution is normalized consider the term involving zzt again we can make use of the eigenvector expansion of the covariance matrix given by 2 45 together with the completeness of the set of eigenvectors to write z d j 1 yjuj 2 60 where yj ut j z which gives 1 2œÄ d 2 1 œÉ 1 2 exp 1 2ztœÇ 1z zzt dz 1 2œÄ d 2 1 œÉ 1 2 d i 1 d j 1 uiut j exp d k 1 y2 k 2Œªk yiyj dy d i 1 uiut i Œªi œÉ 2 61 where we have made use of the eigenvector equation 2 45 together with the fact that the integral on the right hand side of the middle line vanishes by symmetry unless i j and in the Ô¨Ånal line we have made use of the results 1 50 and 2 55 together with 2 48',\n",
       " 'consider the term involving zzt again we can make use of the eigenvector expansion of the covariance matrix given by 2 45 together with the completeness of the set of eigenvectors to write z d j 1 yjuj 2 60 where yj ut j z which gives 1 2œÄ d 2 1 œÉ 1 2 exp 1 2ztœÇ 1z zzt dz 1 2œÄ d 2 1 œÉ 1 2 d i 1 d j 1 uiut j exp d k 1 y2 k 2Œªk yiyj dy d i 1 uiut i Œªi œÉ 2 61 where we have made use of the eigenvector equation 2 45 together with the fact that the integral on the right hand side of the middle line vanishes by symmetry unless i j and in the Ô¨Ånal line we have made use of the results 1 50 and 2 55 together with 2 48 thus we have e xx t ¬µ¬µt œÉ',\n",
       " 'again we can make use of the eigenvector expansion of the covariance matrix given by 2 45 together with the completeness of the set of eigenvectors to write z d j 1 yjuj 2 60 where yj ut j z which gives 1 2œÄ d 2 1 œÉ 1 2 exp 1 2ztœÇ 1z zzt dz 1 2œÄ d 2 1 œÉ 1 2 d i 1 d j 1 uiut j exp d k 1 y2 k 2Œªk yiyj dy d i 1 uiut i Œªi œÉ 2 61 where we have made use of the eigenvector equation 2 45 together with the fact that the integral on the right hand side of the middle line vanishes by symmetry unless i j and in the Ô¨Ånal line we have made use of the results 1 50 and 2 55 together with 2 48 thus we have e xx t ¬µ¬µt œÉ 2 62 for single random variables we subtracted the mean before taking second mo ments in order to deÔ¨Åne a variance',\n",
       " 'thus we have e xx t ¬µ¬µt œÉ 2 62 for single random variables we subtracted the mean before taking second mo ments in order to deÔ¨Åne a variance similarly in the multivariate case it is again convenient to subtract off the mean giving rise to thecovariance of a random vector x deÔ¨Åned by cov x e x e x x e x t 2 63 for the speciÔ¨Åc case of a gaussian distribution we can make use of e x ¬µ together with the result 2 62 to give cov x œÉ 2 64 because the parameter matrix œÉ governs the covariance of x under the gaussian distribution it is called the covariance matrix although the gaussian distribution 2 43 is widely used as a density model it suffers from some signiÔ¨Åcant limitations consider the number of free parameters in the distribution',\n",
       " 'although the gaussian distribution 2 43 is widely used as a density model it suffers from some signiÔ¨Åcant limitations consider the number of free parameters in the distribution a general symmetric covariance matrix œÉ will have d d 1 2 independent parameters and there are another d independent parameters in ¬µ g i v exercise 2 21 ing d d 3 2 parameters in total for large d the total number of parameters 84 2 probability distributions figure 2 8 contours of constant probability density for a gaussian distribution in two dimensions in which the covariance matrix is a of general form b diagonal in which the elliptical contours are aligned with the coordinate axes and c proportional to the identity matrix in which the contours are concentric circles',\n",
       " 'for large d the total number of parameters 84 2 probability distributions figure 2 8 contours of constant probability density for a gaussian distribution in two dimensions in which the covariance matrix is a of general form b diagonal in which the elliptical contours are aligned with the coordinate axes and c proportional to the identity matrix in which the contours are concentric circles x1 x2 a x1 x2 b x1 x2 c therefore grows quadratically with d and the computational task of manipulating and inverting large matrices can become prohibitive one way to address this prob lem is to use restricted forms of the covariance matrix',\n",
       " 'x1 x2 a x1 x2 b x1 x2 c therefore grows quadratically with d and the computational task of manipulating and inverting large matrices can become prohibitive one way to address this prob lem is to use restricted forms of the covariance matrix if we consider covariance matrices that are diagonal so that œÉ diag œÉ2 i we then have a total of 2d inde pendent parameters in the density model the corresponding contours of constant density are given by axis aligned ellipsoids we could further restrict the covariance matrix to be proportional to the identity matrix œÉ œÉ2i known as an isotropic co variance giving d 1 independent parameters in the model and spherical surfaces of constant density',\n",
       " 'the corresponding contours of constant density are given by axis aligned ellipsoids we could further restrict the covariance matrix to be proportional to the identity matrix œÉ œÉ2i known as an isotropic co variance giving d 1 independent parameters in the model and spherical surfaces of constant density the three possibilities of general diagonal and isotropic covari ance matrices are illustrated in figure 2 8 unfortunately whereas such approaches limit the number of degrees of freedom in the distribution and make inversion of the covariance matrix a much faster operation they also greatly restrict the form of the probability density and limit its ability to capture interesting correlations in the data',\n",
       " 'the three possibilities of general diagonal and isotropic covari ance matrices are illustrated in figure 2 8 unfortunately whereas such approaches limit the number of degrees of freedom in the distribution and make inversion of the covariance matrix a much faster operation they also greatly restrict the form of the probability density and limit its ability to capture interesting correlations in the data a further limitation of the gaussian distribution is that it is intrinsically uni modal i e has a single maximum and so is unable to provide a good approximation to multimodal distributions thus the gaussian distribution can be both too Ô¨Çexible in the sense of having too many parameters while also being too limited in the range of distributions that it can adequately represent',\n",
       " 'a further limitation of the gaussian distribution is that it is intrinsically uni modal i e has a single maximum and so is unable to provide a good approximation to multimodal distributions thus the gaussian distribution can be both too Ô¨Çexible in the sense of having too many parameters while also being too limited in the range of distributions that it can adequately represent we will see later that the introduc tion of latent variables also called hidden variables or unobserved variables allows both of these problems to be addressed in particular a rich family of multimodal distributions is obtained by introducing discrete latent variables leading to mixtures of gaussians as discussed in section 2 3 9',\n",
       " 'we will see later that the introduc tion of latent variables also called hidden variables or unobserved variables allows both of these problems to be addressed in particular a rich family of multimodal distributions is obtained by introducing discrete latent variables leading to mixtures of gaussians as discussed in section 2 3 9 similarly the introduction of continuous latent variables as described in chapter 12 leads to models in which the number of free parameters can be controlled independently of the dimensionality d of the data space while still allowing the model to capture the dominant correlations in the data set',\n",
       " 'in particular a rich family of multimodal distributions is obtained by introducing discrete latent variables leading to mixtures of gaussians as discussed in section 2 3 9 similarly the introduction of continuous latent variables as described in chapter 12 leads to models in which the number of free parameters can be controlled independently of the dimensionality d of the data space while still allowing the model to capture the dominant correlations in the data set indeed these two approaches can be combined and further extended to derive a very rich set of hierarchical models that can be adapted to a broad range of prac tical applications',\n",
       " 'similarly the introduction of continuous latent variables as described in chapter 12 leads to models in which the number of free parameters can be controlled independently of the dimensionality d of the data space while still allowing the model to capture the dominant correlations in the data set indeed these two approaches can be combined and further extended to derive a very rich set of hierarchical models that can be adapted to a broad range of prac tical applications for instance the gaussian version of the markov random Ô¨Åeld section 8 3 which is widely used as a probabilistic model of images is a gaussian distribution over the joint space of pixel intensities but rendered tractable through the imposition of considerable structure reÔ¨Çecting the spatial organization of the pixels',\n",
       " 'indeed these two approaches can be combined and further extended to derive a very rich set of hierarchical models that can be adapted to a broad range of prac tical applications for instance the gaussian version of the markov random Ô¨Åeld section 8 3 which is widely used as a probabilistic model of images is a gaussian distribution over the joint space of pixel intensities but rendered tractable through the imposition of considerable structure reÔ¨Çecting the spatial organization of the pixels similarly the linear dynamical system used to model time series data for applications suchsection 13 3 as tracking is also a joint gaussian distribution over a potentially large number of observed and latent variables and again is tractable due to the structure imposed on the distribution',\n",
       " 'for instance the gaussian version of the markov random Ô¨Åeld section 8 3 which is widely used as a probabilistic model of images is a gaussian distribution over the joint space of pixel intensities but rendered tractable through the imposition of considerable structure reÔ¨Çecting the spatial organization of the pixels similarly the linear dynamical system used to model time series data for applications suchsection 13 3 as tracking is also a joint gaussian distribution over a potentially large number of observed and latent variables and again is tractable due to the structure imposed on the distribution a powerful framework for expressing the form and properties of 2 3',\n",
       " 'similarly the linear dynamical system used to model time series data for applications suchsection 13 3 as tracking is also a joint gaussian distribution over a potentially large number of observed and latent variables and again is tractable due to the structure imposed on the distribution a powerful framework for expressing the form and properties of 2 3 the gaussian distribution 85 such complex distributions is that of probabilistic graphical models which will form the subject of chapter 8 2 3 1 conditional gaussian distributions an important property of the multivariate gaussian distribution is that if two sets of variables are jointly gaussian then the conditional distribution of one set conditioned on the other is again gaussian',\n",
       " 'the gaussian distribution 85 such complex distributions is that of probabilistic graphical models which will form the subject of chapter 8 2 3 1 conditional gaussian distributions an important property of the multivariate gaussian distribution is that if two sets of variables are jointly gaussian then the conditional distribution of one set conditioned on the other is again gaussian similarly the marginal distribution of either set is also gaussian consider Ô¨Årst the case of conditional distributions supposexis a d dimensional vector with gaussian distribution n x ¬µ œÉ and that we partition x into two dis joint subsets xa and xb without loss of generality we can take xa to form the Ô¨Årst m components of x with xb comprising the remaining d m components so that x xa xb',\n",
       " 'supposexis a d dimensional vector with gaussian distribution n x ¬µ œÉ and that we partition x into two dis joint subsets xa and xb without loss of generality we can take xa to form the Ô¨Årst m components of x with xb comprising the remaining d m components so that x xa xb 2 65 we also deÔ¨Åne corresponding partitions of the mean vector ¬µ given by ¬µ ¬µa ¬µb 2 66 and of the covariance matrix œÉ given by œÉ œÉaa œÉab œÉba œÉbb 2 67 note that the symmetry œÉt œÉ of the covariance matrix implies that œÉaa and œÉbb are symmetric while œÉba œÉt ab in many situations it will be convenient to work with the inverse of the covari ance matrix Œª œÉ 1 2 68 which is known as the precision matrix',\n",
       " '2 67 note that the symmetry œÉt œÉ of the covariance matrix implies that œÉaa and œÉbb are symmetric while œÉba œÉt ab in many situations it will be convenient to work with the inverse of the covari ance matrix Œª œÉ 1 2 68 which is known as the precision matrix in fact we shall see that some properties of gaussian distributions are most naturally expressed in terms of the covariance whereas others take a simpler form when viewed in terms of the precision we therefore also introduce the partitioned form of the precision matrix Œª Œªaa Œªab Œªba Œªbb 2 69 corresponding to the partitioning 2 65 of the vector x because the inverse of a symmetric matrix is also symmetric we see that Œªaa and Œªbb are symmetric whileexercise 2 22 Œªt ab Œªba',\n",
       " 'we therefore also introduce the partitioned form of the precision matrix Œª Œªaa Œªab Œªba Œªbb 2 69 corresponding to the partitioning 2 65 of the vector x because the inverse of a symmetric matrix is also symmetric we see that Œªaa and Œªbb are symmetric whileexercise 2 22 Œªt ab Œªba it should be stressed at this point that for instance Œªaa is not simply given by the inverse of œÉaa in fact we shall shortly examine the relation between the inverse of a partitioned matrix and the inverses of its partitions let us begin by Ô¨Ånding an expression for the conditional distribution p xa xb from the product rule of probability we see that this conditional distribution can be 86 2',\n",
       " 'let us begin by Ô¨Ånding an expression for the conditional distribution p xa xb from the product rule of probability we see that this conditional distribution can be 86 2 probability distributions evaluated from the joint distribution p x p xa xb simply by Ô¨Åxing xb to the observed value and normalizing the resulting expression to obtain a valid probability distribution over x a instead of performing this normalization explicitly we can obtain the solution more efÔ¨Åciently by considering the quadratic form in the exponent of the gaussian distribution given by 2 44 and then reinstating the normalization coefÔ¨Åcient at the end of the calculation',\n",
       " 'probability distributions evaluated from the joint distribution p x p xa xb simply by Ô¨Åxing xb to the observed value and normalizing the resulting expression to obtain a valid probability distribution over x a instead of performing this normalization explicitly we can obtain the solution more efÔ¨Åciently by considering the quadratic form in the exponent of the gaussian distribution given by 2 44 and then reinstating the normalization coefÔ¨Åcient at the end of the calculation if we make use of the partitioning 2 65 2 66 and 2 69 we obtain 1 2 x ¬µ tœÇ 1 x ¬µ 1 2 xa ¬µa tŒªaa xa ¬µa 1 2 xa ¬µa tŒªab xb ¬µb 1 2 xb ¬µb tŒªba xa ¬µa 1 2 xb ¬µb tŒªbb xb ¬µb',\n",
       " 'instead of performing this normalization explicitly we can obtain the solution more efÔ¨Åciently by considering the quadratic form in the exponent of the gaussian distribution given by 2 44 and then reinstating the normalization coefÔ¨Åcient at the end of the calculation if we make use of the partitioning 2 65 2 66 and 2 69 we obtain 1 2 x ¬µ tœÇ 1 x ¬µ 1 2 xa ¬µa tŒªaa xa ¬µa 1 2 xa ¬µa tŒªab xb ¬µb 1 2 xb ¬µb tŒªba xa ¬µa 1 2 xb ¬µb tŒªbb xb ¬µb 2 70 we see that as a function of xa this is again a quadratic form and hence the cor responding conditional distribution p xa xb will be gaussian',\n",
       " 'if we make use of the partitioning 2 65 2 66 and 2 69 we obtain 1 2 x ¬µ tœÇ 1 x ¬µ 1 2 xa ¬µa tŒªaa xa ¬µa 1 2 xa ¬µa tŒªab xb ¬µb 1 2 xb ¬µb tŒªba xa ¬µa 1 2 xb ¬µb tŒªbb xb ¬µb 2 70 we see that as a function of xa this is again a quadratic form and hence the cor responding conditional distribution p xa xb will be gaussian because this distri bution is completely characterized by its mean and its covariance our goal will be to identify expressions for the mean and covariance of p x a xb by inspection of 2 70',\n",
       " '2 70 we see that as a function of xa this is again a quadratic form and hence the cor responding conditional distribution p xa xb will be gaussian because this distri bution is completely characterized by its mean and its covariance our goal will be to identify expressions for the mean and covariance of p x a xb by inspection of 2 70 this is an example of a rather common operation associated with gaussian distributions sometimes called completing the square in which we are given a quadratic form deÔ¨Åning the exponent terms in a gaussian distribution and we need to determine the corresponding mean and covariance',\n",
       " 'because this distri bution is completely characterized by its mean and its covariance our goal will be to identify expressions for the mean and covariance of p x a xb by inspection of 2 70 this is an example of a rather common operation associated with gaussian distributions sometimes called completing the square in which we are given a quadratic form deÔ¨Åning the exponent terms in a gaussian distribution and we need to determine the corresponding mean and covariance such problems can be solved straightforwardly by noting that the exponent in a general gaussian distribution n x ¬µ œÉ can be written 1 2 x ¬µ tœÇ 1 x ¬µ 1 2xtœÇ 1x xtœÇ 1¬µ c o n s t 2 71 where const denotes terms which are independent of x and we have made use of the symmetry of œÉ',\n",
       " 'this is an example of a rather common operation associated with gaussian distributions sometimes called completing the square in which we are given a quadratic form deÔ¨Åning the exponent terms in a gaussian distribution and we need to determine the corresponding mean and covariance such problems can be solved straightforwardly by noting that the exponent in a general gaussian distribution n x ¬µ œÉ can be written 1 2 x ¬µ tœÇ 1 x ¬µ 1 2xtœÇ 1x xtœÇ 1¬µ c o n s t 2 71 where const denotes terms which are independent of x and we have made use of the symmetry of œÉ thus if we take our general quadratic form and express it in the form given by the right hand side of 2 71 then we can immediately equate the matrix of coefÔ¨Åcients entering the second order term in x to the inverse covariance matrix œÉ 1 and the coefÔ¨Åcient of the linear term in x to œÉ 1¬µ from which we can obtain ¬µ',\n",
       " 'such problems can be solved straightforwardly by noting that the exponent in a general gaussian distribution n x ¬µ œÉ can be written 1 2 x ¬µ tœÇ 1 x ¬µ 1 2xtœÇ 1x xtœÇ 1¬µ c o n s t 2 71 where const denotes terms which are independent of x and we have made use of the symmetry of œÉ thus if we take our general quadratic form and express it in the form given by the right hand side of 2 71 then we can immediately equate the matrix of coefÔ¨Åcients entering the second order term in x to the inverse covariance matrix œÉ 1 and the coefÔ¨Åcient of the linear term in x to œÉ 1¬µ from which we can obtain ¬µ now let us apply this procedure to the conditional gaussian distributionp xa xb for which the quadratic form in the exponent is given by 2 70',\n",
       " 'thus if we take our general quadratic form and express it in the form given by the right hand side of 2 71 then we can immediately equate the matrix of coefÔ¨Åcients entering the second order term in x to the inverse covariance matrix œÉ 1 and the coefÔ¨Åcient of the linear term in x to œÉ 1¬µ from which we can obtain ¬µ now let us apply this procedure to the conditional gaussian distributionp xa xb for which the quadratic form in the exponent is given by 2 70 we will denote the mean and covariance of this distribution by ¬µ a b and œÉa b respectively consider the functional dependence of 2 70 on xa in which xb is regarded as a constant',\n",
       " 'we will denote the mean and covariance of this distribution by ¬µ a b and œÉa b respectively consider the functional dependence of 2 70 on xa in which xb is regarded as a constant if we pick out all terms that are second order in xa w eh a v e 1 2xt a Œªaaxa 2 72 from which we can immediately conclude that the covariance inverse precision of p xa xb is given by œÉa b Œª 1 aa 2 73 2 3 the gaussian distribution 87 now consider all of the terms in 2 70 that are linear in xa xt a Œªaa¬µa Œªab xb ¬µb 2 74 where we have used Œªt ba Œªab from our discussion of the general form 2 71 the coefÔ¨Åcient of xa in this expression must equal œÉ 1 a b¬µa b and hence ¬µa b œÉa b Œªaa¬µa Œªab xb ¬µb ¬µa Œª 1 aa Œªab xb ¬µb 2 75 where we have made use of 2 73',\n",
       " 'the gaussian distribution 87 now consider all of the terms in 2 70 that are linear in xa xt a Œªaa¬µa Œªab xb ¬µb 2 74 where we have used Œªt ba Œªab from our discussion of the general form 2 71 the coefÔ¨Åcient of xa in this expression must equal œÉ 1 a b¬µa b and hence ¬µa b œÉa b Œªaa¬µa Œªab xb ¬µb ¬µa Œª 1 aa Œªab xb ¬µb 2 75 where we have made use of 2 73 the results 2 73 and 2 75 are expressed in terms of the partitioned precision matrix of the original joint distribution p xa xb we can also express these results in terms of the corresponding partitioned covariance matrix',\n",
       " 'the results 2 73 and 2 75 are expressed in terms of the partitioned precision matrix of the original joint distribution p xa xb we can also express these results in terms of the corresponding partitioned covariance matrix to do this we make use of the following identity for the inverse of a partitioned matrixexercise 2 24 ab cd 1 m mbd 1 d 1cm d 1 d 1cmbd 1 2 76 where we have deÔ¨Åned m a bd 1c 1 2 77 the quantity m 1 is known as the schur complementof the matrix on the left hand side of 2 76 with respect to the submatrix d using the deÔ¨Ånition œÉaa œÉab œÉba œÉbb 1 Œªaa Œªab Œªba Œªbb 2 78 and making use of 2 76 we have Œªaa œÉaa œÉabœÇ 1 bb œÉba 1 2 79 Œªab œÉaa œÉabœÇ 1 bb œÉba 1œÉabœÇ 1 bb',\n",
       " 'to do this we make use of the following identity for the inverse of a partitioned matrixexercise 2 24 ab cd 1 m mbd 1 d 1cm d 1 d 1cmbd 1 2 76 where we have deÔ¨Åned m a bd 1c 1 2 77 the quantity m 1 is known as the schur complementof the matrix on the left hand side of 2 76 with respect to the submatrix d using the deÔ¨Ånition œÉaa œÉab œÉba œÉbb 1 Œªaa Œªab Œªba Œªbb 2 78 and making use of 2 76 we have Œªaa œÉaa œÉabœÇ 1 bb œÉba 1 2 79 Œªab œÉaa œÉabœÇ 1 bb œÉba 1œÉabœÇ 1 bb 2 80 from these we obtain the following expressions for the mean and covariance of the conditional distribution p xa xb ¬µa b ¬µa œÉabœÇ 1 bb xb ¬µb 2 81 œÉa b œÉaa œÉabœÇ 1 bb œÉba',\n",
       " '2 77 the quantity m 1 is known as the schur complementof the matrix on the left hand side of 2 76 with respect to the submatrix d using the deÔ¨Ånition œÉaa œÉab œÉba œÉbb 1 Œªaa Œªab Œªba Œªbb 2 78 and making use of 2 76 we have Œªaa œÉaa œÉabœÇ 1 bb œÉba 1 2 79 Œªab œÉaa œÉabœÇ 1 bb œÉba 1œÉabœÇ 1 bb 2 80 from these we obtain the following expressions for the mean and covariance of the conditional distribution p xa xb ¬µa b ¬µa œÉabœÇ 1 bb xb ¬µb 2 81 œÉa b œÉaa œÉabœÇ 1 bb œÉba 2 82 comparing 2 73 and 2 82 we see that the conditional distributionp xa xb takes a simpler form when expressed in terms of the partitioned precision matrix than when it is expressed in terms of the partitioned covariance matrix',\n",
       " '2 80 from these we obtain the following expressions for the mean and covariance of the conditional distribution p xa xb ¬µa b ¬µa œÉabœÇ 1 bb xb ¬µb 2 81 œÉa b œÉaa œÉabœÇ 1 bb œÉba 2 82 comparing 2 73 and 2 82 we see that the conditional distributionp xa xb takes a simpler form when expressed in terms of the partitioned precision matrix than when it is expressed in terms of the partitioned covariance matrix note that the mean of the conditional distributionp xa xb given by 2 81 is a linear function of xb and that the covariance given by 2 82 is independent ofxa this represents an example of a linear gaussian model section 8 1 4 88 2',\n",
       " 'note that the mean of the conditional distributionp xa xb given by 2 81 is a linear function of xb and that the covariance given by 2 82 is independent ofxa this represents an example of a linear gaussian model section 8 1 4 88 2 probability distributions 2 3 2 marginal gaussian distributions we have seen that if a joint distribution p xa xb is gaussian then the condi tional distribution p xa xb will again be gaussian now we turn to a discussion of the marginal distribution given by p xa p xa xb d xb 2 83 which as we shall see is also gaussian',\n",
       " 'probability distributions 2 3 2 marginal gaussian distributions we have seen that if a joint distribution p xa xb is gaussian then the condi tional distribution p xa xb will again be gaussian now we turn to a discussion of the marginal distribution given by p xa p xa xb d xb 2 83 which as we shall see is also gaussian once again our strategy for evaluating this distribution efÔ¨Åciently will be to focus on the quadratic form in the exponent of the joint distribution and thereby to identify the mean and covariance of the marginal distribution p xa the quadratic form for the joint distribution can be expressed using the par titioned precision matrix in the form 2 70',\n",
       " 'once again our strategy for evaluating this distribution efÔ¨Åciently will be to focus on the quadratic form in the exponent of the joint distribution and thereby to identify the mean and covariance of the marginal distribution p xa the quadratic form for the joint distribution can be expressed using the par titioned precision matrix in the form 2 70 because our goal is to integrate out xb this is most easily achieved by Ô¨Årst considering the terms involving xb and then completing the square in order to facilitate integration picking out just those terms that involve xb w eh a v e 1 2xt b Œªbbxb xt b m 1 2 xb Œª 1 bb m tŒªbb xb Œª 1 bb m 1 2mtŒª 1 bb m 2 84 where we have deÔ¨Åned m Œªbb¬µb Œªba xa ¬µa',\n",
       " 'because our goal is to integrate out xb this is most easily achieved by Ô¨Årst considering the terms involving xb and then completing the square in order to facilitate integration picking out just those terms that involve xb w eh a v e 1 2xt b Œªbbxb xt b m 1 2 xb Œª 1 bb m tŒªbb xb Œª 1 bb m 1 2mtŒª 1 bb m 2 84 where we have deÔ¨Åned m Œªbb¬µb Œªba xa ¬µa 2 85 we see that the dependence onxb has been cast into the standard quadratic form of a gaussian distribution corresponding to the Ô¨Årst term on the right hand side of 2 84 plus a term that does not depend on xb but that does depend on xa thus when we take the exponential of this quadratic form we see that the integration over xb required by 2 83 will take the form exp 1 2 xb Œª 1 bb m tŒªbb xb Œª 1 bb m dxb',\n",
       " '2 85 we see that the dependence onxb has been cast into the standard quadratic form of a gaussian distribution corresponding to the Ô¨Årst term on the right hand side of 2 84 plus a term that does not depend on xb but that does depend on xa thus when we take the exponential of this quadratic form we see that the integration over xb required by 2 83 will take the form exp 1 2 xb Œª 1 bb m tŒªbb xb Œª 1 bb m dxb 2 86 this integration is easily performed by noting that it is the integral over an unnor malized gaussian and so the result will be the reciprocal of the normalization co efÔ¨Åcient we know from the form of the normalized gaussian given by 2 43 that this coefÔ¨Åcient is independent of the mean and depends only on the determinant of the covariance matrix',\n",
       " '2 86 this integration is easily performed by noting that it is the integral over an unnor malized gaussian and so the result will be the reciprocal of the normalization co efÔ¨Åcient we know from the form of the normalized gaussian given by 2 43 that this coefÔ¨Åcient is independent of the mean and depends only on the determinant of the covariance matrix thus by completing the square with respect to x b we can integrate out xb and the only term remaining from the contributions on the left hand side of 2 84 that depends on xa is the last term on the right hand side of 2 84 in which m is given by 2 85 combining this term with the remaining terms from 2 3',\n",
       " 'thus by completing the square with respect to x b we can integrate out xb and the only term remaining from the contributions on the left hand side of 2 84 that depends on xa is the last term on the right hand side of 2 84 in which m is given by 2 85 combining this term with the remaining terms from 2 3 the gaussian distribution 89 2 70 that depend on xa we obtain 1 2 Œªbb¬µb Œªba xa ¬µa t Œª 1 bb Œªbb¬µb Œªba xa ¬µa 1 2xt a Œªaaxa xt a Œªaa¬µa Œªab¬µb c o n s t 1 2xt a Œªaa ŒªabŒª 1 bb Œªba xa xt a Œªaa ŒªabŒª 1 bb Œªba 1¬µa c o n s t 2 87 where const denotes quantities independent of xa again by comparison with 2 71 we see that the covariance of the marginal distribution of p xa is given by œÉa Œªaa ŒªabŒª 1 bb Œªba 1',\n",
       " 'the gaussian distribution 89 2 70 that depend on xa we obtain 1 2 Œªbb¬µb Œªba xa ¬µa t Œª 1 bb Œªbb¬µb Œªba xa ¬µa 1 2xt a Œªaaxa xt a Œªaa¬µa Œªab¬µb c o n s t 1 2xt a Œªaa ŒªabŒª 1 bb Œªba xa xt a Œªaa ŒªabŒª 1 bb Œªba 1¬µa c o n s t 2 87 where const denotes quantities independent of xa again by comparison with 2 71 we see that the covariance of the marginal distribution of p xa is given by œÉa Œªaa ŒªabŒª 1 bb Œªba 1 2 88 similarly the mean is given by œÉa Œªaa ŒªabŒª 1 bb Œªba ¬µa ¬µa 2 89 where we have used 2 88 the covariance in 2 88 is expressed in terms of the partitioned precision matrix given by 2 69 we can rewrite this in terms of the corresponding partitioning of the covariance matrix given by 2 67 as we did for the conditional distribution',\n",
       " 'the covariance in 2 88 is expressed in terms of the partitioned precision matrix given by 2 69 we can rewrite this in terms of the corresponding partitioning of the covariance matrix given by 2 67 as we did for the conditional distribution these partitioned matrices are related by Œªaa Œªab Œªba Œªbb 1 œÉaa œÉab œÉba œÉbb 2 90 making use of 2 76 we then have Œªaa ŒªabŒª 1 bb Œªba 1 œÉaa 2 91 thus we obtain the intuitively satisfying result that the marginal distribution p xa has mean and covariance given by e xa ¬µa 2 92 cov xa œÉaa',\n",
       " 'these partitioned matrices are related by Œªaa Œªab Œªba Œªbb 1 œÉaa œÉab œÉba œÉbb 2 90 making use of 2 76 we then have Œªaa ŒªabŒª 1 bb Œªba 1 œÉaa 2 91 thus we obtain the intuitively satisfying result that the marginal distribution p xa has mean and covariance given by e xa ¬µa 2 92 cov xa œÉaa 2 93 we see that for a marginal distribution the mean and covariance are most simply ex pressed in terms of the partitioned covariance matrix in contrast to the conditional distribution for which the partitioned precision matrix gives rise to simpler expres sions our results for the marginal and conditional distributions of a partitioned gaus sian are summarized below',\n",
       " '2 93 we see that for a marginal distribution the mean and covariance are most simply ex pressed in terms of the partitioned covariance matrix in contrast to the conditional distribution for which the partitioned precision matrix gives rise to simpler expres sions our results for the marginal and conditional distributions of a partitioned gaus sian are summarized below partitioned gaussians given a joint gaussian distribution n x ¬µ œÉ with Œª œÉ 1 and x xa xb ¬µ ¬µa ¬µb 2 94 90 2',\n",
       " 'our results for the marginal and conditional distributions of a partitioned gaus sian are summarized below partitioned gaussians given a joint gaussian distribution n x ¬µ œÉ with Œª œÉ 1 and x xa xb ¬µ ¬µa ¬µb 2 94 90 2 probability distributions xa xb 0 7 xb p xa xb 0 0 5 1 0 0 5 1 xa p xa p xa xb 0 7 0 0 5 1 0 5 10 figure 2 9 the plot on the left shows the contours of a gaussian distribution p xa x b over two variables and the plot on the right shows the marginal distribution p xa blue curve and the conditional distribution p xa xb for xb 0 7 red curve œÉ œÉaa œÉab œÉba œÉbb Œª Œªaa Œªab Œªba Œªbb 2 95 conditional distribution p xa xb n x ¬µa b Œª 1 aa 2 96 ¬µa b ¬µa Œª 1 aa Œªab xb ¬µb 2 97 marginal distribution p xa n xa ¬µa œÉaa',\n",
       " '2 95 conditional distribution p xa xb n x ¬µa b Œª 1 aa 2 96 ¬µa b ¬µa Œª 1 aa Œªab xb ¬µb 2 97 marginal distribution p xa n xa ¬µa œÉaa 2 98 we illustrate the idea of conditional and marginal distributions associated with a multivariate gaussian using an example involving two variables in figure 2 9 2 3 3 bayes theorem for gaussian variables in sections 2 3 1 and 2 3 2 we considered a gaussian p x in which we parti tioned the vector x into two subvectors x xa xb and then found expressions for the conditional distribution p xa xb and the marginal distribution p xa we noted that the mean of the conditional distribution p xa xb was a linear function of xb',\n",
       " '2 3 3 bayes theorem for gaussian variables in sections 2 3 1 and 2 3 2 we considered a gaussian p x in which we parti tioned the vector x into two subvectors x xa xb and then found expressions for the conditional distribution p xa xb and the marginal distribution p xa we noted that the mean of the conditional distribution p xa xb was a linear function of xb here we shall suppose that we are given a gaussian marginal distributionp x and a gaussian conditional distribution p y x in which p y x has a mean that is a linear function of x and a covariance which is independent of x this is an example of 2 3 the gaussian distribution 91 a linear gaussian model roweis and ghahramani 1999 which we shall study in greater generality in section 8 1 4',\n",
       " 'this is an example of 2 3 the gaussian distribution 91 a linear gaussian model roweis and ghahramani 1999 which we shall study in greater generality in section 8 1 4 we wish to Ô¨Ånd the marginal distribution p y and the conditional distribution p x y this is a problem that will arise frequently in subsequent chapters and it will prove convenient to derive the general results here we shall take the marginal and conditional distributions to be p x n x ¬µ Œª 1 2 99 p y x n y ax b l 1 2 100 where ¬µ a and b are parameters governing the means and Œª and l are precision matrices if x has dimensionality m and y has dimensionality d then the matrix a has size d m first we Ô¨Ånd an expression for the joint distribution overx and y',\n",
       " 'we shall take the marginal and conditional distributions to be p x n x ¬µ Œª 1 2 99 p y x n y ax b l 1 2 100 where ¬µ a and b are parameters governing the means and Œª and l are precision matrices if x has dimensionality m and y has dimensionality d then the matrix a has size d m first we Ô¨Ånd an expression for the joint distribution overx and y to do this we deÔ¨Åne z x y 2 101 and then consider the log of the joint distribution ln p z l n p x l n p y x 1 2 x ¬µ tŒª x ¬µ 1 2 y ax b tl y ax b const 2 102 where const denotes terms independent of x and y as before we see that this is a quadratic function of the components of z and hence p z is gaussian distribution',\n",
       " 'to do this we deÔ¨Åne z x y 2 101 and then consider the log of the joint distribution ln p z l n p x l n p y x 1 2 x ¬µ tŒª x ¬µ 1 2 y ax b tl y ax b const 2 102 where const denotes terms independent of x and y as before we see that this is a quadratic function of the components of z and hence p z is gaussian distribution to Ô¨Ånd the precision of this gaussian we consider the second order terms in 2 102 which can be written as 1 2xt Œª atla x 1 2ytly 1 2ytlax 1 2xtatly 1 2 x y t Œª atla atl la l x y 1 2ztrz 2 103 and so the gaussian distribution over z has precision inverse covariance matrix given by r Œª atla atl la l',\n",
       " 'as before we see that this is a quadratic function of the components of z and hence p z is gaussian distribution to Ô¨Ånd the precision of this gaussian we consider the second order terms in 2 102 which can be written as 1 2xt Œª atla x 1 2ytly 1 2ytlax 1 2xtatly 1 2 x y t Œª atla atl la l x y 1 2ztrz 2 103 and so the gaussian distribution over z has precision inverse covariance matrix given by r Œª atla atl la l 2 104 the covariance matrix is found by taking the inverse of the precision which can be done using the matrix inversion formula 2 76 to giveexercise 2 29 cov z r 1 Œª 1 Œª 1at aŒª 1 l 1 aŒª 1at 2 105 92 2',\n",
       " '2 104 the covariance matrix is found by taking the inverse of the precision which can be done using the matrix inversion formula 2 76 to giveexercise 2 29 cov z r 1 Œª 1 Œª 1at aŒª 1 l 1 aŒª 1at 2 105 92 2 probability distributions similarly we can Ô¨Ånd the mean of the gaussian distribution over z by identify ing the linear terms in 2 102 which are given by xtŒª¬µ xtatlb ytlb x y t Œª¬µ atlb lb 2 106 using our earlier result 2 71 obtained by completing the square over the quadratic form of a multivariate gaussian we Ô¨Ånd that the mean of z is given by e z r 1 Œª¬µ atlb lb 2 107 making use of 2 105 we then obtainexercise 2 30 e z ¬µ a¬µ b',\n",
       " '2 106 using our earlier result 2 71 obtained by completing the square over the quadratic form of a multivariate gaussian we Ô¨Ånd that the mean of z is given by e z r 1 Œª¬µ atlb lb 2 107 making use of 2 105 we then obtainexercise 2 30 e z ¬µ a¬µ b 2 108 next we Ô¨Ånd an expression for the marginal distribution p y in which we have marginalized over x recall that the marginal distribution over a subset of the com ponents of a gaussian random vector takes a particularly simple form when ex pressed in terms of the partitioned covariance matrix speciÔ¨Åcally its mean andsection 2 3 covariance are given by 2 92 and 2 93 respectively',\n",
       " 'recall that the marginal distribution over a subset of the com ponents of a gaussian random vector takes a particularly simple form when ex pressed in terms of the partitioned covariance matrix speciÔ¨Åcally its mean andsection 2 3 covariance are given by 2 92 and 2 93 respectively making use of 2 105 and 2 108 we see that the mean and covariance of the marginal distribution p y are given by e y a¬µ b 2 109 cov y l 1 aŒª 1at 2 110 a special case of this result is when a i in which case it reduces to the convolu tion of two gaussians for which we see that the mean of the convolution is the sum of the mean of the two gaussians and the covariance of the convolution is the sum of their covariances finally we seek an expression for the conditionalp x y',\n",
       " '2 110 a special case of this result is when a i in which case it reduces to the convolu tion of two gaussians for which we see that the mean of the convolution is the sum of the mean of the two gaussians and the covariance of the convolution is the sum of their covariances finally we seek an expression for the conditionalp x y recall that the results for the conditional distribution are most easily expressed in terms of the partitioned precision matrix using 2 73 and 2 75 applying these results to 2 105 andsection 2 3 2 108 we see that the conditional distribution p x y has mean and covariance given by e x y Œª a tla 1 atl y b Œª¬µ 2 111 cov x y Œª atla 1 2 112 the evaluation of this conditional can be seen as an example of bayes theorem',\n",
       " 'applying these results to 2 105 andsection 2 3 2 108 we see that the conditional distribution p x y has mean and covariance given by e x y Œª a tla 1 atl y b Œª¬µ 2 111 cov x y Œª atla 1 2 112 the evaluation of this conditional can be seen as an example of bayes theorem we can interpret the distribution p x as a prior distribution over x if the variable y is observed then the conditional distribution p x y represents the corresponding posterior distribution over x having found the marginal and conditional distribu tions we effectively expressed the joint distribution p z p x p y x in the form p x y p y these results are summarized below 2 3',\n",
       " 'these results are summarized below 2 3 the gaussian distribution 93 marginal and conditional gaussians given a marginal gaussian distribution for x and a conditional gaussian distri bution for y given x in the form p x n x ¬µ Œª 1 2 113 p y x n y ax b l 1 2 114 the marginal distribution of y and the conditional distribution of x given y are given by p y n y a¬µ b l 1 aŒª 1at 2 115 p x y n x œÉ atl y b Œª¬µ œÉ 2 116 where œÉ Œª atla 1 2 117 2 3 4 maximum likelihood for the gaussian given a data set x x1 xn t in which the observations xn are as sumed to be drawn independently from a multivariate gaussian distribution we can estimate the parameters of the distribution by maximum likelihood',\n",
       " 'the gaussian distribution 93 marginal and conditional gaussians given a marginal gaussian distribution for x and a conditional gaussian distri bution for y given x in the form p x n x ¬µ Œª 1 2 113 p y x n y ax b l 1 2 114 the marginal distribution of y and the conditional distribution of x given y are given by p y n y a¬µ b l 1 aŒª 1at 2 115 p x y n x œÉ atl y b Œª¬µ œÉ 2 116 where œÉ Œª atla 1 2 117 2 3 4 maximum likelihood for the gaussian given a data set x x1 xn t in which the observations xn are as sumed to be drawn independently from a multivariate gaussian distribution we can estimate the parameters of the distribution by maximum likelihood the log likeli hood function is given by lnp x ¬µ œÉ nd 2 ln 2œÄ n 2 ln œÉ 1 2 n n 1 xn ¬µ tœÇ 1 xn ¬µ',\n",
       " '2 117 2 3 4 maximum likelihood for the gaussian given a data set x x1 xn t in which the observations xn are as sumed to be drawn independently from a multivariate gaussian distribution we can estimate the parameters of the distribution by maximum likelihood the log likeli hood function is given by lnp x ¬µ œÉ nd 2 ln 2œÄ n 2 ln œÉ 1 2 n n 1 xn ¬µ tœÇ 1 xn ¬µ 2 118 by simple rearrangement we see that the likelihood function depends on the data set only through the two quantities n n 1 xn n n 1 xnxt n 2 119 these are known as the sufÔ¨Åcient statistics for the gaussian distribution',\n",
       " 'the log likeli hood function is given by lnp x ¬µ œÉ nd 2 ln 2œÄ n 2 ln œÉ 1 2 n n 1 xn ¬µ tœÇ 1 xn ¬µ 2 118 by simple rearrangement we see that the likelihood function depends on the data set only through the two quantities n n 1 xn n n 1 xnxt n 2 119 these are known as the sufÔ¨Åcient statistics for the gaussian distribution using c 19 the derivative of the log likelihood with respect to ¬µ is given byappendix c ¬µ ln p x ¬µ œÉ n n 1 œÉ 1 xn ¬µ 2 120 and setting this derivative to zero we obtain the solution for the maximum likelihood estimate of the mean given by ¬µml 1 n n n 1 xn 2 121 94 2 probability distributions which is the mean of the observed set of data points the maximization of 2 118 with respect to œÉ is rather more involved',\n",
       " 'probability distributions which is the mean of the observed set of data points the maximization of 2 118 with respect to œÉ is rather more involved the simplest approach is to ignore the symmetry constraint and show that the resulting solution is symmetric as required exercise 2 34 alternative derivations of this result which impose the symmetry and positive deÔ¨Å niteness constraints explicitly can be found in magnus and neudecker 1999 the result is as expected and takes the form œÉ ml 1 n n n 1 xn ¬µml xn ¬µml t 2 122 which involves ¬µml because this is the result of a joint maximization with respect to ¬µ and œÉ note that the solution 2 121 for ¬µml does not depend on œÉml and so we can Ô¨Årst evaluate ¬µml and then use this to evaluate œÉml',\n",
       " 'the result is as expected and takes the form œÉ ml 1 n n n 1 xn ¬µml xn ¬µml t 2 122 which involves ¬µml because this is the result of a joint maximization with respect to ¬µ and œÉ note that the solution 2 121 for ¬µml does not depend on œÉml and so we can Ô¨Årst evaluate ¬µml and then use this to evaluate œÉml if we evaluate the expectations of the maximum likelihood solutions under the true distribution we obtain the following resultsexercise 2 35 e ¬µml ¬µ 2 123 e œÉml n 1 n œÉ 2 124 we see that the expectation of the maximum likelihood estimate for the mean is equal to the true mean however the maximum likelihood estimate for the covariance has an expectation that is less than the true value and hence it is biased',\n",
       " '2 124 we see that the expectation of the maximum likelihood estimate for the mean is equal to the true mean however the maximum likelihood estimate for the covariance has an expectation that is less than the true value and hence it is biased we can correct this bias by deÔ¨Åning a different estimator œÉ given by œÉ 1 n 1 n n 1 xn ¬µml xn ¬µml t 2 125 clearly from 2 122 and 2 124 the expectation of œÉ is equal to œÉ 2 3 5 sequential estimation our discussion of the maximum likelihood solution for the parameters of a gaus sian distribution provides a convenient opportunity to give a more general discussion of the topic of sequential estimation for maximum likelihood',\n",
       " 'we can correct this bias by deÔ¨Åning a different estimator œÉ given by œÉ 1 n 1 n n 1 xn ¬µml xn ¬µml t 2 125 clearly from 2 122 and 2 124 the expectation of œÉ is equal to œÉ 2 3 5 sequential estimation our discussion of the maximum likelihood solution for the parameters of a gaus sian distribution provides a convenient opportunity to give a more general discussion of the topic of sequential estimation for maximum likelihood sequential methods allow data points to be processed one at a time and then discarded and are important for on line applications and also where large data sets are involved so that batch processing of all data points at once is infeasible',\n",
       " '2 3 5 sequential estimation our discussion of the maximum likelihood solution for the parameters of a gaus sian distribution provides a convenient opportunity to give a more general discussion of the topic of sequential estimation for maximum likelihood sequential methods allow data points to be processed one at a time and then discarded and are important for on line applications and also where large data sets are involved so that batch processing of all data points at once is infeasible consider the result 2 121 for the maximum likelihood estimator of the mean ¬µml which we will denote by ¬µ n ml when it is based on n observations if we 2 3',\n",
       " 'consider the result 2 121 for the maximum likelihood estimator of the mean ¬µml which we will denote by ¬µ n ml when it is based on n observations if we 2 3 the gaussian distribution 95 figure 2 10 a schematic illustration of two correlated ran dom variables z and Œ∏ together with the regression function f Œ∏ given by the con ditional expectation e z Œ∏ the robbins monro algorithm provides a general sequen tial procedure for Ô¨Ånding the root Œ∏ of such functions Œ∏ z Œ∏ f Œ∏ dissect out the contribution from the Ô¨Ånal data point xn we obtain ¬µ n ml 1 n n n 1 xn 1 n xn 1 n n 1 n 1 xn 1 n xn n 1 n ¬µ n 1 ml ¬µ n 1 ml 1 n xn ¬µ n 1 ml 2 126 this result has a nice interpretation as follows after observing n 1 data points we have estimated ¬µ by ¬µ n 1 ml',\n",
       " '2 126 this result has a nice interpretation as follows after observing n 1 data points we have estimated ¬µ by ¬µ n 1 ml we now observe data point xn and we obtain our revised estimate ¬µ n ml by moving the old estimate a small amount proportional to 1 n in the direction of the error signal xn ¬µ n 1 ml note that as n increases so the contribution from successive data points gets smaller the result 2 126 will clearly give the same answer as the batch result 2 121 because the two formulae are equivalent however we will not always be able to de rive a sequential algorithm by this route and so we seek a more general formulation of sequential learning which leads us to the robbins monro algorithm',\n",
       " 'the result 2 126 will clearly give the same answer as the batch result 2 121 because the two formulae are equivalent however we will not always be able to de rive a sequential algorithm by this route and so we seek a more general formulation of sequential learning which leads us to the robbins monro algorithm consider a pair of random variables Œ∏ and z governed by a joint distribution p z Œ∏ the con ditional expectation of z given Œ∏ deÔ¨Ånes a deterministic function f Œ∏ that is given by f Œ∏ e z Œ∏ zp z Œ∏ d z 2 127 and is illustrated schematically in figure 2 10 functions deÔ¨Åned in this way are called regression functions our goal is to Ô¨Ånd the root Œ∏ at which f Œ∏ 0',\n",
       " 'functions deÔ¨Åned in this way are called regression functions our goal is to Ô¨Ånd the root Œ∏ at which f Œ∏ 0 if we had a large data set of observations of z and Œ∏ then we could model the regression function directly and then obtain an estimate of its root suppose however that we observe values of z one at a time and we wish to Ô¨Ånd a corresponding sequential estimation scheme for Œ∏ the following general procedure for solving such problems was given by 96 2 probability distributions robbins and monro 1951 we shall assume that the conditional variance of z is Ô¨Ånite so that e z f 2 Œ∏ 2 128 and we shall also without loss of generality consider the case where f Œ∏ 0 for Œ∏ Œ∏ and f Œ∏ 0 for Œ∏ Œ∏ as is the case in figure 2 10',\n",
       " 'probability distributions robbins and monro 1951 we shall assume that the conditional variance of z is Ô¨Ånite so that e z f 2 Œ∏ 2 128 and we shall also without loss of generality consider the case where f Œ∏ 0 for Œ∏ Œ∏ and f Œ∏ 0 for Œ∏ Œ∏ as is the case in figure 2 10 the robbins monro procedure then deÔ¨Ånes a sequence of successive estimates of the root Œ∏ given by Œ∏ n Œ∏ n 1 an 1z Œ∏ n 1 2 129 where z Œ∏ n is an observed value ofz when Œ∏ takes the valueŒ∏ n the coefÔ¨Åcients an represent a sequence of positive numbers that satisfy the conditions lim n an 0 2 130 n 1 an 2 131 n 1 a2 n',\n",
       " 'the robbins monro procedure then deÔ¨Ånes a sequence of successive estimates of the root Œ∏ given by Œ∏ n Œ∏ n 1 an 1z Œ∏ n 1 2 129 where z Œ∏ n is an observed value ofz when Œ∏ takes the valueŒ∏ n the coefÔ¨Åcients an represent a sequence of positive numbers that satisfy the conditions lim n an 0 2 130 n 1 an 2 131 n 1 a2 n 2 132 it can then be shown robbins and monro 1951 fukunaga 1990 that the sequence of estimates given by 2 129 does indeed converge to the root with probability one note that the Ô¨Årst condition 2 130 ensures that the successive corrections decrease in magnitude so that the process can converge to a limiting value',\n",
       " '2 132 it can then be shown robbins and monro 1951 fukunaga 1990 that the sequence of estimates given by 2 129 does indeed converge to the root with probability one note that the Ô¨Årst condition 2 130 ensures that the successive corrections decrease in magnitude so that the process can converge to a limiting value the second con dition 2 131 is required to ensure that the algorithm does not converge short of the root and the third condition 2 132 is needed to ensure that the accumulated noise has Ô¨Ånite variance and hence does not spoil convergence now let us consider how a general maximum likelihood problem can be solved sequentially using the robbins monro algorithm',\n",
       " 'the second con dition 2 131 is required to ensure that the algorithm does not converge short of the root and the third condition 2 132 is needed to ensure that the accumulated noise has Ô¨Ånite variance and hence does not spoil convergence now let us consider how a general maximum likelihood problem can be solved sequentially using the robbins monro algorithm by deÔ¨Ånition the maximum like lihood solution Œ∏ ml is a stationary point of the log likelihood function and hence satisÔ¨Åes Œ∏ 1 n n n 1 lnp xn Œ∏ Œ∏ml 0 2 133 exchanging the derivative and the summation and taking the limitn we have lim n 1 n n n 1 Œ∏ lnp xn Œ∏ ex Œ∏ lnp x Œ∏ 2 134 and so we see that Ô¨Ånding the maximum likelihood solution corresponds to Ô¨Ånd ing the root of a regression function',\n",
       " 'by deÔ¨Ånition the maximum like lihood solution Œ∏ ml is a stationary point of the log likelihood function and hence satisÔ¨Åes Œ∏ 1 n n n 1 lnp xn Œ∏ Œ∏ml 0 2 133 exchanging the derivative and the summation and taking the limitn we have lim n 1 n n n 1 Œ∏ lnp xn Œ∏ ex Œ∏ lnp x Œ∏ 2 134 and so we see that Ô¨Ånding the maximum likelihood solution corresponds to Ô¨Ånd ing the root of a regression function we can therefore apply the robbins monro procedure which now takes the form Œ∏ n Œ∏ n 1 an 1 Œ∏ n 1 ln p xn Œ∏ n 1 2 135 2 3 the gaussian distribution 97 figure 2 11 in the case of a gaussian distribution with Œ∏ corresponding to the mean ¬µ the regression function illustrated in figure 2 10 takes the form of a straight line as shown in red',\n",
       " '2 135 2 3 the gaussian distribution 97 figure 2 11 in the case of a gaussian distribution with Œ∏ corresponding to the mean ¬µ the regression function illustrated in figure 2 10 takes the form of a straight line as shown in red in this case the random variable z corresponds to the derivative of the log likelihood function and is given by x ¬µml œÉ2 and its expectation that deÔ¨Ånes the regression function is a straight line given by ¬µ ¬µml œÉ2 the root of the regres sion function corresponds to the maximum like lihood estimator ¬µml',\n",
       " 'in this case the random variable z corresponds to the derivative of the log likelihood function and is given by x ¬µml œÉ2 and its expectation that deÔ¨Ånes the regression function is a straight line given by ¬µ ¬µml œÉ2 the root of the regres sion function corresponds to the maximum like lihood estimator ¬µml ¬µ z p z ¬µ ¬µml as a speciÔ¨Åc example we consider once again the sequential estimation of the mean of a gaussian distribution in which case the parameter Œ∏ n is the estimate ¬µ n ml of the mean of the gaussian and the random variable z is given by z ¬µml lnp x ¬µml œÉ 2 1 œÉ2 x ¬µml 2 136 thus the distribution of z is gaussian with mean ¬µ ¬µml as illustrated in fig ure 2 11',\n",
       " '¬µ z p z ¬µ ¬µml as a speciÔ¨Åc example we consider once again the sequential estimation of the mean of a gaussian distribution in which case the parameter Œ∏ n is the estimate ¬µ n ml of the mean of the gaussian and the random variable z is given by z ¬µml lnp x ¬µml œÉ 2 1 œÉ2 x ¬µml 2 136 thus the distribution of z is gaussian with mean ¬µ ¬µml as illustrated in fig ure 2 11 substituting 2 136 into 2 135 we obtain the univariate form of 2 126 provided we choose the coefÔ¨Åcients an to have the form an œÉ2 n note that although we have focussed on the case of a single variable the same technique together with the same restrictions 2 130 2 132 on the coefÔ¨Åcients an apply equally to the multivariate case blum 1965',\n",
       " 'substituting 2 136 into 2 135 we obtain the univariate form of 2 126 provided we choose the coefÔ¨Åcients an to have the form an œÉ2 n note that although we have focussed on the case of a single variable the same technique together with the same restrictions 2 130 2 132 on the coefÔ¨Åcients an apply equally to the multivariate case blum 1965 2 3 6 bayesian inference for the gaussian the maximum likelihood framework gave point estimates for the parameters ¬µ and œÉ now we develop a bayesian treatment by introducing prior distributions over these parameters let us begin with a simple example in which we consider a single gaussian random variable x',\n",
       " 'now we develop a bayesian treatment by introducing prior distributions over these parameters let us begin with a simple example in which we consider a single gaussian random variable x we shall suppose that the variance œÉ2 is known and we consider the task of inferring the mean ¬µ given a set of n observations x x1 x n the likelihood function that is the probability of the observed data given ¬µ viewed as a function of ¬µ i sg i v e nb y p x ¬µ n n 1 p xn ¬µ 1 2œÄœÉ2 n 2 exp 1 2œÉ2 n n 1 xn ¬µ 2 2 137 again we emphasize that the likelihood function p x ¬µ is not a probability distri bution over ¬µ and is not normalized we see that the likelihood function takes the form of the exponential of a quad ratic form in ¬µ',\n",
       " '2 137 again we emphasize that the likelihood function p x ¬µ is not a probability distri bution over ¬µ and is not normalized we see that the likelihood function takes the form of the exponential of a quad ratic form in ¬µ thus if we choose a prior p ¬µ given by a gaussian it will be a 98 2 probability distributions conjugate distribution for this likelihood function because the corresponding poste rior will be a product of two exponentials of quadratic functions of ¬µ and hence will also be gaussian we therefore take our prior distribution to be p ¬µ n ¬µ ¬µ0 œÉ 2 0 2 138 and the posterior distribution is given by p ¬µ x p x ¬µ p ¬µ',\n",
       " 'probability distributions conjugate distribution for this likelihood function because the corresponding poste rior will be a product of two exponentials of quadratic functions of ¬µ and hence will also be gaussian we therefore take our prior distribution to be p ¬µ n ¬µ ¬µ0 œÉ 2 0 2 138 and the posterior distribution is given by p ¬µ x p x ¬µ p ¬µ 2 139 simple manipulation involving completing the square in the exponent shows that theexercise 2 38 posterior distribution is given by p ¬µ x n ¬µ ¬µn œÉ 2 n 2 140 where ¬µn œÉ2 nœÉ2 0 œÉ2 ¬µ0 nœÉ2 0 nœÉ2 0 œÉ2 ¬µml 2 141 1 œÉ2 n 1 œÉ2 0 n œÉ2 2 142 in which ¬µml is the maximum likelihood solution for ¬µ given by the sample mean ¬µml 1 n n n 1 xn',\n",
       " 'we therefore take our prior distribution to be p ¬µ n ¬µ ¬µ0 œÉ 2 0 2 138 and the posterior distribution is given by p ¬µ x p x ¬µ p ¬µ 2 139 simple manipulation involving completing the square in the exponent shows that theexercise 2 38 posterior distribution is given by p ¬µ x n ¬µ ¬µn œÉ 2 n 2 140 where ¬µn œÉ2 nœÉ2 0 œÉ2 ¬µ0 nœÉ2 0 nœÉ2 0 œÉ2 ¬µml 2 141 1 œÉ2 n 1 œÉ2 0 n œÉ2 2 142 in which ¬µml is the maximum likelihood solution for ¬µ given by the sample mean ¬µml 1 n n n 1 xn 2 143 it is worth spending a moment studying the form of the posterior mean and variance first of all we note that the mean of the posterior distribution given by 2 141 is a compromise between the prior mean ¬µ0 and the maximum likelihood solution ¬µml',\n",
       " '2 143 it is worth spending a moment studying the form of the posterior mean and variance first of all we note that the mean of the posterior distribution given by 2 141 is a compromise between the prior mean ¬µ0 and the maximum likelihood solution ¬µml if the number of observed data points n 0 then 2 141 reduces to the prior mean as expected for n the posterior mean is given by the maximum likelihood solution similarly consider the result 2 142 for the variance of the posterior distribution we see that this is most naturally expressed in terms of the inverse variance which is called the precision',\n",
       " 'similarly consider the result 2 142 for the variance of the posterior distribution we see that this is most naturally expressed in terms of the inverse variance which is called the precision furthermore the precisions are additive so that the precision of the posterior is given by the precision of the prior plus one contribution of the data precision from each of the observed data points as we increase the number of observed data points the precision steadily increases corresponding to a posterior distribution with steadily decreasing variance with no observed data points we have the prior variance whereas if the number of data points n the variance œÉ 2 n goes to zero and the posterior distribution becomes inÔ¨Ånitely peaked around the maximum likelihood solution',\n",
       " 'as we increase the number of observed data points the precision steadily increases corresponding to a posterior distribution with steadily decreasing variance with no observed data points we have the prior variance whereas if the number of data points n the variance œÉ 2 n goes to zero and the posterior distribution becomes inÔ¨Ånitely peaked around the maximum likelihood solution we therefore see that the maximum likelihood result of a point estimate for ¬µ given by 2 143 is recovered precisely from the bayesian formalism in the limit of an inÔ¨Ånite number of observations',\n",
       " 'with no observed data points we have the prior variance whereas if the number of data points n the variance œÉ 2 n goes to zero and the posterior distribution becomes inÔ¨Ånitely peaked around the maximum likelihood solution we therefore see that the maximum likelihood result of a point estimate for ¬µ given by 2 143 is recovered precisely from the bayesian formalism in the limit of an inÔ¨Ånite number of observations note also that for Ô¨Åniten if we take the limitœÉ 2 0 in which the prior has inÔ¨Ånite variance then the posterior mean 2 141 reduces to the maximum likelihood result while from 2 142 the posterior variance is given byœÉ2 n œÉ2 n 2 3',\n",
       " 'note also that for Ô¨Åniten if we take the limitœÉ 2 0 in which the prior has inÔ¨Ånite variance then the posterior mean 2 141 reduces to the maximum likelihood result while from 2 142 the posterior variance is given byœÉ2 n œÉ2 n 2 3 the gaussian distribution 99 figure 2 12 illustration of bayesian inference for the mean ¬µ of a gaussian distri bution in which the variance is as sumed to be known the curves show the prior distribution over ¬µ the curve labelled n 0 which in this case is itself gaussian along with the posterior distribution given by 2 140 for increasing numbers n of data points the data points are generated from a gaussian of mean 0 8 and variance 0 1 and the prior is chosen to have mean 0',\n",
       " 'the curves show the prior distribution over ¬µ the curve labelled n 0 which in this case is itself gaussian along with the posterior distribution given by 2 140 for increasing numbers n of data points the data points are generated from a gaussian of mean 0 8 and variance 0 1 and the prior is chosen to have mean 0 in both the prior and the likelihood function the variance is set to the true value n 0 n 1 n 2 n 1 0 1 0 1 0 5 we illustrate our analysis of bayesian inference for the mean of a gaussian distribution in figure 2 12',\n",
       " 'in both the prior and the likelihood function the variance is set to the true value n 0 n 1 n 2 n 1 0 1 0 1 0 5 we illustrate our analysis of bayesian inference for the mean of a gaussian distribution in figure 2 12 the generalization of this result to the case of a d dimensional gaussian random variablex with known covariance and unknown mean is straightforward exercise 2 40 we have already seen how the maximum likelihood expression for the mean of a gaussian can be re cast as a sequential update formula in which the mean aftersection 2 3 5 observing n data points was expressed in terms of the mean after observing n 1 data points together with the contribution from data point xn in fact the bayesian paradigm leads very naturally to a sequential view of the inference problem',\n",
       " 'the generalization of this result to the case of a d dimensional gaussian random variablex with known covariance and unknown mean is straightforward exercise 2 40 we have already seen how the maximum likelihood expression for the mean of a gaussian can be re cast as a sequential update formula in which the mean aftersection 2 3 5 observing n data points was expressed in terms of the mean after observing n 1 data points together with the contribution from data point xn in fact the bayesian paradigm leads very naturally to a sequential view of the inference problem to see this in the context of the inference of the mean of a gaussian we write the posterior distribution with the contribution from the Ô¨Ånal data point xn separated out so that p ¬µ d p ¬µ n 1 n 1 p xn ¬µ p xn ¬µ',\n",
       " 'in fact the bayesian paradigm leads very naturally to a sequential view of the inference problem to see this in the context of the inference of the mean of a gaussian we write the posterior distribution with the contribution from the Ô¨Ånal data point xn separated out so that p ¬µ d p ¬µ n 1 n 1 p xn ¬µ p xn ¬µ 2 144 the term in square brackets is up to a normalization coefÔ¨Åcient just the posterior distribution after observing n 1 data points we see that this can be viewed as a prior distribution which is combined using bayes theorem with the likelihood function associated with data point xn to arrive at the posterior distribution after observing n data points',\n",
       " '2 144 the term in square brackets is up to a normalization coefÔ¨Åcient just the posterior distribution after observing n 1 data points we see that this can be viewed as a prior distribution which is combined using bayes theorem with the likelihood function associated with data point xn to arrive at the posterior distribution after observing n data points this sequential view of bayesian inference is very general and applies to any problem in which the observed data are assumed to be independent and identically distributed so far we have assumed that the variance of the gaussian distribution over the data is known and our goal is to infer the mean now let us suppose that the mean is known and we wish to infer the variance',\n",
       " 'so far we have assumed that the variance of the gaussian distribution over the data is known and our goal is to infer the mean now let us suppose that the mean is known and we wish to infer the variance again our calculations will be greatly simpliÔ¨Åed if we choose a conjugate form for the prior distribution it turns out to be most convenient to work with the precisionŒª 1 œÉ2 the likelihood function for Œª takes the form p x Œª n n 1 n xn ¬µ Œª 1 Œªn 2 exp Œª 2 n n 1 xn ¬µ 2 2 145 100 2 probability distributions Œª a 0 1 b 0 1 0 1 2 0 1 2 Œª a 1 b 1 0 1 2 0 1 2 Œª a 4 b 6 0 1 2 0 1 2 figure 2 13 plot of the gamma distribution gam Œª a b deÔ¨Åned by 2 146 for various values of the parameters a and b',\n",
       " '2 145 100 2 probability distributions Œª a 0 1 b 0 1 0 1 2 0 1 2 Œª a 1 b 1 0 1 2 0 1 2 Œª a 4 b 6 0 1 2 0 1 2 figure 2 13 plot of the gamma distribution gam Œª a b deÔ¨Åned by 2 146 for various values of the parameters a and b the corresponding conjugate prior should therefore be proportional to the product of a power of Œª and the exponential of a linear function of Œª this corresponds to the gamma distribution which is deÔ¨Åned by gam Œª a b 1 Œ≥ a baŒªa 1 exp bŒª 2 146 here Œ≥ a is the gamma function that is deÔ¨Åned by 1 141 and that ensures that 2 146 is correctly normalized the gamma distribution has a Ô¨Ånite integral ifa 0 exercise 2 41 and the distribution itself is Ô¨Ånite if a 1 it is plotted for various values of a and b in figure 2 13',\n",
       " 'the gamma distribution has a Ô¨Ånite integral ifa 0 exercise 2 41 and the distribution itself is Ô¨Ånite if a 1 it is plotted for various values of a and b in figure 2 13 the mean and variance of the gamma distribution are given byexercise 2 42 e Œª a b 2 147 var Œª a b2 2 148 consider a prior distribution gam Œª a0 b 0 if we multiply by the likelihood function 2 145 then we obtain a posterior distribution p Œª x Œªa0 1Œªn 2 exp b0Œª Œª 2 n n 1 xn ¬µ 2 2 149 which we recognize as a gamma distribution of the form gam Œª an b n where an a0 n 2 2 150 bn b0 1 2 n n 1 xn ¬µ 2 b0 n 2 œÉ2 ml 2 151 where œÉ2 ml is the maximum likelihood estimator of the variance',\n",
       " '2 148 consider a prior distribution gam Œª a0 b 0 if we multiply by the likelihood function 2 145 then we obtain a posterior distribution p Œª x Œªa0 1Œªn 2 exp b0Œª Œª 2 n n 1 xn ¬µ 2 2 149 which we recognize as a gamma distribution of the form gam Œª an b n where an a0 n 2 2 150 bn b0 1 2 n n 1 xn ¬µ 2 b0 n 2 œÉ2 ml 2 151 where œÉ2 ml is the maximum likelihood estimator of the variance note that in 2 149 there is no need to keep track of the normalization constants in the prior and the likelihood function because if required the correct coefÔ¨Åcient can be found at the end using the normalized form 2 146 for the gamma distribution 2 3',\n",
       " 'note that in 2 149 there is no need to keep track of the normalization constants in the prior and the likelihood function because if required the correct coefÔ¨Åcient can be found at the end using the normalized form 2 146 for the gamma distribution 2 3 the gaussian distribution 101 from 2 150 we see that the effect of observing n data points is to increase the value of the coefÔ¨Åcient a by n 2 thus we can interpret the parameter a0 in the prior in terms of 2a0 effective prior observations similarly from 2 151 we see that the n data points contribute nœÉ2 ml 2 to the parameter b where œÉ2 ml is the variance and so we can interpret the parameter b0 in the prior as arising from the 2a0 effective prior observations having variance 2b0 2a0 b0 a0',\n",
       " 'thus we can interpret the parameter a0 in the prior in terms of 2a0 effective prior observations similarly from 2 151 we see that the n data points contribute nœÉ2 ml 2 to the parameter b where œÉ2 ml is the variance and so we can interpret the parameter b0 in the prior as arising from the 2a0 effective prior observations having variance 2b0 2a0 b0 a0 recall that we made an analogous interpretation for the dirichlet prior these distributionssection 2 2 are examples of the exponential family and we shall see that the interpretation of a conjugate prior in terms of effective Ô¨Åctitious data points is a general one for the exponential family of distributions instead of working with the precision we can consider the variance itself',\n",
       " 'these distributionssection 2 2 are examples of the exponential family and we shall see that the interpretation of a conjugate prior in terms of effective Ô¨Åctitious data points is a general one for the exponential family of distributions instead of working with the precision we can consider the variance itself the conjugate prior in this case is called the inverse gamma distribution although we shall not discuss this further because we will Ô¨Ånd it more convenient to work with the precision now suppose that both the mean and the precision are unknown to Ô¨Ånd a conjugate prior we consider the dependence of the likelihood function on ¬µ and Œª p x ¬µ Œª n n 1 Œª 2œÄ 1 2 exp Œª 2 xn ¬µ 2 Œª1 2 exp Œª¬µ2 2 n exp Œª¬µ n n 1 xn Œª 2 n n 1 x2 n',\n",
       " 'now suppose that both the mean and the precision are unknown to Ô¨Ånd a conjugate prior we consider the dependence of the likelihood function on ¬µ and Œª p x ¬µ Œª n n 1 Œª 2œÄ 1 2 exp Œª 2 xn ¬µ 2 Œª1 2 exp Œª¬µ2 2 n exp Œª¬µ n n 1 xn Œª 2 n n 1 x2 n 2 152 we now wish to identify a prior distribution p ¬µ Œª that has the same functional dependence on ¬µ and Œª as the likelihood function and that should therefore take the form p ¬µ Œª Œª1 2 exp Œª¬µ2 2 Œ≤ exp cŒª¬µ dŒª e x p Œ≤Œª 2 ¬µ c Œ≤ 2 ŒªŒ≤ 2 exp d c2 2Œ≤ Œª 2 153 where c d and Œ≤ are constants since we can always write p ¬µ Œª p ¬µ Œª p Œª we can Ô¨Ånd p ¬µ Œª and p Œª by inspection',\n",
       " '2 152 we now wish to identify a prior distribution p ¬µ Œª that has the same functional dependence on ¬µ and Œª as the likelihood function and that should therefore take the form p ¬µ Œª Œª1 2 exp Œª¬µ2 2 Œ≤ exp cŒª¬µ dŒª e x p Œ≤Œª 2 ¬µ c Œ≤ 2 ŒªŒ≤ 2 exp d c2 2Œ≤ Œª 2 153 where c d and Œ≤ are constants since we can always write p ¬µ Œª p ¬µ Œª p Œª we can Ô¨Ånd p ¬µ Œª and p Œª by inspection in particular we see that p ¬µ Œª is a gaussian whose precision is a linear function of Œª and that p Œª is a gamma distri bution so that the normalized prior takes the form p ¬µ Œª n ¬µ ¬µ0 Œ≤Œª 1 gam Œª a b 2 154 where we have deÔ¨Åned new constants given by ¬µ0 c Œ≤ a 1 Œ≤ 2 b d c2 2Œ≤',\n",
       " 'since we can always write p ¬µ Œª p ¬µ Œª p Œª we can Ô¨Ånd p ¬µ Œª and p Œª by inspection in particular we see that p ¬µ Œª is a gaussian whose precision is a linear function of Œª and that p Œª is a gamma distri bution so that the normalized prior takes the form p ¬µ Œª n ¬µ ¬µ0 Œ≤Œª 1 gam Œª a b 2 154 where we have deÔ¨Åned new constants given by ¬µ0 c Œ≤ a 1 Œ≤ 2 b d c2 2Œ≤ the distribution 2 154 is called thenormal gamma or gaussian gamma distribution and is plotted in figure 2 14 note that this is not simply the product of an independent gaussian prior over ¬µ and a gamma prior over Œª because the precision of ¬µ is a linear function of Œª',\n",
       " 'the distribution 2 154 is called thenormal gamma or gaussian gamma distribution and is plotted in figure 2 14 note that this is not simply the product of an independent gaussian prior over ¬µ and a gamma prior over Œª because the precision of ¬µ is a linear function of Œª even if we chose a prior in which ¬µ and Œª were independent the posterior distribution would exhibit a coupling between the precision of ¬µ and the value of Œª 102 2 probability distributions figure 2 14 contour plot of the normal gamma distribution 2 154 for parameter values ¬µ0 0 Œ≤ 2 a 5 and b 6 ¬µ Œª 2 0 2 0 1 2 in the case of the multivariate gaussian distribution n x ¬µ Œª 1 for a d dimensional variable x the conjugate prior distribution for the mean ¬µ assuming the precision is known is again a gaussian',\n",
       " 'probability distributions figure 2 14 contour plot of the normal gamma distribution 2 154 for parameter values ¬µ0 0 Œ≤ 2 a 5 and b 6 ¬µ Œª 2 0 2 0 1 2 in the case of the multivariate gaussian distribution n x ¬µ Œª 1 for a d dimensional variable x the conjugate prior distribution for the mean ¬µ assuming the precision is known is again a gaussian for known mean and unknown precision matrix Œª the conjugate prior is the wishart distribution given byexercise 2 45 w Œª w ŒΩ b Œª ŒΩ d 1 2 exp 1 2tr w 1Œª 2 155 where ŒΩ is called the number ofdegrees of freedomof the distribution w is a d d scale matrix and tr denotes the trace the normalization constant b is given by b w ŒΩ w ŒΩ 2 2ŒΩd 2 œÄd d 1 4 d i 1 Œ≥ ŒΩ 1 i 2 1',\n",
       " 'for known mean and unknown precision matrix Œª the conjugate prior is the wishart distribution given byexercise 2 45 w Œª w ŒΩ b Œª ŒΩ d 1 2 exp 1 2tr w 1Œª 2 155 where ŒΩ is called the number ofdegrees of freedomof the distribution w is a d d scale matrix and tr denotes the trace the normalization constant b is given by b w ŒΩ w ŒΩ 2 2ŒΩd 2 œÄd d 1 4 d i 1 Œ≥ ŒΩ 1 i 2 1 2 156 again it is also possible to deÔ¨Åne a conjugate prior over the covariance matrix itself rather than over the precision matrix which leads to the inverse wishart distribu tion although we shall not discuss this further',\n",
       " 'the normalization constant b is given by b w ŒΩ w ŒΩ 2 2ŒΩd 2 œÄd d 1 4 d i 1 Œ≥ ŒΩ 1 i 2 1 2 156 again it is also possible to deÔ¨Åne a conjugate prior over the covariance matrix itself rather than over the precision matrix which leads to the inverse wishart distribu tion although we shall not discuss this further if both the mean and the precision are unknown then following a similar line of reasoning to the univariate case the conjugate prior is given by p ¬µ Œª ¬µ0 Œ≤ w ŒΩ n ¬µ ¬µ0 Œ≤Œª 1 w Œª w ŒΩ 2 157 which is known as the normal wishart or gaussian wishart distribution 2 3 7 student s t distribution we have seen that the conjugate prior for the precision of a gaussian is given by a gamma distribution',\n",
       " 'if both the mean and the precision are unknown then following a similar line of reasoning to the univariate case the conjugate prior is given by p ¬µ Œª ¬µ0 Œ≤ w ŒΩ n ¬µ ¬µ0 Œ≤Œª 1 w Œª w ŒΩ 2 157 which is known as the normal wishart or gaussian wishart distribution 2 3 7 student s t distribution we have seen that the conjugate prior for the precision of a gaussian is given by a gamma distribution if we have a univariate gaussian n x ¬µ œÑ 1 togethersection 2 3 6 with a gamma prior gam œÑ a b and we integrate out the precision we obtain the marginal distribution of x in the formexercise 2 46 2 3 the gaussian distribution 103 figure 2 15 plot of student s t distribution 2 159 for ¬µ 0 and Œª 1 for various values of ŒΩ',\n",
       " 'if we have a univariate gaussian n x ¬µ œÑ 1 togethersection 2 3 6 with a gamma prior gam œÑ a b and we integrate out the precision we obtain the marginal distribution of x in the formexercise 2 46 2 3 the gaussian distribution 103 figure 2 15 plot of student s t distribution 2 159 for ¬µ 0 and Œª 1 for various values of ŒΩ the limit ŒΩ corresponds to a gaussian distribution with mean ¬µ and precision Œª ŒΩ ŒΩ 1 0 ŒΩ 0 1 5 0 5 0 0 1 0 2 0 3 0 4 0 5 p x ¬µ a b 0 n x ¬µ œÑ 1 gam œÑ a b d œÑ 2 158 0 bae bœÑ œÑa 1 Œ≥ a œÑ 2œÄ 1 2 exp œÑ 2 x ¬µ 2 dœÑ ba Œ≥ a 1 2œÄ 1 2 b x ¬µ 2 2 a 1 2 Œ≥ a 1 2 where we have made the change of variable z œÑ b x ¬µ 2 2',\n",
       " 'the gaussian distribution 103 figure 2 15 plot of student s t distribution 2 159 for ¬µ 0 and Œª 1 for various values of ŒΩ the limit ŒΩ corresponds to a gaussian distribution with mean ¬µ and precision Œª ŒΩ ŒΩ 1 0 ŒΩ 0 1 5 0 5 0 0 1 0 2 0 3 0 4 0 5 p x ¬µ a b 0 n x ¬µ œÑ 1 gam œÑ a b d œÑ 2 158 0 bae bœÑ œÑa 1 Œ≥ a œÑ 2œÄ 1 2 exp œÑ 2 x ¬µ 2 dœÑ ba Œ≥ a 1 2œÄ 1 2 b x ¬µ 2 2 a 1 2 Œ≥ a 1 2 where we have made the change of variable z œÑ b x ¬µ 2 2 by convention we deÔ¨Åne new parameters given by ŒΩ 2 a and Œª a b in terms of which the distribution p x ¬µ a b takes the form st x ¬µ Œª ŒΩ Œ≥ ŒΩ 2 1 2 Œ≥ ŒΩ 2 Œª œÄŒΩ 1 2 1 Œª x ¬µ 2 ŒΩ ŒΩ 2 1 2 2 159 which is known as student s t distribution',\n",
       " 'the limit ŒΩ corresponds to a gaussian distribution with mean ¬µ and precision Œª ŒΩ ŒΩ 1 0 ŒΩ 0 1 5 0 5 0 0 1 0 2 0 3 0 4 0 5 p x ¬µ a b 0 n x ¬µ œÑ 1 gam œÑ a b d œÑ 2 158 0 bae bœÑ œÑa 1 Œ≥ a œÑ 2œÄ 1 2 exp œÑ 2 x ¬µ 2 dœÑ ba Œ≥ a 1 2œÄ 1 2 b x ¬µ 2 2 a 1 2 Œ≥ a 1 2 where we have made the change of variable z œÑ b x ¬µ 2 2 by convention we deÔ¨Åne new parameters given by ŒΩ 2 a and Œª a b in terms of which the distribution p x ¬µ a b takes the form st x ¬µ Œª ŒΩ Œ≥ ŒΩ 2 1 2 Œ≥ ŒΩ 2 Œª œÄŒΩ 1 2 1 Œª x ¬µ 2 ŒΩ ŒΩ 2 1 2 2 159 which is known as student s t distribution the parameter Œª is sometimes called the precision of the t distribution even though it is not in general equal to the inverse of the variance',\n",
       " 'by convention we deÔ¨Åne new parameters given by ŒΩ 2 a and Œª a b in terms of which the distribution p x ¬µ a b takes the form st x ¬µ Œª ŒΩ Œ≥ ŒΩ 2 1 2 Œ≥ ŒΩ 2 Œª œÄŒΩ 1 2 1 Œª x ¬µ 2 ŒΩ ŒΩ 2 1 2 2 159 which is known as student s t distribution the parameter Œª is sometimes called the precision of the t distribution even though it is not in general equal to the inverse of the variance the parameter ŒΩ is called the degrees of freedom and its effect is illustrated in figure 2 15',\n",
       " 'the parameter Œª is sometimes called the precision of the t distribution even though it is not in general equal to the inverse of the variance the parameter ŒΩ is called the degrees of freedom and its effect is illustrated in figure 2 15 for the particular case of ŒΩ 1 the t distribution reduces to the cauchy distribution while in the limit ŒΩ the t distribution st x ¬µ Œª ŒΩ becomes a gaussian n x ¬µ Œª 1 with mean ¬µ and precision Œª exercise 2 47 from 2 158 we see that student s t distribution is obtained by adding up an inÔ¨Ånite number of gaussian distributions having the same mean but different preci sions this can be interpreted as an inÔ¨Ånite mixture of gaussians gaussian mixtures will be discussed in detail in section 2 3 9',\n",
       " 'for the particular case of ŒΩ 1 the t distribution reduces to the cauchy distribution while in the limit ŒΩ the t distribution st x ¬µ Œª ŒΩ becomes a gaussian n x ¬µ Œª 1 with mean ¬µ and precision Œª exercise 2 47 from 2 158 we see that student s t distribution is obtained by adding up an inÔ¨Ånite number of gaussian distributions having the same mean but different preci sions this can be interpreted as an inÔ¨Ånite mixture of gaussians gaussian mixtures will be discussed in detail in section 2 3 9 the result is a distribution that in gen eral has longer tails than a gaussian as was seen in figure 2 15',\n",
       " 'this can be interpreted as an inÔ¨Ånite mixture of gaussians gaussian mixtures will be discussed in detail in section 2 3 9 the result is a distribution that in gen eral has longer tails than a gaussian as was seen in figure 2 15 this gives the t distribution an important property calledrobustness which means that it is much less sensitive than the gaussian to the presence of a few data points which are outliers the robustness of the t distribution is illustrated in figure 2 16 which compares the maximum likelihood solutions for a gaussian and a t distribution note that the max imum likelihood solution for the t distribution can be found using the expectation maximization em algorithm here we see that the effect of a small number ofexercise 12 24 104 2',\n",
       " 'note that the max imum likelihood solution for the t distribution can be found using the expectation maximization em algorithm here we see that the effect of a small number ofexercise 12 24 104 2 probability distributions a 5 0 5 10 0 0 1 0 2 0 3 0 4 0 5 b 5 0 5 10 0 0 1 0 2 0 3 0 4 0 5 figure 2 16 illustration of the robustness of student s t distribution compared to a gaussian a histogram distribution of 30 data points drawn from a gaussian distribution together with the maximum likelihood Ô¨Åt ob tained from a t distribution red curve and a gaussian green curve largely hidden by the red curve because the t distribution contains the gaussian as a special case it gives almost the same solution as the gaussian',\n",
       " 'a histogram distribution of 30 data points drawn from a gaussian distribution together with the maximum likelihood Ô¨Åt ob tained from a t distribution red curve and a gaussian green curve largely hidden by the red curve because the t distribution contains the gaussian as a special case it gives almost the same solution as the gaussian b the same data set but with three additional outlying data points showing how the gaussian green curve is strongly distorted by the outliers whereas the t distribution red curve is relatively unaffected outliers is much less signiÔ¨Åcant for the t distribution than for the gaussian',\n",
       " 'b the same data set but with three additional outlying data points showing how the gaussian green curve is strongly distorted by the outliers whereas the t distribution red curve is relatively unaffected outliers is much less signiÔ¨Åcant for the t distribution than for the gaussian outliers can arise in practical applications either because the process that generates the data corresponds to a distribution having a heavy tail or simply through mislabelled data robustness is also an important property for regression problems unsurprisingly the least squares approach to regression does not exhibit robustness because it cor responds to maximum likelihood under a conditional gaussian distribution',\n",
       " 'robustness is also an important property for regression problems unsurprisingly the least squares approach to regression does not exhibit robustness because it cor responds to maximum likelihood under a conditional gaussian distribution by basing a regression model on a heavy tailed distribution such as a t distribution we obtain a more robust model if we go back to 2 158 and substitute the alternative parameters ŒΩ 2 a Œª a b and Œ∑ œÑb a we see that the t distribution can be written in the form st x ¬µ Œª ŒΩ 0 n x ¬µ Œ∑Œª 1 gam Œ∑ ŒΩ 2 ŒΩ 2 dŒ∑ 2 160 we can then generalize this to a multivariate gaussiann x ¬µ Œª to obtain the cor responding multivariate student s t distribution in the form st x ¬µ Œª ŒΩ 0 n x ¬µ Œ∑Œª 1 gam Œ∑ ŒΩ 2 ŒΩ 2 dŒ∑',\n",
       " 'if we go back to 2 158 and substitute the alternative parameters ŒΩ 2 a Œª a b and Œ∑ œÑb a we see that the t distribution can be written in the form st x ¬µ Œª ŒΩ 0 n x ¬µ Œ∑Œª 1 gam Œ∑ ŒΩ 2 ŒΩ 2 dŒ∑ 2 160 we can then generalize this to a multivariate gaussiann x ¬µ Œª to obtain the cor responding multivariate student s t distribution in the form st x ¬µ Œª ŒΩ 0 n x ¬µ Œ∑Œª 1 gam Œ∑ ŒΩ 2 ŒΩ 2 dŒ∑ 2 161 using the same technique as for the univariate case we can evaluate this integral to giveexercise 2 48 2 3 the gaussian distribution 105 st x ¬µ Œª ŒΩ Œ≥ d 2 ŒΩ 2 Œ≥ ŒΩ 2 Œª 1 2 œÄŒΩ d 2 1 2 ŒΩ d 2 ŒΩ 2 2 162 where d is the dimensionality of x and 2 is the squared mahalanobis distance deÔ¨Åned by 2 x ¬µ tŒª x ¬µ',\n",
       " '2 161 using the same technique as for the univariate case we can evaluate this integral to giveexercise 2 48 2 3 the gaussian distribution 105 st x ¬µ Œª ŒΩ Œ≥ d 2 ŒΩ 2 Œ≥ ŒΩ 2 Œª 1 2 œÄŒΩ d 2 1 2 ŒΩ d 2 ŒΩ 2 2 162 where d is the dimensionality of x and 2 is the squared mahalanobis distance deÔ¨Åned by 2 x ¬µ tŒª x ¬µ 2 163 this is the multivariate form of student s t distribution and satisÔ¨Åes the following propertiesexercise 2 49 e x ¬µ if ŒΩ 1 2 164 cov x ŒΩ ŒΩ 2 Œª 1 if ŒΩ 2 2 165 mode x ¬µ 2 166 with corresponding results for the univariate case',\n",
       " 'the gaussian distribution 105 st x ¬µ Œª ŒΩ Œ≥ d 2 ŒΩ 2 Œ≥ ŒΩ 2 Œª 1 2 œÄŒΩ d 2 1 2 ŒΩ d 2 ŒΩ 2 2 162 where d is the dimensionality of x and 2 is the squared mahalanobis distance deÔ¨Åned by 2 x ¬µ tŒª x ¬µ 2 163 this is the multivariate form of student s t distribution and satisÔ¨Åes the following propertiesexercise 2 49 e x ¬µ if ŒΩ 1 2 164 cov x ŒΩ ŒΩ 2 Œª 1 if ŒΩ 2 2 165 mode x ¬µ 2 166 with corresponding results for the univariate case 2 3 8 periodic variables although gaussian distributions are of great practical signiÔ¨Åcance both in their own right and as building blocks for more complex probabilistic models there are situations in which they are inappropriate as density models for continuous vari ables',\n",
       " '2 163 this is the multivariate form of student s t distribution and satisÔ¨Åes the following propertiesexercise 2 49 e x ¬µ if ŒΩ 1 2 164 cov x ŒΩ ŒΩ 2 Œª 1 if ŒΩ 2 2 165 mode x ¬µ 2 166 with corresponding results for the univariate case 2 3 8 periodic variables although gaussian distributions are of great practical signiÔ¨Åcance both in their own right and as building blocks for more complex probabilistic models there are situations in which they are inappropriate as density models for continuous vari ables one important case which arises in practical applications is that of periodic variables an example of a periodic variable would be the wind direction at a particular geographical location',\n",
       " 'one important case which arises in practical applications is that of periodic variables an example of a periodic variable would be the wind direction at a particular geographical location we might for instance measure values of wind direction on a number of days and wish to summarize this using a parametric distribution another example is calendar time where we may be interested in modelling quantities that are believed to be periodic over 24 hours or over an annual cycle such quantities can conveniently be represented using an angular polar coordinate 0 Œ∏ 2œÄ we might be tempted to treat periodic variables by choosing some direction as the origin and then applying a conventional distribution such as the gaussian',\n",
       " 'such quantities can conveniently be represented using an angular polar coordinate 0 Œ∏ 2œÄ we might be tempted to treat periodic variables by choosing some direction as the origin and then applying a conventional distribution such as the gaussian such an approach however would give results that were strongly dependent on the arbitrary choice of origin suppose for instance that we have two observations at Œ∏ 1 1 and Œ∏2 359 and we model them using a standard univariate gaussian distribution if we choose the origin at 0 then the sample mean of this data set will be 180 with standard deviation 179 whereas if we choose the origin at 180 then the mean will be 0 and the standard deviation will be 1',\n",
       " 'suppose for instance that we have two observations at Œ∏ 1 1 and Œ∏2 359 and we model them using a standard univariate gaussian distribution if we choose the origin at 0 then the sample mean of this data set will be 180 with standard deviation 179 whereas if we choose the origin at 180 then the mean will be 0 and the standard deviation will be 1 we clearly need to develop a special approach for the treatment of periodic variables let us consider the problem of evaluating the mean of a set of observations d Œ∏1 Œ∏ n of a periodic variable from now on we shall assume that Œ∏ is measured in radians we have already seen that the simple average Œ∏1 Œ∏n n will be strongly coordinate dependent',\n",
       " 'from now on we shall assume that Œ∏ is measured in radians we have already seen that the simple average Œ∏1 Œ∏n n will be strongly coordinate dependent to Ô¨Ånd an invariant measure of the mean we note that the observations can be viewed as points on the unit circle and can therefore be described instead by two dimensional unit vectors x 1 xn where xn 1 for n 1 n as illustrated in figure 2 17 we can average the vectors xn 106 2 probability distributions figure 2 17 illustration of the representation of val ues Œ∏n of a periodic variable as two dimensional vectors xn living on the unit circle also shown is the average x of those vectors x1 x2 x1 x2 x3x4 x r Œ∏ instead to give x 1 n n n 1 xn 2 167 and then Ô¨Ånd the corresponding angle Œ∏ of this average',\n",
       " 'also shown is the average x of those vectors x1 x2 x1 x2 x3x4 x r Œ∏ instead to give x 1 n n n 1 xn 2 167 and then Ô¨Ånd the corresponding angle Œ∏ of this average clearly this deÔ¨Ånition will ensure that the location of the mean is independent of the origin of the angular coor dinate note that x will typically lie inside the unit circle the cartesian coordinates of the observations are given by xn c o sŒ∏n sinŒ∏n and we can write the carte sian coordinates of the sample mean in the form x r cos Œ∏ r sinŒ∏ substituting into 2 167 and equating the x1 and x2 components then gives r cos Œ∏ 1 n n n 1 cos Œ∏n r sin Œ∏ 1 n n n 1 sin Œ∏n 2 168 taking the ratio and using the identity tanŒ∏ s i nŒ∏ cos Œ∏ we can solve for Œ∏ to give Œ∏ tan 1 n sinŒ∏n n cos Œ∏n',\n",
       " 'substituting into 2 167 and equating the x1 and x2 components then gives r cos Œ∏ 1 n n n 1 cos Œ∏n r sin Œ∏ 1 n n n 1 sin Œ∏n 2 168 taking the ratio and using the identity tanŒ∏ s i nŒ∏ cos Œ∏ we can solve for Œ∏ to give Œ∏ tan 1 n sinŒ∏n n cos Œ∏n 2 169 shortly we shall see how this result arises naturally as the maximum likelihood estimator for an appropriately deÔ¨Åned distribution over a periodic variable we now consider a periodic generalization of the gaussian called thevon mises distribution here we shall limit our attention to univariate distributions although periodic distributions can also be found over hyperspheres of arbitrary dimension for an extensive discussion of periodic distributions see mardia and jupp 2000',\n",
       " 'here we shall limit our attention to univariate distributions although periodic distributions can also be found over hyperspheres of arbitrary dimension for an extensive discussion of periodic distributions see mardia and jupp 2000 by convention we will consider distributions p Œ∏ that have period 2œÄ a n y probability density p Œ∏ deÔ¨Åned over Œ∏ must not only be nonnegative and integrate 2 3 the gaussian distribution 107 figure 2 18 the von mises distribution can be derived by considering a two dimensional gaussian of the form 2 173 whose density contours are shown in blue and conditioning on the unit circle shown in red x1 x2 p x r 1 to one but it must also be periodic thus p Œ∏ must satisfy the three conditions p Œ∏ 0 2 170 2œÄ 0 p Œ∏ d Œ∏ 1 2 171 p Œ∏ 2 œÄ p Œ∏',\n",
       " 'x1 x2 p x r 1 to one but it must also be periodic thus p Œ∏ must satisfy the three conditions p Œ∏ 0 2 170 2œÄ 0 p Œ∏ d Œ∏ 1 2 171 p Œ∏ 2 œÄ p Œ∏ 2 172 from 2 172 it follows that p Œ∏ m2œÄ p Œ∏ for any integer m we can easily obtain a gaussian like distribution that satisÔ¨Åes these three prop erties as follows consider a gaussian distribution over two variables x x1 x2 having mean ¬µ ¬µ1 ¬µ2 and a covariance matrix œÉ œÉ2i where i is the 2 2 identity matrix so that p x1 x2 1 2œÄœÉ2 exp x1 ¬µ1 2 x2 ¬µ2 2 2œÉ2 2 173 the contours of constant p x are circles as illustrated in figure 2 18 now suppose we consider the value of this distribution along a circle of Ô¨Åxed radius',\n",
       " '2 173 the contours of constant p x are circles as illustrated in figure 2 18 now suppose we consider the value of this distribution along a circle of Ô¨Åxed radius then by con struction this distribution will be periodic although it will not be normalized we can determine the form of this distribution by transforming from cartesian coordinates x1 x2 to polar coordinates r Œ∏ so that x1 r cos Œ∏ x 2 r sin Œ∏ 2 174 we also map the mean ¬µ into polar coordinates by writing ¬µ1 r0 cos Œ∏0 ¬µ 2 r0 sin Œ∏0',\n",
       " 'we can determine the form of this distribution by transforming from cartesian coordinates x1 x2 to polar coordinates r Œ∏ so that x1 r cos Œ∏ x 2 r sin Œ∏ 2 174 we also map the mean ¬µ into polar coordinates by writing ¬µ1 r0 cos Œ∏0 ¬µ 2 r0 sin Œ∏0 2 175 next we substitute these transformations into the two dimensional gaussian distribu tion 2 173 and then condition on the unit circler 1 noting that we are interested only in the dependence on Œ∏ focussing on the exponent in the gaussian distribution we have 1 2œÉ2 r cos Œ∏ r0 cos Œ∏0 2 r sin Œ∏ r0 sin Œ∏0 2 1 2œÉ2 1 r2 0 2r0 cos Œ∏ cos Œ∏0 2r0 sinŒ∏ sin Œ∏0 r0 œÉ2 cos Œ∏ Œ∏0 c o n s t 2 176 108 2',\n",
       " '2 174 we also map the mean ¬µ into polar coordinates by writing ¬µ1 r0 cos Œ∏0 ¬µ 2 r0 sin Œ∏0 2 175 next we substitute these transformations into the two dimensional gaussian distribu tion 2 173 and then condition on the unit circler 1 noting that we are interested only in the dependence on Œ∏ focussing on the exponent in the gaussian distribution we have 1 2œÉ2 r cos Œ∏ r0 cos Œ∏0 2 r sin Œ∏ r0 sin Œ∏0 2 1 2œÉ2 1 r2 0 2r0 cos Œ∏ cos Œ∏0 2r0 sinŒ∏ sin Œ∏0 r0 œÉ2 cos Œ∏ Œ∏0 c o n s t 2 176 108 2 probability distributions m 5 Œ∏0 œÄ 4 m 1 Œ∏0 3 œÄ 4 2œÄ 0 œÄ 4 3œÄ 4 m 5 Œ∏0 œÄ 4 m 1 Œ∏0 3 œÄ 4 figure 2 19 the von mises distribution plotted for two different parameter values shown as a cartesian plot on the left and as the corresponding polar plot on the right',\n",
       " '2 175 next we substitute these transformations into the two dimensional gaussian distribu tion 2 173 and then condition on the unit circler 1 noting that we are interested only in the dependence on Œ∏ focussing on the exponent in the gaussian distribution we have 1 2œÉ2 r cos Œ∏ r0 cos Œ∏0 2 r sin Œ∏ r0 sin Œ∏0 2 1 2œÉ2 1 r2 0 2r0 cos Œ∏ cos Œ∏0 2r0 sinŒ∏ sin Œ∏0 r0 œÉ2 cos Œ∏ Œ∏0 c o n s t 2 176 108 2 probability distributions m 5 Œ∏0 œÄ 4 m 1 Œ∏0 3 œÄ 4 2œÄ 0 œÄ 4 3œÄ 4 m 5 Œ∏0 œÄ 4 m 1 Œ∏0 3 œÄ 4 figure 2 19 the von mises distribution plotted for two different parameter values shown as a cartesian plot on the left and as the corresponding polar plot on the right where const denotes terms independent ofŒ∏ and we have made use of the following trigonometrical identitiesexercise 2 51 cos2 a s i n2 a 1 2 177 cos a cos b s i na sin b c o s a b',\n",
       " 'probability distributions m 5 Œ∏0 œÄ 4 m 1 Œ∏0 3 œÄ 4 2œÄ 0 œÄ 4 3œÄ 4 m 5 Œ∏0 œÄ 4 m 1 Œ∏0 3 œÄ 4 figure 2 19 the von mises distribution plotted for two different parameter values shown as a cartesian plot on the left and as the corresponding polar plot on the right where const denotes terms independent ofŒ∏ and we have made use of the following trigonometrical identitiesexercise 2 51 cos2 a s i n2 a 1 2 177 cos a cos b s i na sin b c o s a b 2 178 if we now deÔ¨Åne m r0 œÉ2 we obtain our Ô¨Ånal expression for the distribution of p Œ∏ along the unit circle r 1 in the form p Œ∏ Œ∏0 m 1 2œÄi0 m exp m cos Œ∏ Œ∏0 2 179 which is called the von mises distribution or the circular normal',\n",
       " 'where const denotes terms independent ofŒ∏ and we have made use of the following trigonometrical identitiesexercise 2 51 cos2 a s i n2 a 1 2 177 cos a cos b s i na sin b c o s a b 2 178 if we now deÔ¨Åne m r0 œÉ2 we obtain our Ô¨Ånal expression for the distribution of p Œ∏ along the unit circle r 1 in the form p Œ∏ Œ∏0 m 1 2œÄi0 m exp m cos Œ∏ Œ∏0 2 179 which is called the von mises distribution or the circular normal here the param eter Œ∏0 corresponds to the mean of the distribution while m which is known as the concentration parameter is analogous to the inverse variance precision for the gaussian',\n",
       " '2 178 if we now deÔ¨Åne m r0 œÉ2 we obtain our Ô¨Ånal expression for the distribution of p Œ∏ along the unit circle r 1 in the form p Œ∏ Œ∏0 m 1 2œÄi0 m exp m cos Œ∏ Œ∏0 2 179 which is called the von mises distribution or the circular normal here the param eter Œ∏0 corresponds to the mean of the distribution while m which is known as the concentration parameter is analogous to the inverse variance precision for the gaussian the normalization coefÔ¨Åcient in 2 179 is expressed in terms of i0 m which is the zeroth order bessel function of the Ô¨Årst kind abramowitz and stegun 1965 and is deÔ¨Åned by i0 m 1 2œÄ 2œÄ 0 exp m cos Œ∏ dŒ∏ 2 180 for large m the distribution becomes approximately gaussian',\n",
       " 'the normalization coefÔ¨Åcient in 2 179 is expressed in terms of i0 m which is the zeroth order bessel function of the Ô¨Årst kind abramowitz and stegun 1965 and is deÔ¨Åned by i0 m 1 2œÄ 2œÄ 0 exp m cos Œ∏ dŒ∏ 2 180 for large m the distribution becomes approximately gaussian the von mises dis exercise 2 52 tribution is plotted in figure 2 19 and the function i0 m is plotted in figure 2 20 now consider the maximum likelihood estimators for the parameters Œ∏0 and m for the von mises distribution the log likelihood function is given by lnp d Œ∏0 m n ln 2œÄ n ln i0 m m n n 1 cos Œ∏n Œ∏0 2 181 2 3',\n",
       " 'the log likelihood function is given by lnp d Œ∏0 m n ln 2œÄ n ln i0 m m n n 1 cos Œ∏n Œ∏0 2 181 2 3 the gaussian distribution 109 i0 m m 0 5 10 0 1000 2000 3000 a m m 0 5 10 0 0 5 1 figure 2 20 plot of the bessel function i0 m deÔ¨Åned by 2 180 together with the function a m deÔ¨Åned by 2 186 setting the derivative with respect to Œ∏0 equal to zero gives n n 1 sin Œ∏n Œ∏0 0 2 182 to solve for Œ∏0 we make use of the trigonometric identity sin a b c o sb sina cos a sinb 2 183 from which we obtainexercise 2 53 Œ∏ml 0 tan 1 n sin Œ∏n n cos Œ∏n 2 184 which we recognize as the result 2 169 obtained earlier for the mean of the obser vations viewed in a two dimensional cartesian space',\n",
       " 'setting the derivative with respect to Œ∏0 equal to zero gives n n 1 sin Œ∏n Œ∏0 0 2 182 to solve for Œ∏0 we make use of the trigonometric identity sin a b c o sb sina cos a sinb 2 183 from which we obtainexercise 2 53 Œ∏ml 0 tan 1 n sin Œ∏n n cos Œ∏n 2 184 which we recognize as the result 2 169 obtained earlier for the mean of the obser vations viewed in a two dimensional cartesian space similarly maximizing 2 181 with respect to m and making use of i 0 m i1 m abramowitz and stegun 1965 we have a m 1 n n n 1 cos Œ∏n Œ∏ml 0 2 185 where we have substituted for the maximum likelihood solution for Œ∏ml 0 recalling that we are performing a joint optimization over Œ∏ and m and we have deÔ¨Åned a m i1 m i0 m',\n",
       " '2 182 to solve for Œ∏0 we make use of the trigonometric identity sin a b c o sb sina cos a sinb 2 183 from which we obtainexercise 2 53 Œ∏ml 0 tan 1 n sin Œ∏n n cos Œ∏n 2 184 which we recognize as the result 2 169 obtained earlier for the mean of the obser vations viewed in a two dimensional cartesian space similarly maximizing 2 181 with respect to m and making use of i 0 m i1 m abramowitz and stegun 1965 we have a m 1 n n n 1 cos Œ∏n Œ∏ml 0 2 185 where we have substituted for the maximum likelihood solution for Œ∏ml 0 recalling that we are performing a joint optimization over Œ∏ and m and we have deÔ¨Åned a m i1 m i0 m 2 186 the function a m is plotted in figure 2 20',\n",
       " 'similarly maximizing 2 181 with respect to m and making use of i 0 m i1 m abramowitz and stegun 1965 we have a m 1 n n n 1 cos Œ∏n Œ∏ml 0 2 185 where we have substituted for the maximum likelihood solution for Œ∏ml 0 recalling that we are performing a joint optimization over Œ∏ and m and we have deÔ¨Åned a m i1 m i0 m 2 186 the function a m is plotted in figure 2 20 making use of the trigonometric iden tity 2 178 we can write 2 185 in the form a mml 1 n n n 1 cos Œ∏n cos Œ∏ml 0 1 n n n 1 sin Œ∏n sinŒ∏ml 0 2 187 110 2 probability distributions figure 2 21 plots of the old faith ful data in which the blue curves show contours of constant proba bility density',\n",
       " '2 187 110 2 probability distributions figure 2 21 plots of the old faith ful data in which the blue curves show contours of constant proba bility density on the left is a single gaussian distribution which has been Ô¨Åtted to the data us ing maximum likelihood note that this distribution fails to capture the two clumps in the data and indeed places much of its probability mass in the central region between the clumps where the data are relatively sparse on the right the distribution is given by a linear combination of two gaussians which has been Ô¨Åtted to the data by maximum likelihood using techniques discussed chap ter 9 and which gives a better rep resentation of the data',\n",
       " 'note that this distribution fails to capture the two clumps in the data and indeed places much of its probability mass in the central region between the clumps where the data are relatively sparse on the right the distribution is given by a linear combination of two gaussians which has been Ô¨Åtted to the data by maximum likelihood using techniques discussed chap ter 9 and which gives a better rep resentation of the data 1 2 3 4 5 6 40 60 80 100 1 2 3 4 5 6 40 60 80 100 the right hand side of 2 187 is easily evaluated and the function a m can be inverted numerically for completeness we mention brieÔ¨Çy some alternative techniques for the con struction of periodic distributions',\n",
       " '1 2 3 4 5 6 40 60 80 100 1 2 3 4 5 6 40 60 80 100 the right hand side of 2 187 is easily evaluated and the function a m can be inverted numerically for completeness we mention brieÔ¨Çy some alternative techniques for the con struction of periodic distributions the simplest approach is to use a histogram of observations in which the angular coordinate is divided into Ô¨Åxed bins this has the virtue of simplicity and Ô¨Çexibility but also suffers from signiÔ¨Åcant limitations as we shall see when we discuss histogram methods in more detail in section 2 5 another approach starts like the von mises distribution from a gaussian distribution over a euclidean space but now marginalizes onto the unit circle rather than conditioning mardia and jupp 2000',\n",
       " 'this has the virtue of simplicity and Ô¨Çexibility but also suffers from signiÔ¨Åcant limitations as we shall see when we discuss histogram methods in more detail in section 2 5 another approach starts like the von mises distribution from a gaussian distribution over a euclidean space but now marginalizes onto the unit circle rather than conditioning mardia and jupp 2000 however this leads to more complex forms of distribution and will not be discussed further finally any valid distribution over the real axis such as a gaussian can be turned into a periodic distribution by mapping succes sive intervals of width 2œÄ onto the periodic variable 0 2œÄ which corresponds to wrapping the real axis around unit circle',\n",
       " 'however this leads to more complex forms of distribution and will not be discussed further finally any valid distribution over the real axis such as a gaussian can be turned into a periodic distribution by mapping succes sive intervals of width 2œÄ onto the periodic variable 0 2œÄ which corresponds to wrapping the real axis around unit circle again the resulting distribution is more complex to handle than the von mises distribution one limitation of the von mises distribution is that it is unimodal by forming mixtures of von mises distributions we obtain a Ô¨Çexible framework for modelling periodic variables that can handle multimodality for an example of a machine learn ing application that makes use of von mises distributions see lawrenceet al',\n",
       " 'by forming mixtures of von mises distributions we obtain a Ô¨Çexible framework for modelling periodic variables that can handle multimodality for an example of a machine learn ing application that makes use of von mises distributions see lawrenceet al 2002 and for extensions to modelling conditional densities for regression problems see bishop and nabney 1996 2 3 9 mixtures of gaussians while the gaussian distribution has some important analytical properties it suf fers from signiÔ¨Åcant limitations when it comes to modelling real data sets consider the example shown in figure 2 21 this is known as the old faithful data set and comprises 272 measurements of the eruption of the old faithful geyser at yel lowstone national park in the usa',\n",
       " 'consider the example shown in figure 2 21 this is known as the old faithful data set and comprises 272 measurements of the eruption of the old faithful geyser at yel lowstone national park in the usa each measurement comprises the duration ofappendix a 2 3 the gaussian distribution 111 figure 2 22 example of a gaussian mixture distribution in one dimension showing three gaussians each scaled by a coefÔ¨Åcient in blue and their sum in red x p x the eruption in minutes horizontal axis and the time in minutes to the next erup tion vertical axis we see that the data set forms two dominant clumps and that a simple gaussian distribution is unable to capture this structure whereas a linear superposition of two gaussians gives a better characterization of the data set',\n",
       " 'x p x the eruption in minutes horizontal axis and the time in minutes to the next erup tion vertical axis we see that the data set forms two dominant clumps and that a simple gaussian distribution is unable to capture this structure whereas a linear superposition of two gaussians gives a better characterization of the data set such superpositions formed by taking linear combinations of more basic dis tributions such as gaussians can be formulated as probabilistic models known as mixture distributions mclachlan and basford 1988 mclachlan and peel 2000 in figure 2 22 we see that a linear combination of gaussians can give rise to very complex densities',\n",
       " 'such superpositions formed by taking linear combinations of more basic dis tributions such as gaussians can be formulated as probabilistic models known as mixture distributions mclachlan and basford 1988 mclachlan and peel 2000 in figure 2 22 we see that a linear combination of gaussians can give rise to very complex densities by using a sufÔ¨Åcient number of gaussians and by adjusting their means and covariances as well as the coefÔ¨Åcients in the linear combination almost any continuous density can be approximated to arbitrary accuracy we therefore consider a superposition of k gaussian densities of the form p x k k 1 œÄkn x ¬µk œÉk 2 188 which is called a mixture of gaussians',\n",
       " 'by using a sufÔ¨Åcient number of gaussians and by adjusting their means and covariances as well as the coefÔ¨Åcients in the linear combination almost any continuous density can be approximated to arbitrary accuracy we therefore consider a superposition of k gaussian densities of the form p x k k 1 œÄkn x ¬µk œÉk 2 188 which is called a mixture of gaussians each gaussian density n x ¬µk œÉk is called a component of the mixture and has its own mean ¬µk and covariance œÉk contour and surface plots for a gaussian mixture having 3 components are shown in figure 2 23 in this section we shall consider gaussian components to illustrate the frame work of mixture models more generally mixture models can comprise linear com binations of other distributions',\n",
       " 'in this section we shall consider gaussian components to illustrate the frame work of mixture models more generally mixture models can comprise linear com binations of other distributions for instance in section 9 3 3 we shall consider mixtures of bernoulli distributions as an example of a mixture model for discrete variables section 9 3 3 the parameters œÄk in 2 188 are called mixing coefÔ¨Åcients if we integrate both sides of 2 188 with respect tox and note that bothp x and the individual gaussian components are normalized we obtain k k 1 œÄk 1 2 189 also the requirement that p x 0 together with n x ¬µk œÉk 0 implies œÄk 0 for all k combining this with the condition 2 189 we obtain 0 œÄk 1 2 190 112 2',\n",
       " '2 189 also the requirement that p x 0 together with n x ¬µk œÉk 0 implies œÄk 0 for all k combining this with the condition 2 189 we obtain 0 œÄk 1 2 190 112 2 probability distributions 0 5 0 3 0 2 a 0 0 5 1 0 0 5 1 b 0 0 5 1 0 0 5 1 figure 2 23 illustration of a mixture of 3 gaussians in a two dimensional space a contours of constant density for each of the mixture components in which the 3 components are denoted red blue and green and the values of the mixing coefÔ¨Åcients are shown below each component b contours of the marginal probability density p x of the mixture distribution c a surface plot of the distribution p x we therefore see that the mixing coefÔ¨Åcients satisfy the requirements to be probabil ities',\n",
       " 'c a surface plot of the distribution p x we therefore see that the mixing coefÔ¨Åcients satisfy the requirements to be probabil ities from the sum and product rules the marginal density is given by p x k k 1 p k p x k 2 191 which is equivalent to 2 188 in which we can view œÄk p k as the prior prob ability of picking the kth component and the density n x ¬µk œÉk p x k as the probability of x conditioned on k as we shall see in later chapters an impor tant role is played by the posterior probabilities p k x which are also known as responsibilities from bayes theorem these are given by Œ≥k x p k x p k p x k l p l p x l œÄkn x ¬µk œÉk l œÄln x ¬µl œÉl',\n",
       " 'from the sum and product rules the marginal density is given by p x k k 1 p k p x k 2 191 which is equivalent to 2 188 in which we can view œÄk p k as the prior prob ability of picking the kth component and the density n x ¬µk œÉk p x k as the probability of x conditioned on k as we shall see in later chapters an impor tant role is played by the posterior probabilities p k x which are also known as responsibilities from bayes theorem these are given by Œ≥k x p k x p k p x k l p l p x l œÄkn x ¬µk œÉk l œÄln x ¬µl œÉl 2 192 we shall discuss the probabilistic interpretation of the mixture distribution in greater detail in chapter 9',\n",
       " 'from bayes theorem these are given by Œ≥k x p k x p k p x k l p l p x l œÄkn x ¬µk œÉk l œÄln x ¬µl œÉl 2 192 we shall discuss the probabilistic interpretation of the mixture distribution in greater detail in chapter 9 the form of the gaussian mixture distribution is governed by the parameters œÄ ¬µ and œÉ where we have used the notation œÄ œÄ1 œÄ k ¬µ ¬µ1 ¬µk and œÉ œÉ1 œÉk one way to set the values of these parameters is to use maximum likelihood from 2 188 the log of the likelihood function is given by lnp x œÄ ¬µ œÉ n n 1 ln k k 1 œÄkn xn ¬µk œÉk 2 193 2 4 the exponential family 113 where x x1 xn',\n",
       " 'from 2 188 the log of the likelihood function is given by lnp x œÄ ¬µ œÉ n n 1 ln k k 1 œÄkn xn ¬µk œÉk 2 193 2 4 the exponential family 113 where x x1 xn we immediately see that the situation is now much more complex than with a single gaussian due to the presence of the summation over k inside the logarithm as a result the maximum likelihood solution for the parameters no longer has a closed form analytical solution one approach to maxi mizing the likelihood function is to use iterative numerical optimization techniques fletcher 1987 nocedal and wright 1999 bishop and nabney 2008 alterna tively we can employ a powerful framework calledexpectation maximization which will be discussed at length in chapter 9 2 4',\n",
       " 'alterna tively we can employ a powerful framework calledexpectation maximization which will be discussed at length in chapter 9 2 4 the exponential family the probability distributions that we have studied so far in this chapter with the exception of the gaussian mixture are speciÔ¨Åc examples of a broad class of distri butions called the exponential family duda and hart 1973 bernardo and smith 1994 members of the exponential family have many important properties in com mon and it is illuminating to discuss these properties in some generality the exponential family of distributions overx given parameters Œ∑ is deÔ¨Åned to be the set of distributions of the form p x Œ∑ h x g Œ∑ e x p Œ∑tu x 2 194 where x may be scalar or vector and may be discrete or continuous',\n",
       " 'members of the exponential family have many important properties in com mon and it is illuminating to discuss these properties in some generality the exponential family of distributions overx given parameters Œ∑ is deÔ¨Åned to be the set of distributions of the form p x Œ∑ h x g Œ∑ e x p Œ∑tu x 2 194 where x may be scalar or vector and may be discrete or continuous here Œ∑ are called the natural parameters of the distribution and u x is some function of x the function g Œ∑ can be interpreted as the coefÔ¨Åcient that ensures that the distribu tion is normalized and therefore satisÔ¨Åes g Œ∑ h x e x p Œ∑tu x dx 1 2 195 where the integration is replaced by summation if x is a discrete variable',\n",
       " 'here Œ∑ are called the natural parameters of the distribution and u x is some function of x the function g Œ∑ can be interpreted as the coefÔ¨Åcient that ensures that the distribu tion is normalized and therefore satisÔ¨Åes g Œ∑ h x e x p Œ∑tu x dx 1 2 195 where the integration is replaced by summation if x is a discrete variable we begin by taking some examples of the distributions introduced earlier in the chapter and showing that they are indeed members of the exponential family consider Ô¨Årst the bernoulli distribution p x ¬µ b e r n x ¬µ ¬µx 1 ¬µ 1 x 2 196 expressing the right hand side as the exponential of the logarithm we have p x ¬µ exp xln¬µ 1 x l n 1 ¬µ 1 ¬µ e x p ln ¬µ 1 ¬µ x',\n",
       " 'consider Ô¨Årst the bernoulli distribution p x ¬µ b e r n x ¬µ ¬µx 1 ¬µ 1 x 2 196 expressing the right hand side as the exponential of the logarithm we have p x ¬µ exp xln¬µ 1 x l n 1 ¬µ 1 ¬µ e x p ln ¬µ 1 ¬µ x 2 197 comparison with 2 194 allows us to identify Œ∑ l n ¬µ 1 ¬µ 2 198 114 2 probability distributions which we can solve for ¬µ to give ¬µ œÉ Œ∑ where œÉ Œ∑ 1 1 e x p Œ∑ 2 199 is called the logistic sigmoid function thus we can write the bernoulli distribution using the standard representation 2 194 in the form p x Œ∑ œÉ Œ∑ exp Œ∑x 2 200 where we have used 1 œÉ Œ∑ œÉ Œ∑ which is easily proved from 2 199 com parison with 2 194 shows that u x x 2 201 h x 1 2 202 g Œ∑ œÉ Œ∑',\n",
       " 'thus we can write the bernoulli distribution using the standard representation 2 194 in the form p x Œ∑ œÉ Œ∑ exp Œ∑x 2 200 where we have used 1 œÉ Œ∑ œÉ Œ∑ which is easily proved from 2 199 com parison with 2 194 shows that u x x 2 201 h x 1 2 202 g Œ∑ œÉ Œ∑ 2 203 next consider the multinomial distribution that for a single observationx takes the form p x ¬µ m k 1 ¬µxk k e x p m k 1 xk ln ¬µk 2 204 where x x1 x n t again we can write this in the standard representation 2 194 so that p x Œ∑ e x p Œ∑tx 2 205 where Œ∑k l n¬µk and we have deÔ¨Åned Œ∑ Œ∑1 Œ∑ m t again comparing with 2 194 we have u x x 2 206 h x 1 2 207 g Œ∑ 1',\n",
       " 'com parison with 2 194 shows that u x x 2 201 h x 1 2 202 g Œ∑ œÉ Œ∑ 2 203 next consider the multinomial distribution that for a single observationx takes the form p x ¬µ m k 1 ¬µxk k e x p m k 1 xk ln ¬µk 2 204 where x x1 x n t again we can write this in the standard representation 2 194 so that p x Œ∑ e x p Œ∑tx 2 205 where Œ∑k l n¬µk and we have deÔ¨Åned Œ∑ Œ∑1 Œ∑ m t again comparing with 2 194 we have u x x 2 206 h x 1 2 207 g Œ∑ 1 2 208 note that the parameters Œ∑k are not independent because the parameters ¬µk are sub ject to the constraint m k 1 ¬µk 1 2 209 so that given any m 1 of the parameters ¬µk the value of the remaining parameter is Ô¨Åxed',\n",
       " '2 203 next consider the multinomial distribution that for a single observationx takes the form p x ¬µ m k 1 ¬µxk k e x p m k 1 xk ln ¬µk 2 204 where x x1 x n t again we can write this in the standard representation 2 194 so that p x Œ∑ e x p Œ∑tx 2 205 where Œ∑k l n¬µk and we have deÔ¨Åned Œ∑ Œ∑1 Œ∑ m t again comparing with 2 194 we have u x x 2 206 h x 1 2 207 g Œ∑ 1 2 208 note that the parameters Œ∑k are not independent because the parameters ¬µk are sub ject to the constraint m k 1 ¬µk 1 2 209 so that given any m 1 of the parameters ¬µk the value of the remaining parameter is Ô¨Åxed in some circumstances it will be convenient to remove this constraint by expressing the distribution in terms of onlym 1 parameters',\n",
       " '2 208 note that the parameters Œ∑k are not independent because the parameters ¬µk are sub ject to the constraint m k 1 ¬µk 1 2 209 so that given any m 1 of the parameters ¬µk the value of the remaining parameter is Ô¨Åxed in some circumstances it will be convenient to remove this constraint by expressing the distribution in terms of onlym 1 parameters this can be achieved by using the relationship 2 209 to eliminate ¬µm by expressing it in terms of the remaining ¬µk where k 1 m 1 thereby leaving m 1 parameters note that these remaining parameters are still subject to the constraints 0 ¬µk 1 m 1 k 1 ¬µk 1 2 210 2 4',\n",
       " 'note that these remaining parameters are still subject to the constraints 0 ¬µk 1 m 1 k 1 ¬µk 1 2 210 2 4 the exponential family 115 making use of the constraint 2 209 the multinomial distribution in this representa tion then becomes exp m k 1 xk ln ¬µk e x p m 1 k 1 xk ln¬µk 1 m 1 k 1 xk ln 1 m 1 k 1 ¬µk e x p m 1 k 1 xk ln ¬µk 1 m 1 j 1 ¬µj l n 1 m 1 k 1 ¬µk 2 211 we now identify ln ¬µk 1 j ¬µj Œ∑k 2 212 which we can solve for ¬µk by Ô¨Årst summing both sides over k and then rearranging and back substituting to give ¬µk exp Œ∑k 1 j exp Œ∑j 2 213 this is called the softmax function or the normalized exponential',\n",
       " '2 211 we now identify ln ¬µk 1 j ¬µj Œ∑k 2 212 which we can solve for ¬µk by Ô¨Årst summing both sides over k and then rearranging and back substituting to give ¬µk exp Œ∑k 1 j exp Œ∑j 2 213 this is called the softmax function or the normalized exponential in this represen tation the multinomial distribution therefore takes the form p x Œ∑ 1 m 1 k 1 exp Œ∑k 1 exp Œ∑tx 2 214 this is the standard form of the exponential family with parameter vector Œ∑ Œ∑1 Œ∑ m 1 t in which u x x 2 215 h x 1 2 216 g Œ∑ 1 m 1 k 1 exp Œ∑k 1 2 217 finally let us consider the gaussian distribution for the univariate gaussian we have p x ¬µ œÉ2 1 2œÄœÉ2 1 2 exp 1 2œÉ2 x ¬µ 2 2 218 1 2œÄœÉ2 1 2 exp 1 2œÉ2 x2 ¬µ œÉ2 x 1 2œÉ2 ¬µ2 2 219 116 2',\n",
       " '2 217 finally let us consider the gaussian distribution for the univariate gaussian we have p x ¬µ œÉ2 1 2œÄœÉ2 1 2 exp 1 2œÉ2 x ¬µ 2 2 218 1 2œÄœÉ2 1 2 exp 1 2œÉ2 x2 ¬µ œÉ2 x 1 2œÉ2 ¬µ2 2 219 116 2 probability distributions which after some simple rearrangement can be cast in the standard exponential family form 2 194 withexercise 2 57 Œ∑ ¬µ œÉ2 1 2œÉ2 2 220 u x x x2 2 221 h x 2 œÄ 1 2 2 222 g Œ∑ 2Œ∑2 1 2 exp Œ∑2 1 4Œ∑2 2 223 2 4 1 maximum likelihood and sufÔ¨Åcient statistics let us now consider the problem of estimating the parameter vectorŒ∑ in the gen eral exponential family distribution 2 194 using the technique of maximum likeli hood',\n",
       " 'probability distributions which after some simple rearrangement can be cast in the standard exponential family form 2 194 withexercise 2 57 Œ∑ ¬µ œÉ2 1 2œÉ2 2 220 u x x x2 2 221 h x 2 œÄ 1 2 2 222 g Œ∑ 2Œ∑2 1 2 exp Œ∑2 1 4Œ∑2 2 223 2 4 1 maximum likelihood and sufÔ¨Åcient statistics let us now consider the problem of estimating the parameter vectorŒ∑ in the gen eral exponential family distribution 2 194 using the technique of maximum likeli hood taking the gradient of both sides of 2 195 with respect to Œ∑ w eh a v e g Œ∑ h x e x p Œ∑tu x dx g Œ∑ h x e x p Œ∑tu x u x dx 0 2 224 rearranging and making use again of 2 195 then gives 1 g Œ∑ g Œ∑ g Œ∑ h x e x p Œ∑tu x u x dx e u x 2 225 where we have used 2 194',\n",
       " 'taking the gradient of both sides of 2 195 with respect to Œ∑ w eh a v e g Œ∑ h x e x p Œ∑tu x dx g Œ∑ h x e x p Œ∑tu x u x dx 0 2 224 rearranging and making use again of 2 195 then gives 1 g Œ∑ g Œ∑ g Œ∑ h x e x p Œ∑tu x u x dx e u x 2 225 where we have used 2 194 we therefore obtain the result lng Œ∑ e u x 2 226 note that the covariance of u x can be expressed in terms of the second derivatives of g Œ∑ and similarly for higher order moments thus provided we can normalize aexercise 2 58 distribution from the exponential family we can always Ô¨Ånd its moments by simple differentiation',\n",
       " '2 226 note that the covariance of u x can be expressed in terms of the second derivatives of g Œ∑ and similarly for higher order moments thus provided we can normalize aexercise 2 58 distribution from the exponential family we can always Ô¨Ånd its moments by simple differentiation now consider a set of independent identically distributed data denoted by x x1 xn for which the likelihood function is given by p x Œ∑ n n 1 h xn g Œ∑ n exp Œ∑t n n 1 u xn 2 227 setting the gradient of ln p x Œ∑ with respect to Œ∑ to zero we get the following condition to be satisÔ¨Åed by the maximum likelihood estimator Œ∑ml lng Œ∑ml 1 n n n 1 u xn 2 228 2 4 the exponential family 117 which can in principle be solved to obtain Œ∑ml',\n",
       " '2 227 setting the gradient of ln p x Œ∑ with respect to Œ∑ to zero we get the following condition to be satisÔ¨Åed by the maximum likelihood estimator Œ∑ml lng Œ∑ml 1 n n n 1 u xn 2 228 2 4 the exponential family 117 which can in principle be solved to obtain Œ∑ml we see that the solution for the maximum likelihood estimator depends on the data only through n u xn which is therefore called the sufÔ¨Åcient statistic of the distribution 2 194 we do not need to store the entire data set itself but only the value of the sufÔ¨Åcient statistic for the bernoulli distribution for example the function u x is given just by x and so we need only keep the sum of the data points xn whereas for the gaussian u x x x2 t and so we should keep both the sum of xn and the sum of x2 n',\n",
       " 'we do not need to store the entire data set itself but only the value of the sufÔ¨Åcient statistic for the bernoulli distribution for example the function u x is given just by x and so we need only keep the sum of the data points xn whereas for the gaussian u x x x2 t and so we should keep both the sum of xn and the sum of x2 n if we consider the limit n then the right hand side of 2 228 becomes e u x and so by comparing with 2 226 we see that in this limit Œ∑ml will equal the true value Œ∑ in fact this sufÔ¨Åciency property holds also for bayesian inference although we shall defer discussion of this until chapter 8 when we have equipped ourselves with the tools of graphical models and can thereby gain a deeper insight into these important concepts',\n",
       " 'if we consider the limit n then the right hand side of 2 228 becomes e u x and so by comparing with 2 226 we see that in this limit Œ∑ml will equal the true value Œ∑ in fact this sufÔ¨Åciency property holds also for bayesian inference although we shall defer discussion of this until chapter 8 when we have equipped ourselves with the tools of graphical models and can thereby gain a deeper insight into these important concepts 2 4 2 conjugate priors we have already encountered the concept of a conjugate prior several times for example in the context of the bernoulli distribution for which the conjugate prior is the beta distribution or the gaussian where the conjugate prior for the mean is a gaussian and the conjugate prior for the precision is the wishart distribution',\n",
       " 'in fact this sufÔ¨Åciency property holds also for bayesian inference although we shall defer discussion of this until chapter 8 when we have equipped ourselves with the tools of graphical models and can thereby gain a deeper insight into these important concepts 2 4 2 conjugate priors we have already encountered the concept of a conjugate prior several times for example in the context of the bernoulli distribution for which the conjugate prior is the beta distribution or the gaussian where the conjugate prior for the mean is a gaussian and the conjugate prior for the precision is the wishart distribution in general for a given probability distribution p x Œ∑ we can seek a prior p Œ∑ that is conjugate to the likelihood function so that the posterior distribution has the same functional form as the prior',\n",
       " '2 4 2 conjugate priors we have already encountered the concept of a conjugate prior several times for example in the context of the bernoulli distribution for which the conjugate prior is the beta distribution or the gaussian where the conjugate prior for the mean is a gaussian and the conjugate prior for the precision is the wishart distribution in general for a given probability distribution p x Œ∑ we can seek a prior p Œ∑ that is conjugate to the likelihood function so that the posterior distribution has the same functional form as the prior for any member of the exponential family 2 194 there exists a conjugate prior that can be written in the form p Œ∑ œá ŒΩ f œá ŒΩ g Œ∑ ŒΩ exp ŒΩŒ∑tœá 2 229 where f œá ŒΩ is a normalization coefÔ¨Åcient and g Œ∑ is the same function as ap pears in 2 194',\n",
       " 'in general for a given probability distribution p x Œ∑ we can seek a prior p Œ∑ that is conjugate to the likelihood function so that the posterior distribution has the same functional form as the prior for any member of the exponential family 2 194 there exists a conjugate prior that can be written in the form p Œ∑ œá ŒΩ f œá ŒΩ g Œ∑ ŒΩ exp ŒΩŒ∑tœá 2 229 where f œá ŒΩ is a normalization coefÔ¨Åcient and g Œ∑ is the same function as ap pears in 2 194 to see that this is indeed conjugate let us multiply the prior 2 229 by the likelihood function 2 227 to obtain the posterior distribution up to a nor malization coefÔ¨Åcient in the form p Œ∑ x œá ŒΩ g Œ∑ ŒΩ n exp Œ∑t n n 1 u xn ŒΩœá',\n",
       " 'for any member of the exponential family 2 194 there exists a conjugate prior that can be written in the form p Œ∑ œá ŒΩ f œá ŒΩ g Œ∑ ŒΩ exp ŒΩŒ∑tœá 2 229 where f œá ŒΩ is a normalization coefÔ¨Åcient and g Œ∑ is the same function as ap pears in 2 194 to see that this is indeed conjugate let us multiply the prior 2 229 by the likelihood function 2 227 to obtain the posterior distribution up to a nor malization coefÔ¨Åcient in the form p Œ∑ x œá ŒΩ g Œ∑ ŒΩ n exp Œ∑t n n 1 u xn ŒΩœá 2 230 this again takes the same functional form as the prior 2 229 conÔ¨Årming conjugacy furthermore we see that the parameter ŒΩ can be interpreted as a effective number of pseudo observations in the prior each of which has a value for the sufÔ¨Åcient statistic u x given by œá',\n",
       " '2 230 this again takes the same functional form as the prior 2 229 conÔ¨Årming conjugacy furthermore we see that the parameter ŒΩ can be interpreted as a effective number of pseudo observations in the prior each of which has a value for the sufÔ¨Åcient statistic u x given by œá 2 4 3 noninformative priors in some applications of probabilistic inference we may have prior knowledge that can be conveniently expressed through the prior distribution for example if the prior assigns zero probability to some value of variable then the posterior dis tribution will necessarily also assign zero probability to that value irrespective of 118 2 probability distributions any subsequent observations of data in many cases however we may have little idea of what form the distribution should take',\n",
       " 'probability distributions any subsequent observations of data in many cases however we may have little idea of what form the distribution should take we may then seek a form of prior distribution called a noninformative prior which is intended to have as little inÔ¨Çu ence on the posterior distribution as possible jeffries 1946 box and tao 1973 bernardo and smith 1994 this is sometimes referred to as letting the data speak for themselves if we have a distributionp x Œª governed by a parameterŒª we might be tempted to propose a prior distribution p Œª const as a suitable prior if Œª is a discrete variable with k states this simply amounts to setting the prior probability of each state to 1 k',\n",
       " 'if we have a distributionp x Œª governed by a parameterŒª we might be tempted to propose a prior distribution p Œª const as a suitable prior if Œª is a discrete variable with k states this simply amounts to setting the prior probability of each state to 1 k in the case of continuous parameters however there are two potential difÔ¨Åculties with this approach the Ô¨Årst is that if the domain of Œª is unbounded this prior distribution cannot be correctly normalized because the integral over Œª diverges such priors are called improper in practice improper priors can often be used provided the corresponding posterior distribution is proper i e that it can be correctly normalized',\n",
       " 'such priors are called improper in practice improper priors can often be used provided the corresponding posterior distribution is proper i e that it can be correctly normalized for instance if we put a uniform prior distribution over the mean of a gaussian then the posterior distribution for the mean once we have observed at least one data point will be proper a second difÔ¨Åculty arises from the transformation behaviour of a probability density under a nonlinear change of variables given by 1 27 if a function h Œª is constant and we change variables to Œª Œ∑ 2 then ÀÜh Œ∑ h Œ∑2 will also be constant',\n",
       " 'a second difÔ¨Åculty arises from the transformation behaviour of a probability density under a nonlinear change of variables given by 1 27 if a function h Œª is constant and we change variables to Œª Œ∑ 2 then ÀÜh Œ∑ h Œ∑2 will also be constant however if we choose the density pŒª Œª to be constant then the density of Œ∑ will be given from 1 27 by pŒ∑ Œ∑ pŒª Œª dŒª dŒ∑ pŒª Œ∑2 2Œ∑ Œ∑ 2 231 and so the density over Œ∑ will not be constant this issue does not arise when we use maximum likelihood because the likelihood function p x Œª is a simple function of Œª and so we are free to use any convenient parameterization if however we are to choose a prior distribution that is constant we must take care to use an appropriate representation for the parameters',\n",
       " 'this issue does not arise when we use maximum likelihood because the likelihood function p x Œª is a simple function of Œª and so we are free to use any convenient parameterization if however we are to choose a prior distribution that is constant we must take care to use an appropriate representation for the parameters here we consider two simple examples of noninformative priors berger 1985 first of all if a density takes the form p x ¬µ f x ¬µ 2 232 then the parameter ¬µ is known as a location parameter',\n",
       " 'here we consider two simple examples of noninformative priors berger 1985 first of all if a density takes the form p x ¬µ f x ¬µ 2 232 then the parameter ¬µ is known as a location parameter this family of densities exhibits translation invariancebecause if we shift x by a constant to giveÀÜx x c then p ÀÜx ÀÜ¬µ f ÀÜx ÀÜ¬µ 2 233 where we have deÔ¨Åned ÀÜ¬µ ¬µ c thus the density takes the same form in the new variable as in the original one and so the density is independent of the choice of origin we would like to choose a prior distribution that reÔ¨Çects this translation invariance property and so we choose a prior that assigns equal probability mass to 2 4',\n",
       " 'this family of densities exhibits translation invariancebecause if we shift x by a constant to giveÀÜx x c then p ÀÜx ÀÜ¬µ f ÀÜx ÀÜ¬µ 2 233 where we have deÔ¨Åned ÀÜ¬µ ¬µ c thus the density takes the same form in the new variable as in the original one and so the density is independent of the choice of origin we would like to choose a prior distribution that reÔ¨Çects this translation invariance property and so we choose a prior that assigns equal probability mass to 2 4 the exponential family 119 an interval a ¬µ b as to the shifted interval a c ¬µ b c this implies b a p ¬µ d ¬µ b c a c p ¬µ d ¬µ b a p ¬µ c d¬µ 2 234 and because this must hold for all choices of a and b w eh a v e p ¬µ c p ¬µ 2 235 which implies that p ¬µ is constant',\n",
       " 'we would like to choose a prior distribution that reÔ¨Çects this translation invariance property and so we choose a prior that assigns equal probability mass to 2 4 the exponential family 119 an interval a ¬µ b as to the shifted interval a c ¬µ b c this implies b a p ¬µ d ¬µ b c a c p ¬µ d ¬µ b a p ¬µ c d¬µ 2 234 and because this must hold for all choices of a and b w eh a v e p ¬µ c p ¬µ 2 235 which implies that p ¬µ is constant an example of a location parameter would be the mean ¬µ of a gaussian distribution as we have seen the conjugate prior distri bution for ¬µ in this case is a gaussian p ¬µ ¬µ0 œÉ 2 0 n ¬µ ¬µ0 œÉ 2 0 and we obtain a noninformative prior by taking the limit œÉ2 0',\n",
       " 'an example of a location parameter would be the mean ¬µ of a gaussian distribution as we have seen the conjugate prior distri bution for ¬µ in this case is a gaussian p ¬µ ¬µ0 œÉ 2 0 n ¬µ ¬µ0 œÉ 2 0 and we obtain a noninformative prior by taking the limit œÉ2 0 indeed from 2 141 and 2 142 we see that this gives a posterior distribution over ¬µ in which the contributions from the prior vanish as a second example consider a density of the form p x œÉ 1 œÉf x œÉ 2 236 where œÉ 0 note that this will be a normalized density provided f x is correctly normalized the parameter œÉ is known as ascale parameter and the density exhibitsexercise 2 59 scale invariancebecause if we scale x by a constant to give ÀÜx cx then p ÀÜx ÀÜœÉ 1 ÀÜœÉf ÀÜx ÀÜœÉ 2 237 where we have deÔ¨Åned ÀÜœÉ cœÉ',\n",
       " 'note that this will be a normalized density provided f x is correctly normalized the parameter œÉ is known as ascale parameter and the density exhibitsexercise 2 59 scale invariancebecause if we scale x by a constant to give ÀÜx cx then p ÀÜx ÀÜœÉ 1 ÀÜœÉf ÀÜx ÀÜœÉ 2 237 where we have deÔ¨Åned ÀÜœÉ cœÉ this transformation corresponds to a change of scale for example from meters to kilometers if x is a length and we would like to choose a prior distribution that reÔ¨Çects this scale invariance if we consider an interval a œÉ b and a scaled interval a c œÉ b c then the prior should assign equal probability mass to these two intervals',\n",
       " 'this transformation corresponds to a change of scale for example from meters to kilometers if x is a length and we would like to choose a prior distribution that reÔ¨Çects this scale invariance if we consider an interval a œÉ b and a scaled interval a c œÉ b c then the prior should assign equal probability mass to these two intervals thus we have b a p œÉ d œÉ b c a c p œÉ d œÉ b a p 1 cœÉ 1 c dœÉ 2 238 and because this must hold for choices of a and b w eh a v e p œÉ p 1 cœÉ 1 c 2 239 and hence p œÉ 1 œÉ note that again this is an improper prior because the integral of the distribution over 0 œÉ is divergent it is sometimes also convenient to think of the prior distribution for a scale parameter in terms of the density of the log of the parameter',\n",
       " 'note that again this is an improper prior because the integral of the distribution over 0 œÉ is divergent it is sometimes also convenient to think of the prior distribution for a scale parameter in terms of the density of the log of the parameter using the transformation rule 1 27 for densities we see that p lnœÉ const thus for this prior there is the same probability mass in the range 1 œÉ 10 as in the range 10 œÉ 100 and in 100 œÉ 1000 120 2 probability distributions an example of a scale parameter would be the standard deviationœÉ of a gaussian distribution after we have taken account of the location parameter ¬µ because n x ¬µ œÉ2 œÉ 1 exp x œÉ 2 2 240 where x x ¬µ',\n",
       " '120 2 probability distributions an example of a scale parameter would be the standard deviationœÉ of a gaussian distribution after we have taken account of the location parameter ¬µ because n x ¬µ œÉ2 œÉ 1 exp x œÉ 2 2 240 where x x ¬µ as discussed earlier it is often more convenient to work in terms of the precision Œª 1 œÉ2 rather than œÉ itself using the transformation rule for densities we see that a distribution p œÉ 1 œÉ corresponds to a distribution over Œª of the form p Œª 1 Œª we have seen that the conjugate prior forŒª was the gamma distribution gam Œª a0 b 0 given by 2 146 the noninformative prior is obtainedsection 2 3 as the special casea0 b0 0',\n",
       " 'we have seen that the conjugate prior forŒª was the gamma distribution gam Œª a0 b 0 given by 2 146 the noninformative prior is obtainedsection 2 3 as the special casea0 b0 0 again if we examine the results 2 150 and 2 151 for the posterior distribution ofŒª we see that for a0 b0 0 the posterior depends only on terms arising from the data and not from the prior 2 5 nonparametric methods throughout this chapter we have focussed on the use of probability distributions having speciÔ¨Åc functional forms governed by a small number of parameters whose values are to be determined from a data set this is called the parametric approach to density modelling',\n",
       " 'nonparametric methods throughout this chapter we have focussed on the use of probability distributions having speciÔ¨Åc functional forms governed by a small number of parameters whose values are to be determined from a data set this is called the parametric approach to density modelling an important limitation of this approach is that the chosen density might be a poor model of the distribution that generates the data which can result in poor predictive performance for instance if the process that generates the data is multimodal then this aspect of the distribution can never be captured by a gaussian which is necessarily unimodal in this Ô¨Ånal section we consider some nonparametric approaches to density es timation that make few assumptions about the form of the distribution',\n",
       " 'for instance if the process that generates the data is multimodal then this aspect of the distribution can never be captured by a gaussian which is necessarily unimodal in this Ô¨Ånal section we consider some nonparametric approaches to density es timation that make few assumptions about the form of the distribution here we shall focus mainly on simple frequentist methods the reader should be aware however that nonparametric bayesian methods are attracting increasing interest walkeret al 1999 neal 2000 m uller and quintana 2004 teh et al 2006',\n",
       " 'here we shall focus mainly on simple frequentist methods the reader should be aware however that nonparametric bayesian methods are attracting increasing interest walkeret al 1999 neal 2000 m uller and quintana 2004 teh et al 2006 let us start with a discussion of histogram methods for density estimation which we have already encountered in the context of marginal and conditional distributions in figure 1 11 and in the context of the central limit theorem in figure 2 6 here we explore the properties of histogram density models in more detail focussing on the case of a single continuous variable x standard histograms simply partition x into distinct bins of width i and then count the number ni of observations of x falling in bin i',\n",
       " 'here we explore the properties of histogram density models in more detail focussing on the case of a single continuous variable x standard histograms simply partition x into distinct bins of width i and then count the number ni of observations of x falling in bin i in order to turn this count into a normalized probability density we simply divide by the total number n of observations and by the width i of the bins to obtain probability values for each bin given by pi ni n i 2 241 for which it is easily seen that p x d x 1 this gives a model for the density p x that is constant over the width of each bin and often the bins are chosen to have the same width i 2 5',\n",
       " 'this gives a model for the density p x that is constant over the width of each bin and often the bins are chosen to have the same width i 2 5 nonparametric methods 121 figure 2 24 an illustration of the histogram approach to density estimation in which a data set of 50 data points is generated from the distribution shown by the green curve histogram density estimates based on 2 241 with a common bin width are shown for various values of 0 04 0 0 5 1 0 5 0 08 0 0 5 1 0 5 0 25 0 0 5 1 0 5 in figure 2 24 we show an example of histogram density estimation here the data is drawn from the distribution corresponding to the green curve which is formed from a mixture of two gaussians',\n",
       " '0 04 0 0 5 1 0 5 0 08 0 0 5 1 0 5 0 25 0 0 5 1 0 5 in figure 2 24 we show an example of histogram density estimation here the data is drawn from the distribution corresponding to the green curve which is formed from a mixture of two gaussians also shown are three examples of his togram density estimates corresponding to three different choices for the bin width we see that when is very small top Ô¨Ågure the resulting density model is very spiky with a lot of structure that is not present in the underlying distribution that generated the data set conversely if is too large bottom Ô¨Ågure then the result is a model that is too smooth and that consequently fails to capture the bimodal prop erty of the green curve',\n",
       " 'we see that when is very small top Ô¨Ågure the resulting density model is very spiky with a lot of structure that is not present in the underlying distribution that generated the data set conversely if is too large bottom Ô¨Ågure then the result is a model that is too smooth and that consequently fails to capture the bimodal prop erty of the green curve the best results are obtained for some intermediate value of middle Ô¨Ågure in principle a histogram density model is also dependent on the choice of edge location for the bins though this is typically much less signiÔ¨Åcant than the value of',\n",
       " 'the best results are obtained for some intermediate value of middle Ô¨Ågure in principle a histogram density model is also dependent on the choice of edge location for the bins though this is typically much less signiÔ¨Åcant than the value of note that the histogram method has the property unlike the methods to be dis cussed shortly that once the histogram has been computed the data set itself can be discarded which can be advantageous if the data set is large also the histogram approach is easily applied if the data points are arriving sequentially in practice the histogram technique can be useful for obtaining a quick visual ization of data in one or two dimensions but is unsuited to most density estimation applications',\n",
       " 'also the histogram approach is easily applied if the data points are arriving sequentially in practice the histogram technique can be useful for obtaining a quick visual ization of data in one or two dimensions but is unsuited to most density estimation applications one obvious problem is that the estimated density has discontinuities that are due to the bin edges rather than any property of the underlying distribution that generated the data another major limitation of the histogram approach is its scaling with dimensionality if we divide each variable in a d dimensional space into m bins then the total number of bins will be md this exponential scaling with d is an example of the curse of dimensionality',\n",
       " 'if we divide each variable in a d dimensional space into m bins then the total number of bins will be md this exponential scaling with d is an example of the curse of dimensionality in a space of high dimensional section 1 4 ity the quantity of data needed to provide meaningful estimates of local probability density would be prohibitive the histogram approach to density estimation does however teach us two im portant lessons first to estimate the probability density at a particular location we should consider the data points that lie within some local neighbourhood of that point note that the concept of locality requires that we assume some form of dis tance measure and here we have been assuming euclidean distance for histograms 122 2',\n",
       " 'note that the concept of locality requires that we assume some form of dis tance measure and here we have been assuming euclidean distance for histograms 122 2 probability distributions this neighbourhood property was deÔ¨Åned by the bins and there is a natural smooth ing parameter describing the spatial extent of the local region in this case the bin width second the value of the smoothing parameter should be neither too large nor too small in order to obtain good results this is reminiscent of the choice of model complexity in polynomial curve Ô¨Åtting discussed in chapter 1 where the degree m of the polynomial or alternatively the value Œ± of the regularization parameter was optimal for some intermediate value neither too large nor too small',\n",
       " 'second the value of the smoothing parameter should be neither too large nor too small in order to obtain good results this is reminiscent of the choice of model complexity in polynomial curve Ô¨Åtting discussed in chapter 1 where the degree m of the polynomial or alternatively the value Œ± of the regularization parameter was optimal for some intermediate value neither too large nor too small armed with these insights we turn now to a discussion of two widely used nonparametric tech niques for density estimation kernel estimators and nearest neighbours which have better scaling with dimensionality than the simple histogram model',\n",
       " 'this is reminiscent of the choice of model complexity in polynomial curve Ô¨Åtting discussed in chapter 1 where the degree m of the polynomial or alternatively the value Œ± of the regularization parameter was optimal for some intermediate value neither too large nor too small armed with these insights we turn now to a discussion of two widely used nonparametric tech niques for density estimation kernel estimators and nearest neighbours which have better scaling with dimensionality than the simple histogram model 2 5 1 kernel density estimators let us suppose that observations are being drawn from some unknown probabil ity density p x in some d dimensional space which we shall take to be euclidean and we wish to estimate the value of p x',\n",
       " 'armed with these insights we turn now to a discussion of two widely used nonparametric tech niques for density estimation kernel estimators and nearest neighbours which have better scaling with dimensionality than the simple histogram model 2 5 1 kernel density estimators let us suppose that observations are being drawn from some unknown probabil ity density p x in some d dimensional space which we shall take to be euclidean and we wish to estimate the value of p x from our earlier discussion of locality let us consider some small region r containing x the probability mass associated with this region is given by p r p x dx 2 242 now suppose that we have collected a data set comprising n observations drawn from p x',\n",
       " 'the probability mass associated with this region is given by p r p x dx 2 242 now suppose that we have collected a data set comprising n observations drawn from p x because each data point has a probability p of falling within r the total number k of points that lie inside r will be distributed according to the binomial distributionsection 2 1 bin k n p n k n k pk 1 p 1 k 2 243 using 2 11 we see that the mean fraction of points falling inside the region is e k n p and similarly using 2 12 we see that the variance around this mean is var k n p 1 p n for large n this distribution will be sharply peaked around the mean and so k np',\n",
       " '2 243 using 2 11 we see that the mean fraction of points falling inside the region is e k n p and similarly using 2 12 we see that the variance around this mean is var k n p 1 p n for large n this distribution will be sharply peaked around the mean and so k np 2 244 if however we also assume that the regionr is sufÔ¨Åciently small that the probability density p x is roughly constant over the region then we have p p x v 2 245 where v is the volume of r combining 2 244 and 2 245 we obtain our density estimate in the form p x k nv',\n",
       " 'for large n this distribution will be sharply peaked around the mean and so k np 2 244 if however we also assume that the regionr is sufÔ¨Åciently small that the probability density p x is roughly constant over the region then we have p p x v 2 245 where v is the volume of r combining 2 244 and 2 245 we obtain our density estimate in the form p x k nv 2 246 note that the validity of 2 246 depends on two contradictory assumptions namely that the regionr be sufÔ¨Åciently small that the density is approximately constant over the region and yet sufÔ¨Åciently large in relation to the value of that density that the number k of points falling inside the region is sufÔ¨Åcient for the binomial distribution to be sharply peaked 2 5',\n",
       " '2 246 note that the validity of 2 246 depends on two contradictory assumptions namely that the regionr be sufÔ¨Åciently small that the density is approximately constant over the region and yet sufÔ¨Åciently large in relation to the value of that density that the number k of points falling inside the region is sufÔ¨Åcient for the binomial distribution to be sharply peaked 2 5 nonparametric methods 123 we can exploit the result 2 246 in two different ways either we can Ô¨Åxk and determine the value of v from the data which gives rise to thek nearest neighbour technique discussed shortly or we can Ô¨Åx v and determine k from the data giv ing rise to the kernel approach',\n",
       " 'nonparametric methods 123 we can exploit the result 2 246 in two different ways either we can Ô¨Åxk and determine the value of v from the data which gives rise to thek nearest neighbour technique discussed shortly or we can Ô¨Åx v and determine k from the data giv ing rise to the kernel approach it can be shown that both the k nearest neighbour density estimator and the kernel density estimator converge to the true probability density in the limit n provided v shrinks suitably with n and k grows with n duda and hart 1973 we begin by discussing the kernel method in detail and to start with we take the region r to be a small hypercube centred on the point x at which we wish to determine the probability density',\n",
       " 'it can be shown that both the k nearest neighbour density estimator and the kernel density estimator converge to the true probability density in the limit n provided v shrinks suitably with n and k grows with n duda and hart 1973 we begin by discussing the kernel method in detail and to start with we take the region r to be a small hypercube centred on the point x at which we wish to determine the probability density in order to count the number k of points falling within this region it is convenient to deÔ¨Åne the following function k u 1 ui 1 2 i 1 d 0 otherwise 2 247 which represents a unit cube centred on the origin the function k u is an example of a kernel function and in this context is also called aparzen window',\n",
       " 'in order to count the number k of points falling within this region it is convenient to deÔ¨Åne the following function k u 1 ui 1 2 i 1 d 0 otherwise 2 247 which represents a unit cube centred on the origin the function k u is an example of a kernel function and in this context is also called aparzen window from 2 247 the quantity k x xn h will be one if the data point xn lies inside a cube of side h centred on x and zero otherwise the total number of data points lying inside this cube will therefore be k n n 1 k x xn h',\n",
       " 'from 2 247 the quantity k x xn h will be one if the data point xn lies inside a cube of side h centred on x and zero otherwise the total number of data points lying inside this cube will therefore be k n n 1 k x xn h 2 248 substituting this expression into 2 246 then gives the following result for the esti mated density at x p x 1 n n n 1 1 hd k x xn h 2 249 where we have used v hd for the volume of a hypercube of side h in d di mensions using the symmetry of the function k u we can now re interpret this equation not as a single cube centred on x but as the sum over n cubes centred on the n data points xn',\n",
       " '2 248 substituting this expression into 2 246 then gives the following result for the esti mated density at x p x 1 n n n 1 1 hd k x xn h 2 249 where we have used v hd for the volume of a hypercube of side h in d di mensions using the symmetry of the function k u we can now re interpret this equation not as a single cube centred on x but as the sum over n cubes centred on the n data points xn as it stands the kernel density estimator 2 249 will suffer from one of the same problems that the histogram method suffered from namely the presence of artiÔ¨Åcial discontinuities in this case at the boundaries of the cubes',\n",
       " 'using the symmetry of the function k u we can now re interpret this equation not as a single cube centred on x but as the sum over n cubes centred on the n data points xn as it stands the kernel density estimator 2 249 will suffer from one of the same problems that the histogram method suffered from namely the presence of artiÔ¨Åcial discontinuities in this case at the boundaries of the cubes we can obtain a smoother density model if we choose a smoother kernel function and a common choice is the gaussian which gives rise to the following kernel density model p x 1 n n n 1 1 2œÄh2 1 2 exp x xn 2 2h2 2 250 where h represents the standard deviation of the gaussian components',\n",
       " 'as it stands the kernel density estimator 2 249 will suffer from one of the same problems that the histogram method suffered from namely the presence of artiÔ¨Åcial discontinuities in this case at the boundaries of the cubes we can obtain a smoother density model if we choose a smoother kernel function and a common choice is the gaussian which gives rise to the following kernel density model p x 1 n n n 1 1 2œÄh2 1 2 exp x xn 2 2h2 2 250 where h represents the standard deviation of the gaussian components thus our density model is obtained by placing a gaussian over each data point and then adding up the contributions over the whole data set and then dividing byn so that the den sity is correctly normalized in figure 2 25 we apply the model 2 250 to the data 124 2',\n",
       " 'thus our density model is obtained by placing a gaussian over each data point and then adding up the contributions over the whole data set and then dividing byn so that the den sity is correctly normalized in figure 2 25 we apply the model 2 250 to the data 124 2 probability distributions figure 2 25 illustration of the kernel density model 2 250 applied to the same data set used to demonstrate the histogram approach in figure 2 24 we see that h acts as a smoothing parameter and that if it is set too small top panel the result is a very noisy density model whereas if it is set too large bottom panel then the bimodal nature of the underlying distribution from which the data is generated shown by the green curve is washed out',\n",
       " 'probability distributions figure 2 25 illustration of the kernel density model 2 250 applied to the same data set used to demonstrate the histogram approach in figure 2 24 we see that h acts as a smoothing parameter and that if it is set too small top panel the result is a very noisy density model whereas if it is set too large bottom panel then the bimodal nature of the underlying distribution from which the data is generated shown by the green curve is washed out the best den sity model is obtained for some intermedi ate value of h middle panel h 0 005 0 0 5 1 0 5 h 0 07 0 0 5 1 0 5 h 0 2 0 0 5 1 0 5 set used earlier to demonstrate the histogram technique',\n",
       " 'the best den sity model is obtained for some intermedi ate value of h middle panel h 0 005 0 0 5 1 0 5 h 0 07 0 0 5 1 0 5 h 0 2 0 0 5 1 0 5 set used earlier to demonstrate the histogram technique we see that as expected the parameter h plays the role of a smoothing parameter and there is a trade off between sensitivity to noise at small h and over smoothing at large h again the optimization of h is a problem in model complexity analogous to the choice of bin width in histogram density estimation or the degree of the polynomial used in curve Ô¨Åtting we can choose any other kernel function k u in 2 249 subject to the condi tions k u 0 2 251 k u d u 1 2 252 which ensure that the resulting probability distribution is nonnegative everywhere and integrates to one',\n",
       " 'we see that as expected the parameter h plays the role of a smoothing parameter and there is a trade off between sensitivity to noise at small h and over smoothing at large h again the optimization of h is a problem in model complexity analogous to the choice of bin width in histogram density estimation or the degree of the polynomial used in curve Ô¨Åtting we can choose any other kernel function k u in 2 249 subject to the condi tions k u 0 2 251 k u d u 1 2 252 which ensure that the resulting probability distribution is nonnegative everywhere and integrates to one the class of density model given by 2 249 is called a kernel density estimator or parzen estimator',\n",
       " 'we can choose any other kernel function k u in 2 249 subject to the condi tions k u 0 2 251 k u d u 1 2 252 which ensure that the resulting probability distribution is nonnegative everywhere and integrates to one the class of density model given by 2 249 is called a kernel density estimator or parzen estimator it has a great merit that there is no compu tation involved in the training phase because this simply requires storage of the training set however this is also one of its great weaknesses because the computa tional cost of evaluating the density grows linearly with the size of the data set',\n",
       " 'it has a great merit that there is no compu tation involved in the training phase because this simply requires storage of the training set however this is also one of its great weaknesses because the computa tional cost of evaluating the density grows linearly with the size of the data set 2 5 2 nearest neighbour methods one of the difÔ¨Åculties with the kernel approach to density estimation is that the parameter h governing the kernel width is Ô¨Åxed for all kernels in regions of high data density a large value of h may lead to over smoothing and a washing out of structure that might otherwise be extracted from the data however reducing h may lead to noisy estimates elsewhere in data space where the density is smaller',\n",
       " 'in regions of high data density a large value of h may lead to over smoothing and a washing out of structure that might otherwise be extracted from the data however reducing h may lead to noisy estimates elsewhere in data space where the density is smaller thus the optimal choice for h may be dependent on location within the data space this issue is addressed by nearest neighbour methods for density estimation we therefore return to our general result 2 246 for local density estimation and instead of Ô¨Åxing v and determining the value of k from the data we consider a Ô¨Åxed value of k and use the data to Ô¨Ånd an appropriate value for v to do this we consider a small sphere centred on the point x at which we wish to estimate the 2 5',\n",
       " 'we therefore return to our general result 2 246 for local density estimation and instead of Ô¨Åxing v and determining the value of k from the data we consider a Ô¨Åxed value of k and use the data to Ô¨Ånd an appropriate value for v to do this we consider a small sphere centred on the point x at which we wish to estimate the 2 5 nonparametric methods 125 figure 2 26 illustration of k nearest neighbour den sity estimation using the same data set as in figures 2 25 and 2 24 we see that the parameter k governs the degree of smoothing so that a small value of k leads to a very noisy density model top panel whereas a large value bot tom panel smoothes out the bimodal na ture of the true distribution shown by the green curve from which the data set was generated',\n",
       " 'nonparametric methods 125 figure 2 26 illustration of k nearest neighbour den sity estimation using the same data set as in figures 2 25 and 2 24 we see that the parameter k governs the degree of smoothing so that a small value of k leads to a very noisy density model top panel whereas a large value bot tom panel smoothes out the bimodal na ture of the true distribution shown by the green curve from which the data set was generated k 1 0 0 5 1 0 5 k 5 0 0 5 1 0 5 k 3 0 0 0 5 1 0 5 density p x and we allow the radius of the sphere to grow until it contains precisely k data points the estimate of the densityp x is then given by 2 246 withv set to the volume of the resulting sphere',\n",
       " 'k 1 0 0 5 1 0 5 k 5 0 0 5 1 0 5 k 3 0 0 0 5 1 0 5 density p x and we allow the radius of the sphere to grow until it contains precisely k data points the estimate of the densityp x is then given by 2 246 withv set to the volume of the resulting sphere this technique is known ask nearest neighbours and is illustrated in figure 2 26 for various choices of the parameter k using the same data set as used in figure 2 24 and figure 2 25 we see that the value of k now governs the degree of smoothing and that again there is an optimum choice for k that is neither too large nor too small',\n",
       " 'this technique is known ask nearest neighbours and is illustrated in figure 2 26 for various choices of the parameter k using the same data set as used in figure 2 24 and figure 2 25 we see that the value of k now governs the degree of smoothing and that again there is an optimum choice for k that is neither too large nor too small note that the model produced byk nearest neighbours is not a true density model because the integral over all space diverges exercise 2 61 we close this chapter by showing how the k nearest neighbour technique for density estimation can be extended to the problem of classiÔ¨Åcation to do this we apply the k nearest neighbour density estimation technique to each class separately and then make use of bayes theorem',\n",
       " 'note that the model produced byk nearest neighbours is not a true density model because the integral over all space diverges exercise 2 61 we close this chapter by showing how the k nearest neighbour technique for density estimation can be extended to the problem of classiÔ¨Åcation to do this we apply the k nearest neighbour density estimation technique to each class separately and then make use of bayes theorem let us suppose that we have a data set com prising nk points in class ck with n points in total so that k nk n i f w e wish to classify a new point x we draw a sphere centred on x containing precisely k points irrespective of their class suppose this sphere has volume v and contains kk points from class ck',\n",
       " 'let us suppose that we have a data set com prising nk points in class ck with n points in total so that k nk n i f w e wish to classify a new point x we draw a sphere centred on x containing precisely k points irrespective of their class suppose this sphere has volume v and contains kk points from class ck then 2 246 provides an estimate of the density associated with each class p x ck kk nkv 2 253 similarly the unconditional density is given by p x k nv 2 254 while the class priors are given by p ck nk n 2 255 we can now combine 2 253 2 254 and 2 255 using bayes theorem to obtain the posterior probability of class membership p ck x p x ck p ck p x kk k 2 256 126 2',\n",
       " '2 255 we can now combine 2 253 2 254 and 2 255 using bayes theorem to obtain the posterior probability of class membership p ck x p x ck p ck p x kk k 2 256 126 2 probability distributions figure 2 27 a in the k nearest neighbour classiÔ¨Åer a new point shown by the black diamond is clas siÔ¨Åed according to the majority class membership of the k closest train ing data points in this case k 3 b in the nearest neighbour k 1 approach to classiÔ¨Åcation the resulting decision boundary is composed of hyperplanes that form perpendicular bisectors of pairs of points from different classes',\n",
       " 'probability distributions figure 2 27 a in the k nearest neighbour classiÔ¨Åer a new point shown by the black diamond is clas siÔ¨Åed according to the majority class membership of the k closest train ing data points in this case k 3 b in the nearest neighbour k 1 approach to classiÔ¨Åcation the resulting decision boundary is composed of hyperplanes that form perpendicular bisectors of pairs of points from different classes x1 x2 a x1 x2 b if we wish to minimize the probability of misclassiÔ¨Åcation this is done by assigning the test point x to the class having the largest posterior probability corresponding to the largest value of kk k',\n",
       " 'b in the nearest neighbour k 1 approach to classiÔ¨Åcation the resulting decision boundary is composed of hyperplanes that form perpendicular bisectors of pairs of points from different classes x1 x2 a x1 x2 b if we wish to minimize the probability of misclassiÔ¨Åcation this is done by assigning the test point x to the class having the largest posterior probability corresponding to the largest value of kk k thus to classify a new point we identify the k nearest points from the training data set and then assign the new point to the class having the largest number of representatives amongst this set ties can be broken at random',\n",
       " 'thus to classify a new point we identify the k nearest points from the training data set and then assign the new point to the class having the largest number of representatives amongst this set ties can be broken at random the particular case of k 1 is called the nearest neighbour rule because a test point is simply assigned to the same class as the nearest point from the training set these concepts are illustrated in figure 2 27 in figure 2 28 we show the results of applying the k nearest neighbour algo rithm to the oil Ô¨Çow data introduced in chapter 1 for various values of k a s expected we see that k controls the degree of smoothing so that small k produces many small regions of each class whereas large k leads to fewer larger regions',\n",
       " 'these concepts are illustrated in figure 2 27 in figure 2 28 we show the results of applying the k nearest neighbour algo rithm to the oil Ô¨Çow data introduced in chapter 1 for various values of k a s expected we see that k controls the degree of smoothing so that small k produces many small regions of each class whereas large k leads to fewer larger regions x6 x7 k 1 0 1 2 0 1 2 x6 x7 k 3 0 1 2 0 1 2 x6 x7 k 31 0 1 2 0 1 2 figure 2 28 plot of 200 data points from the oil data set showing values of x6 plotted against x7 where the red green and blue points correspond to the laminar annular and homogeneous classes respectively',\n",
       " 'in figure 2 28 we show the results of applying the k nearest neighbour algo rithm to the oil Ô¨Çow data introduced in chapter 1 for various values of k a s expected we see that k controls the degree of smoothing so that small k produces many small regions of each class whereas large k leads to fewer larger regions x6 x7 k 1 0 1 2 0 1 2 x6 x7 k 3 0 1 2 0 1 2 x6 x7 k 31 0 1 2 0 1 2 figure 2 28 plot of 200 data points from the oil data set showing values of x6 plotted against x7 where the red green and blue points correspond to the laminar annular and homogeneous classes respectively also shown are the classiÔ¨Åcations of the input space given by the k nearest neighbour algorithm for various values of k exercises 127 an interesting property of the nearest neighbour k 1 classiÔ¨Åer is that in the limit n the error rate is never more than twice the minimum achievable error rate of an optimal classiÔ¨Åer i e one that uses the true class distributions cover and hart 1967',\n",
       " 'x6 x7 k 1 0 1 2 0 1 2 x6 x7 k 3 0 1 2 0 1 2 x6 x7 k 31 0 1 2 0 1 2 figure 2 28 plot of 200 data points from the oil data set showing values of x6 plotted against x7 where the red green and blue points correspond to the laminar annular and homogeneous classes respectively also shown are the classiÔ¨Åcations of the input space given by the k nearest neighbour algorithm for various values of k exercises 127 an interesting property of the nearest neighbour k 1 classiÔ¨Åer is that in the limit n the error rate is never more than twice the minimum achievable error rate of an optimal classiÔ¨Åer i e one that uses the true class distributions cover and hart 1967 as discussed so far both the k nearest neighbour method and the kernel den sity estimator require the entire training data set to be stored leading to expensive computation if the data set is large',\n",
       " 'also shown are the classiÔ¨Åcations of the input space given by the k nearest neighbour algorithm for various values of k exercises 127 an interesting property of the nearest neighbour k 1 classiÔ¨Åer is that in the limit n the error rate is never more than twice the minimum achievable error rate of an optimal classiÔ¨Åer i e one that uses the true class distributions cover and hart 1967 as discussed so far both the k nearest neighbour method and the kernel den sity estimator require the entire training data set to be stored leading to expensive computation if the data set is large this effect can be offset at the expense of some additional one off computation by constructing tree based search structures to allow approximate near neighbours to be found efÔ¨Åciently without doing an exhaustive search of the data set',\n",
       " 'as discussed so far both the k nearest neighbour method and the kernel den sity estimator require the entire training data set to be stored leading to expensive computation if the data set is large this effect can be offset at the expense of some additional one off computation by constructing tree based search structures to allow approximate near neighbours to be found efÔ¨Åciently without doing an exhaustive search of the data set nevertheless these nonparametric methods are still severely limited on the other hand we have seen that simple parametric models are very restricted in terms of the forms of distribution that they can represent',\n",
       " 'nevertheless these nonparametric methods are still severely limited on the other hand we have seen that simple parametric models are very restricted in terms of the forms of distribution that they can represent we therefore need to Ô¨Ånd density models that are very Ô¨Çexible and yet for which the complexity of the models can be controlled independently of the size of the training set and we shall see in subsequent chapters how to achieve this exercises 2 1 www verify that the bernoulli distribution 2 2 satisÔ¨Åes the following prop erties 1 x 0 p x ¬µ 1 2 257 e x ¬µ 2 258 var x ¬µ 1 ¬µ 2 259 show that the entropy h x of a bernoulli distributed random binary variable x is given by h x ¬µln ¬µ 1 ¬µ l n 1 ¬µ',\n",
       " 'exercises 2 1 www verify that the bernoulli distribution 2 2 satisÔ¨Åes the following prop erties 1 x 0 p x ¬µ 1 2 257 e x ¬µ 2 258 var x ¬µ 1 ¬µ 2 259 show that the entropy h x of a bernoulli distributed random binary variable x is given by h x ¬µln ¬µ 1 ¬µ l n 1 ¬µ 2 260 2 2 the form of the bernoulli distribution given by 2 2 is not symmetric be tween the two values of x in some situations it will be more convenient to use an equivalent formulation for which x 1 1 in which case the distribution can be written p x ¬µ 1 ¬µ 2 1 x 2 1 ¬µ 2 1 x 2 2 261 where ¬µ 1 1 show that the distribution 2 261 is normalized and evaluate its mean variance and entropy',\n",
       " 'in some situations it will be more convenient to use an equivalent formulation for which x 1 1 in which case the distribution can be written p x ¬µ 1 ¬µ 2 1 x 2 1 ¬µ 2 1 x 2 2 261 where ¬µ 1 1 show that the distribution 2 261 is normalized and evaluate its mean variance and entropy 2 3 www in this exercise we prove that the binomial distribution 2 9 is nor malized first use the deÔ¨Ånition 2 10 of the number of combinations ofm identical objects chosen from a total of n to show that n m n m 1 n 1 m 2 262 128 2 probability distributions use this result to prove by induction the following result 1 x n n m 0 n m xm 2 263 which is known as the binomial theorem and which is valid for all real values of x',\n",
       " '2 262 128 2 probability distributions use this result to prove by induction the following result 1 x n n m 0 n m xm 2 263 which is known as the binomial theorem and which is valid for all real values of x finally show that the binomial distribution is normalized so that n m 0 n m ¬µm 1 ¬µ n m 1 2 264 which can be done by Ô¨Årst pulling out a factor 1 ¬µ n out of the summation and then making use of the binomial theorem 2 4 show that the mean of the binomial distribution is given by 2 11 to do this differentiate both sides of the normalization condition 2 264 with respect to ¬µ and then rearrange to obtain an expression for the mean ofn',\n",
       " '2 4 show that the mean of the binomial distribution is given by 2 11 to do this differentiate both sides of the normalization condition 2 264 with respect to ¬µ and then rearrange to obtain an expression for the mean ofn similarly by differentiating 2 264 twice with respect to ¬µ and making use of the result 2 11 for the mean of the binomial distribution prove the result 2 12 for the variance of the binomial 2 5 www in this exercise we prove that the beta distribution given by 2 13 is correctly normalized so that 2 14 holds this is equivalent to showing that 1 0 ¬µa 1 1 ¬µ b 1 d¬µ Œ≥ a Œ≥ b Œ≥ a b 2 265 from the deÔ¨Ånition 1 141 of the gamma function we have Œ≥ a Œ≥ b 0 exp x xa 1 dx 0 exp y yb 1 dy',\n",
       " 'this is equivalent to showing that 1 0 ¬µa 1 1 ¬µ b 1 d¬µ Œ≥ a Œ≥ b Œ≥ a b 2 265 from the deÔ¨Ånition 1 141 of the gamma function we have Œ≥ a Œ≥ b 0 exp x xa 1 dx 0 exp y yb 1 dy 2 266 use this expression to prove 2 265 as follows first bring the integral overy inside the integrand of the integral over x next make the change of variable t y x where x is Ô¨Åxed then interchange the order of the x and t integrations and Ô¨Ånally make the change of variable x t¬µ where t is Ô¨Åxed 2 6 make use of the result 2 265 to show that the mean variance and mode of the beta distribution 2 13 are given respectively by e ¬µ a a b 2 267 var ¬µ ab a b 2 a b 1 2 268 mode ¬µ a 1 a b 2',\n",
       " 'first bring the integral overy inside the integrand of the integral over x next make the change of variable t y x where x is Ô¨Åxed then interchange the order of the x and t integrations and Ô¨Ånally make the change of variable x t¬µ where t is Ô¨Åxed 2 6 make use of the result 2 265 to show that the mean variance and mode of the beta distribution 2 13 are given respectively by e ¬µ a a b 2 267 var ¬µ ab a b 2 a b 1 2 268 mode ¬µ a 1 a b 2 2 269 exercises 129 2 7 consider a binomial random variable x given by 2 9 with prior distribution for ¬µ given by the beta distribution 2 13 and suppose we have observed m occur rences of x 1 and l occurrences of x 0',\n",
       " '2 6 make use of the result 2 265 to show that the mean variance and mode of the beta distribution 2 13 are given respectively by e ¬µ a a b 2 267 var ¬µ ab a b 2 a b 1 2 268 mode ¬µ a 1 a b 2 2 269 exercises 129 2 7 consider a binomial random variable x given by 2 9 with prior distribution for ¬µ given by the beta distribution 2 13 and suppose we have observed m occur rences of x 1 and l occurrences of x 0 show that the posterior mean value ofx lies between the prior mean and the maximum likelihood estimate for ¬µ to do this show that the posterior mean can be written as Œª times the prior mean plus 1 Œª times the maximum likelihood estimate where 0 Œª 1',\n",
       " 'show that the posterior mean value ofx lies between the prior mean and the maximum likelihood estimate for ¬µ to do this show that the posterior mean can be written as Œª times the prior mean plus 1 Œª times the maximum likelihood estimate where 0 Œª 1 this illustrates the con cept of the posterior distribution being a compromise between the prior distribution and the maximum likelihood solution 2 8 consider two variables x and y with joint distribution p x y prove the follow ing two results e x ey ex x y 2 270 var x ey varx x y vary ex x y 2 271 here ex x y denotes the expectation of x under the conditional distribution p x y with a similar notation for the conditional variance 2 9 www',\n",
       " '2 271 here ex x y denotes the expectation of x under the conditional distribution p x y with a similar notation for the conditional variance 2 9 www in this exercise we prove the normalization of the dirichlet dis tribution 2 38 using induction we have already shown in exercise 2 5 that the beta distribution which is a special case of the dirichlet for m 2 is normalized we now assume that the dirichlet distribution is normalized for m 1 variables and prove that it is normalized for m variables',\n",
       " 'we have already shown in exercise 2 5 that the beta distribution which is a special case of the dirichlet for m 2 is normalized we now assume that the dirichlet distribution is normalized for m 1 variables and prove that it is normalized for m variables to do this consider the dirichlet distribution over m variables and take account of the constraint m k 1 ¬µk 1 by eliminating ¬µm so that the dirichlet is written pm ¬µ1 ¬µ m 1 cm m 1 k 1 ¬µŒ±k 1 k 1 m 1 j 1 ¬µj Œ±m 1 2 272 and our goal is to Ô¨Ånd an expression forcm to do this integrate over¬µm 1 taking care over the limits of integration and then make a change of variable so that this integral has limits 0 and 1 by assuming the correct result for cm 1 and making use of 2 265 derive the expression for cm',\n",
       " 'to do this integrate over¬µm 1 taking care over the limits of integration and then make a change of variable so that this integral has limits 0 and 1 by assuming the correct result for cm 1 and making use of 2 265 derive the expression for cm 2 10 using the property Œ≥ x 1 xŒ≥ x of the gamma function derive the following results for the mean variance and covariance of the dirichlet distribution given by 2 38 e ¬µj Œ±j Œ±0 2 273 var ¬µj Œ±j Œ±0 Œ±j Œ±2 0 Œ±0 1 2 274 cov ¬µj¬µl Œ±jŒ±l Œ±2 0 Œ±0 1 j l 2 275 where Œ±0 is deÔ¨Åned by 2 39 130 2',\n",
       " '2 10 using the property Œ≥ x 1 xŒ≥ x of the gamma function derive the following results for the mean variance and covariance of the dirichlet distribution given by 2 38 e ¬µj Œ±j Œ±0 2 273 var ¬µj Œ±j Œ±0 Œ±j Œ±2 0 Œ±0 1 2 274 cov ¬µj¬µl Œ±jŒ±l Œ±2 0 Œ±0 1 j l 2 275 where Œ±0 is deÔ¨Åned by 2 39 130 2 probability distributions 2 11 www by expressing the expectation of ln¬µj under the dirichlet distribution 2 38 as a derivative with respect to Œ±j show that e ln¬µj œà Œ±j œà Œ±0 2 276 where Œ±0 is given by 2 39 and œà a d da ln Œ≥ a 2 277 is the digamma function 2 12 the uniform distribution for a continuous variable x is deÔ¨Åned by u x a b 1 b a a x b',\n",
       " 'probability distributions 2 11 www by expressing the expectation of ln¬µj under the dirichlet distribution 2 38 as a derivative with respect to Œ±j show that e ln¬µj œà Œ±j œà Œ±0 2 276 where Œ±0 is given by 2 39 and œà a d da ln Œ≥ a 2 277 is the digamma function 2 12 the uniform distribution for a continuous variable x is deÔ¨Åned by u x a b 1 b a a x b 2 278 verify that this distribution is normalized and Ô¨Ånd expressions for its mean and variance 2 13 evaluate the kullback leibler divergence 1 113 between two gaussians p x n x ¬µ œÉ and q x n x m l 2 14 www this exercise demonstrates that the multivariate distribution with max imum entropy for a given covariance is a gaussian',\n",
       " '2 13 evaluate the kullback leibler divergence 1 113 between two gaussians p x n x ¬µ œÉ and q x n x m l 2 14 www this exercise demonstrates that the multivariate distribution with max imum entropy for a given covariance is a gaussian the entropy of a distribution p x is given by h x p x l np x dx 2 279 we wish to maximize h x over all distributions p x subject to the constraints that p x be normalized and that it have a speciÔ¨Åc mean and covariance so that p x dx 1 2 280 p x xdx ¬µ 2 281 p x x ¬µ x ¬µ t dx œÉ 2 282 by performing a variational maximization of 2 279 and using lagrange multipliers to enforce the constraints 2 280 2 281 and 2 282 show that the maximum likelihood distribution is given by the gaussian 2 43',\n",
       " '2 279 we wish to maximize h x over all distributions p x subject to the constraints that p x be normalized and that it have a speciÔ¨Åc mean and covariance so that p x dx 1 2 280 p x xdx ¬µ 2 281 p x x ¬µ x ¬µ t dx œÉ 2 282 by performing a variational maximization of 2 279 and using lagrange multipliers to enforce the constraints 2 280 2 281 and 2 282 show that the maximum likelihood distribution is given by the gaussian 2 43 2 15 show that the entropy of the multivariate gaussian n x ¬µ œÉ is given by h x 1 2 ln œÉ d 2 1 ln 2œÄ 2 283 where d is the dimensionality of x exercises 131 2 16 www consider two random variables x1 and x2 having gaussian distri butions with means ¬µ1 ¬µ2 and precisions œÑ1 œÑ2 respectively',\n",
       " '2 15 show that the entropy of the multivariate gaussian n x ¬µ œÉ is given by h x 1 2 ln œÉ d 2 1 ln 2œÄ 2 283 where d is the dimensionality of x exercises 131 2 16 www consider two random variables x1 and x2 having gaussian distri butions with means ¬µ1 ¬µ2 and precisions œÑ1 œÑ2 respectively derive an expression for the differential entropy of the variable x x1 x2 to do this Ô¨Årst Ô¨Ånd the distribution of x by using the relation p x p x x2 p x2 d x2 2 284 and completing the square in the exponent then observe that this represents the convolution of two gaussian distributions which itself will be gaussian and Ô¨Ånally make use of the result 1 110 for the entropy of the univariate gaussian',\n",
       " 'to do this Ô¨Årst Ô¨Ånd the distribution of x by using the relation p x p x x2 p x2 d x2 2 284 and completing the square in the exponent then observe that this represents the convolution of two gaussian distributions which itself will be gaussian and Ô¨Ånally make use of the result 1 110 for the entropy of the univariate gaussian 2 17 www consider the multivariate gaussian distribution given by 2 43 by writing the precision matrix inverse covariance matrix œÉ 1 as the sum of a sym metric and an anti symmetric matrix show that the anti symmetric term does not appear in the exponent of the gaussian and hence that the precision matrix may be taken to be symmetric without loss of generality',\n",
       " '2 17 www consider the multivariate gaussian distribution given by 2 43 by writing the precision matrix inverse covariance matrix œÉ 1 as the sum of a sym metric and an anti symmetric matrix show that the anti symmetric term does not appear in the exponent of the gaussian and hence that the precision matrix may be taken to be symmetric without loss of generality because the inverse of a symmetric matrix is also symmetric see exercise 2 22 it follows that the covariance matrix may also be chosen to be symmetric without loss of generality 2 18 consider a real symmetric matrix œÉ whose eigenvalue equation is given by 2 45',\n",
       " 'because the inverse of a symmetric matrix is also symmetric see exercise 2 22 it follows that the covariance matrix may also be chosen to be symmetric without loss of generality 2 18 consider a real symmetric matrix œÉ whose eigenvalue equation is given by 2 45 by taking the complex conjugate of this equation and subtracting the original equation and then forming the inner product with eigenvectoru i show that the eigenvalues Œªi are real similarly use the symmetry property of œÉ to show that two eigenvectors ui and uj will be orthogonal provided Œªj Œªi finally show that without loss of generality the set of eigenvectors can be chosen to be orthonormal so that they satisfy 2 46 even if some of the eigenvalues are zero',\n",
       " 'similarly use the symmetry property of œÉ to show that two eigenvectors ui and uj will be orthogonal provided Œªj Œªi finally show that without loss of generality the set of eigenvectors can be chosen to be orthonormal so that they satisfy 2 46 even if some of the eigenvalues are zero 2 19 show that a real symmetric matrix œÉ having the eigenvector equation 2 45 can be expressed as an expansion in the eigenvectors with coefÔ¨Åcients given by the eigenvalues of the form 2 48 similarly show that the inverse matrix œÉ 1 has a representation of the form 2 49 2 20 www a positive deÔ¨Ånite matrix œÉ can be deÔ¨Åned as one for which the quadratic form atœÉa 2 285 is positive for any real value of the vector a',\n",
       " 'similarly show that the inverse matrix œÉ 1 has a representation of the form 2 49 2 20 www a positive deÔ¨Ånite matrix œÉ can be deÔ¨Åned as one for which the quadratic form atœÉa 2 285 is positive for any real value of the vector a show that a necessary and sufÔ¨Åcient condition for œÉ to be positive deÔ¨Ånite is that all of the eigenvalues Œªi of œÉ deÔ¨Åned by 2 45 are positive 2 21 show that a real symmetric matrix of sized d has d d 1 2 independent parameters 2 22 www show that the inverse of a symmetric matrix is itself symmetric 2 23 by diagonalizing the coordinate system using the eigenvector expansion 2 45 show that the volume contained within the hyperellipsoid corresponding to a constant 132 2',\n",
       " '2 22 www show that the inverse of a symmetric matrix is itself symmetric 2 23 by diagonalizing the coordinate system using the eigenvector expansion 2 45 show that the volume contained within the hyperellipsoid corresponding to a constant 132 2 probability distributions mahalanobis distance is given by vd œÉ 1 2 d 2 286 where vd is the volume of the unit sphere in d dimensions and the mahalanobis distance is deÔ¨Åned by 2 44 2 24 www prove the identity 2 76 by multiplying both sides by the matrix ab cd 2 287 and making use of the deÔ¨Ånition 2 77 2 25 in sections 2 3 1 and 2 3 2 we considered the conditional and marginal distri butions for a multivariate gaussian',\n",
       " '2 24 www prove the identity 2 76 by multiplying both sides by the matrix ab cd 2 287 and making use of the deÔ¨Ånition 2 77 2 25 in sections 2 3 1 and 2 3 2 we considered the conditional and marginal distri butions for a multivariate gaussian more generally we can consider a partitioning of the components of x into three groups x a xb and xc with a corresponding par titioning of the mean vector ¬µ and of the covariance matrix œÉ in the form ¬µ ¬µa ¬µb ¬µc œÉ œÉaa œÉab œÉac œÉba œÉbb œÉbc œÉca œÉcb œÉcc 2 288 by making use of the results of section 2 3 Ô¨Ånd an expression for the conditional distribution p xa xb in which xc has been marginalized out',\n",
       " 'more generally we can consider a partitioning of the components of x into three groups x a xb and xc with a corresponding par titioning of the mean vector ¬µ and of the covariance matrix œÉ in the form ¬µ ¬µa ¬µb ¬µc œÉ œÉaa œÉab œÉac œÉba œÉbb œÉbc œÉca œÉcb œÉcc 2 288 by making use of the results of section 2 3 Ô¨Ånd an expression for the conditional distribution p xa xb in which xc has been marginalized out 2 26 a very useful result from linear algebra is the woodbury matrix inversion formula given by a bcd 1 a 1 a 1b c 1 da 1b 1da 1 2 289 by multiplying both sides by a bcd prove the correctness of this result 2 27 let x and z be two independent random vectors so that p x z p x p z',\n",
       " '2 289 by multiplying both sides by a bcd prove the correctness of this result 2 27 let x and z be two independent random vectors so that p x z p x p z show that the mean of their sumy x z is given by the sum of the means of each of the variable separately similarly show that the covariance matrix ofy is given by the sum of the covariance matrices of x and z conÔ¨Årm that this result agrees with that of exercise 1 10 2 28 www consider a joint distribution over the variable z x y 2 290 whose mean and covariance are given by 2 108 and 2 105 respectively by mak ing use of the results 2 92 and 2 93 show that the marginal distribution p x is given 2 99',\n",
       " '2 28 www consider a joint distribution over the variable z x y 2 290 whose mean and covariance are given by 2 108 and 2 105 respectively by mak ing use of the results 2 92 and 2 93 show that the marginal distribution p x is given 2 99 similarly by making use of the results 2 81 and 2 82 show that the conditional distribution p y x is given by 2 100 exercises 133 2 29 using the partitioned matrix inversion formula 2 76 show that the inverse of the precision matrix 2 104 is given by the covariance matrix 2 105 2 30 by starting from 2 107 and making use of the result 2 105 verify the result 2 108',\n",
       " 'exercises 133 2 29 using the partitioned matrix inversion formula 2 76 show that the inverse of the precision matrix 2 104 is given by the covariance matrix 2 105 2 30 by starting from 2 107 and making use of the result 2 105 verify the result 2 108 2 31 consider two multidimensional random vectors x and z having gaussian distributions p x n x ¬µx œÉx and p z n z ¬µz œÉz respectively together with their sumy x z use the results 2 109 and 2 110 to Ô¨Ånd an expression for the marginal distribution p y by considering the linear gaussian model comprising the product of the marginal distributionp x and the conditional distributionp y x',\n",
       " '2 31 consider two multidimensional random vectors x and z having gaussian distributions p x n x ¬µx œÉx and p z n z ¬µz œÉz respectively together with their sumy x z use the results 2 109 and 2 110 to Ô¨Ånd an expression for the marginal distribution p y by considering the linear gaussian model comprising the product of the marginal distributionp x and the conditional distributionp y x 2 32 www this exercise and the next provide practice at manipulating the quadratic forms that arise in linear gaussian models as well as giving an indepen dent check of results derived in the main text consider a joint distribution p x y deÔ¨Åned by the marginal and conditional distributions given by 2 99 and 2 100',\n",
       " '2 32 www this exercise and the next provide practice at manipulating the quadratic forms that arise in linear gaussian models as well as giving an indepen dent check of results derived in the main text consider a joint distribution p x y deÔ¨Åned by the marginal and conditional distributions given by 2 99 and 2 100 by examining the quadratic form in the exponent of the joint distribution and using the technique of completing the square discussed in section 2 3 Ô¨Ånd expressions for the mean and covariance of the marginal distribution p y in which the variable x has been integrated out to do this make use of the woodbury matrix inversion formula 2 289 verify that these results agree with 2 109 and 2 110 obtained using the results of chapter 2',\n",
       " 'to do this make use of the woodbury matrix inversion formula 2 289 verify that these results agree with 2 109 and 2 110 obtained using the results of chapter 2 2 33 consider the same joint distribution as in exercise 2 32 but now use the technique of completing the square to Ô¨Ånd expressions for the mean and covariance of the conditional distribution p x y again verify that these agree with the corre sponding expressions 2 111 and 2 112 2 34 www to Ô¨Ånd the maximum likelihood solution for the covariance matrix of a multivariate gaussian we need to maximize the log likelihood function 2 118 with respect to œÉ noting that the covariance matrix must be symmetric and positive deÔ¨Ånite',\n",
       " 'again verify that these agree with the corre sponding expressions 2 111 and 2 112 2 34 www to Ô¨Ånd the maximum likelihood solution for the covariance matrix of a multivariate gaussian we need to maximize the log likelihood function 2 118 with respect to œÉ noting that the covariance matrix must be symmetric and positive deÔ¨Ånite here we proceed by ignoring these constraints and doing a straightforward maximization using the results c 21 c 26 and c 28 from appendix c show that the covariance matrix œÉ that maximizes the log likelihood function 2 118 is given by the sample covariance 2 122 we note that the Ô¨Ånal result is necessarily symmetric and positive deÔ¨Ånite provided the sample covariance is nonsingular 2 35 use the result 2 59 to prove 2 62',\n",
       " 'we note that the Ô¨Ånal result is necessarily symmetric and positive deÔ¨Ånite provided the sample covariance is nonsingular 2 35 use the result 2 59 to prove 2 62 now using the results 2 59 and 2 62 show that e x nxm ¬µ¬µt inmœÇ 2 291 where xn denotes a data point sampled from a gaussian distribution with mean ¬µ and covariance œÉ and inm denotes the n m element of the identity matrix hence prove the result 2 124 2 36 www using an analogous procedure to that used to obtain 2 126 derive an expression for the sequential estimation of the variance of a univariate gaussian 134 2 probability distributions distribution by starting with the maximum likelihood expression œÉ2 ml 1 n n n 1 xn ¬µ 2',\n",
       " '2 36 www using an analogous procedure to that used to obtain 2 126 derive an expression for the sequential estimation of the variance of a univariate gaussian 134 2 probability distributions distribution by starting with the maximum likelihood expression œÉ2 ml 1 n n n 1 xn ¬µ 2 2 292 verify that substituting the expression for a gaussian distribution into the robbins monro sequential estimation formula 2 135 gives a result of the same form and hence obtain an expression for the corresponding coefÔ¨Åcients a n 2 37 using an analogous procedure to that used to obtain 2 126 derive an ex pression for the sequential estimation of the covariance of a multivariate gaussian distribution by starting with the maximum likelihood expression 2 122',\n",
       " '2 292 verify that substituting the expression for a gaussian distribution into the robbins monro sequential estimation formula 2 135 gives a result of the same form and hence obtain an expression for the corresponding coefÔ¨Åcients a n 2 37 using an analogous procedure to that used to obtain 2 126 derive an ex pression for the sequential estimation of the covariance of a multivariate gaussian distribution by starting with the maximum likelihood expression 2 122 verify that substituting the expression for a gaussian distribution into the robbins monro se quential estimation formula 2 135 gives a result of the same form and hence obtain an expression for the corresponding coefÔ¨Åcients a n',\n",
       " '2 37 using an analogous procedure to that used to obtain 2 126 derive an ex pression for the sequential estimation of the covariance of a multivariate gaussian distribution by starting with the maximum likelihood expression 2 122 verify that substituting the expression for a gaussian distribution into the robbins monro se quential estimation formula 2 135 gives a result of the same form and hence obtain an expression for the corresponding coefÔ¨Åcients a n 2 38 use the technique of completing the square for the quadratic form in the expo nent to derive the results 2 141 and 2 142',\n",
       " 'verify that substituting the expression for a gaussian distribution into the robbins monro se quential estimation formula 2 135 gives a result of the same form and hence obtain an expression for the corresponding coefÔ¨Åcients a n 2 38 use the technique of completing the square for the quadratic form in the expo nent to derive the results 2 141 and 2 142 2 39 starting from the results 2 141 and 2 142 for the posterior distribution of the mean of a gaussian random variable dissect out the contributions from the Ô¨Årst n 1 data points and hence obtain expressions for the sequential update of ¬µn and œÉ2 n',\n",
       " '2 38 use the technique of completing the square for the quadratic form in the expo nent to derive the results 2 141 and 2 142 2 39 starting from the results 2 141 and 2 142 for the posterior distribution of the mean of a gaussian random variable dissect out the contributions from the Ô¨Årst n 1 data points and hence obtain expressions for the sequential update of ¬µn and œÉ2 n now derive the same results starting from the posterior distribution p ¬µ x1 x n 1 n ¬µ ¬µn 1 œÉ 2 n 1 and multiplying by the likelihood func tion p xn ¬µ n xn ¬µ œÉ2 and then completing the square and normalizing to obtain the posterior distribution after n observations',\n",
       " '2 39 starting from the results 2 141 and 2 142 for the posterior distribution of the mean of a gaussian random variable dissect out the contributions from the Ô¨Årst n 1 data points and hence obtain expressions for the sequential update of ¬µn and œÉ2 n now derive the same results starting from the posterior distribution p ¬µ x1 x n 1 n ¬µ ¬µn 1 œÉ 2 n 1 and multiplying by the likelihood func tion p xn ¬µ n xn ¬µ œÉ2 and then completing the square and normalizing to obtain the posterior distribution after n observations 2 40 www consider a d dimensional gaussian random variable x with distribu tion n x ¬µ œÉ in which the covariance œÉ is known and for which we wish to infer the mean ¬µ from a set of observationsx x1 xn',\n",
       " 'now derive the same results starting from the posterior distribution p ¬µ x1 x n 1 n ¬µ ¬µn 1 œÉ 2 n 1 and multiplying by the likelihood func tion p xn ¬µ n xn ¬µ œÉ2 and then completing the square and normalizing to obtain the posterior distribution after n observations 2 40 www consider a d dimensional gaussian random variable x with distribu tion n x ¬µ œÉ in which the covariance œÉ is known and for which we wish to infer the mean ¬µ from a set of observationsx x1 xn given a prior distribution p ¬µ n ¬µ ¬µ0 œÉ0 Ô¨Ånd the corresponding posterior distribution p ¬µ x 2 41 use the deÔ¨Ånition of the gamma function 1 141 to show that the gamma dis tribution 2 146 is normalized 2 42 evaluate the mean variance and mode of the gamma distribution 2 146',\n",
       " '2 41 use the deÔ¨Ånition of the gamma function 1 141 to show that the gamma dis tribution 2 146 is normalized 2 42 evaluate the mean variance and mode of the gamma distribution 2 146 2 43 the following distribution p x œÉ2 q q 2 2œÉ2 1 qŒ≥ 1 q exp x q 2œÉ2 2 293 is a generalization of the univariate gaussian distribution show that this distribution is normalized so that p x œÉ2 q d x 1 2 294 and that it reduces to the gaussian when q 2 consider a regression model in which the target variable is given by t y x w œµ and œµ is a random noise exercises 135 variable drawn from the distribution 2 293',\n",
       " 'show that this distribution is normalized so that p x œÉ2 q d x 1 2 294 and that it reduces to the gaussian when q 2 consider a regression model in which the target variable is given by t y x w œµ and œµ is a random noise exercises 135 variable drawn from the distribution 2 293 show that the log likelihood function over w and œÉ2 for an observed data set of input vectors x x1 xn and corresponding target variables t t1 t n t i sg i v e nb y ln p t x w œÉ 2 1 2œÉ2 n n 1 y xn w tn q n q ln 2œÉ2 c o n s t 2 295 where const denotes terms independent of both w and œÉ2 note that as a function of w this is the lq error function considered in section 1 5 5',\n",
       " 'show that the log likelihood function over w and œÉ2 for an observed data set of input vectors x x1 xn and corresponding target variables t t1 t n t i sg i v e nb y ln p t x w œÉ 2 1 2œÉ2 n n 1 y xn w tn q n q ln 2œÉ2 c o n s t 2 295 where const denotes terms independent of both w and œÉ2 note that as a function of w this is the lq error function considered in section 1 5 5 2 44 consider a univariate gaussian distribution n x ¬µ œÑ 1 having conjugate gaussian gamma prior given by 2 154 and a data set x x1 x n of i i d observations show that the posterior distribution is also a gaussian gamma distri bution of the same functional form as the prior and write down expressions for the parameters of this posterior distribution',\n",
       " 'observations show that the posterior distribution is also a gaussian gamma distri bution of the same functional form as the prior and write down expressions for the parameters of this posterior distribution 2 45 verify that the wishart distribution deÔ¨Åned by 2 155 is indeed a conjugate prior for the precision matrix of a multivariate gaussian 2 46 www verify that evaluating the integral in 2 158 leads to the result 2 159 2 47 www show that in the limit ŒΩ the t distribution 2 159 becomes a gaussian hint ignore the normalization coefÔ¨Åcient and simply look at the depen dence on x',\n",
       " '2 47 www show that in the limit ŒΩ the t distribution 2 159 becomes a gaussian hint ignore the normalization coefÔ¨Åcient and simply look at the depen dence on x 2 48 by following analogous steps to those used to derive the univariate student s t distribution 2 159 verify the result 2 162 for the multivariate form of the stu dent s t distribution by marginalizing over the variable Œ∑ in 2 161 using the deÔ¨Ånition 2 161 show by exchanging integration variables that the multivariate t distribution is correctly normalized',\n",
       " '2 48 by following analogous steps to those used to derive the univariate student s t distribution 2 159 verify the result 2 162 for the multivariate form of the stu dent s t distribution by marginalizing over the variable Œ∑ in 2 161 using the deÔ¨Ånition 2 161 show by exchanging integration variables that the multivariate t distribution is correctly normalized 2 49 by using the deÔ¨Ånition 2 161 of the multivariate student s t distribution as a convolution of a gaussian with a gamma distribution verify the properties 2 164 2 165 and 2 166 for the multivariate t distribution deÔ¨Åned by 2 162 2 50 show that in the limit ŒΩ the multivariate student s t distribution 2 162 reduces to a gaussian with mean ¬µ and precision Œª',\n",
       " '2 49 by using the deÔ¨Ånition 2 161 of the multivariate student s t distribution as a convolution of a gaussian with a gamma distribution verify the properties 2 164 2 165 and 2 166 for the multivariate t distribution deÔ¨Åned by 2 162 2 50 show that in the limit ŒΩ the multivariate student s t distribution 2 162 reduces to a gaussian with mean ¬µ and precision Œª 2 51 www the various trigonometric identities used in the discussion of periodic variables in this chapter can be proven easily from the relation exp ia c o sa isina 2 296 in which i is the square root of minus one by considering the identity exp ia exp ia 1 2 297 prove the result 2 177 similarly using the identity cos a b ‚Ñúexp i a b 2 298 136 2',\n",
       " 'by considering the identity exp ia exp ia 1 2 297 prove the result 2 177 similarly using the identity cos a b ‚Ñúexp i a b 2 298 136 2 probability distributions where ‚Ñú denotes the real part prove 2 178 finally by using sin a b ‚Ñëexp i a b where ‚Ñë denotes the imaginary part prove the result 2 183 2 52 for large m the von mises distribution 2 179 becomes sharply peaked around the mode Œ∏0 by deÔ¨Åning Œæ m1 2 Œ∏ Œ∏0 and making the taylor ex pansion of the cosine function given by cos Œ± 1 Œ±2 2 o Œ±4 2 299 show that as m the von mises distribution tends to a gaussian 2 53 using the trigonometric identity 2 183 show that solution of 2 182 for Œ∏0 is given by 2 184',\n",
       " 'by deÔ¨Åning Œæ m1 2 Œ∏ Œ∏0 and making the taylor ex pansion of the cosine function given by cos Œ± 1 Œ±2 2 o Œ±4 2 299 show that as m the von mises distribution tends to a gaussian 2 53 using the trigonometric identity 2 183 show that solution of 2 182 for Œ∏0 is given by 2 184 2 54 by computing Ô¨Årst and second derivatives of the von mises distribution 2 179 and using i0 m 0 for m 0 show that the maximum of the distribution occurs when Œ∏ Œ∏0 and that the minimum occurs when Œ∏ Œ∏0 œÄ mod 2œÄ',\n",
       " '2 53 using the trigonometric identity 2 183 show that solution of 2 182 for Œ∏0 is given by 2 184 2 54 by computing Ô¨Årst and second derivatives of the von mises distribution 2 179 and using i0 m 0 for m 0 show that the maximum of the distribution occurs when Œ∏ Œ∏0 and that the minimum occurs when Œ∏ Œ∏0 œÄ mod 2œÄ 2 55 by making use of the result 2 168 together with 2 184 and the trigonometric identity 2 178 show that the maximum likelihood solutionmml for the concentra tion of the von mises distribution satisÔ¨Åes a mml r where r is the radius of the mean of the observations viewed as unit vectors in the two dimensional euclidean plane as illustrated in figure 2 17',\n",
       " '2 54 by computing Ô¨Årst and second derivatives of the von mises distribution 2 179 and using i0 m 0 for m 0 show that the maximum of the distribution occurs when Œ∏ Œ∏0 and that the minimum occurs when Œ∏ Œ∏0 œÄ mod 2œÄ 2 55 by making use of the result 2 168 together with 2 184 and the trigonometric identity 2 178 show that the maximum likelihood solutionmml for the concentra tion of the von mises distribution satisÔ¨Åes a mml r where r is the radius of the mean of the observations viewed as unit vectors in the two dimensional euclidean plane as illustrated in figure 2 17 2 56 www express the beta distribution 2 13 the gamma distribution 2 146 and the von mises distribution 2 179 as members of the exponential family 2 194 and thereby identify their natural parameters',\n",
       " '2 55 by making use of the result 2 168 together with 2 184 and the trigonometric identity 2 178 show that the maximum likelihood solutionmml for the concentra tion of the von mises distribution satisÔ¨Åes a mml r where r is the radius of the mean of the observations viewed as unit vectors in the two dimensional euclidean plane as illustrated in figure 2 17 2 56 www express the beta distribution 2 13 the gamma distribution 2 146 and the von mises distribution 2 179 as members of the exponential family 2 194 and thereby identify their natural parameters 2 57 verify that the multivariate gaussian distribution can be cast in exponential family form 2 194 and derive expressions forŒ∑ u x h x and g Œ∑ analogous to 2 220 2 223',\n",
       " '2 56 www express the beta distribution 2 13 the gamma distribution 2 146 and the von mises distribution 2 179 as members of the exponential family 2 194 and thereby identify their natural parameters 2 57 verify that the multivariate gaussian distribution can be cast in exponential family form 2 194 and derive expressions forŒ∑ u x h x and g Œ∑ analogous to 2 220 2 223 2 58 the result 2 226 showed that the negative gradient oflng Œ∑ for the exponen tial family is given by the expectation of u x by taking the second derivatives of 2 195 show that lng Œ∑ e u x u x t e u x e u x t c o v u x 2 300 2 59 by changing variables using y x œÉ show that the density 2 236 will be correctly normalized provided f x is correctly normalized',\n",
       " 'by taking the second derivatives of 2 195 show that lng Œ∑ e u x u x t e u x e u x t c o v u x 2 300 2 59 by changing variables using y x œÉ show that the density 2 236 will be correctly normalized provided f x is correctly normalized 2 60 www consider a histogram like density model in which the space x is di vided into Ô¨Åxed regions for which the density p x takes the constant value hi over the ith region and that the volume of region i is denoted i suppose we have a set of n observations of x such that ni of these observations fall in region i using a lagrange multiplier to enforce the normalization constraint on the density derive an expression for the maximum likelihood estimator for the hi',\n",
       " 'suppose we have a set of n observations of x such that ni of these observations fall in region i using a lagrange multiplier to enforce the normalization constraint on the density derive an expression for the maximum likelihood estimator for the hi 2 61 show that the k nearest neighbour density model deÔ¨Ånes an improper distribu tion whose integral over all space is divergent 3 linear models for regression the focus so far in this book has been on unsupervised learning including topics such as density estimation and data clustering we turn now to a discussion of super vised learning starting with regression the goal of regression is to predict the value of one or more continuoustarget variables t given the value of ad dimensional vec tor x of input variables',\n",
       " 'we turn now to a discussion of super vised learning starting with regression the goal of regression is to predict the value of one or more continuoustarget variables t given the value of ad dimensional vec tor x of input variables we have already encountered an example of a regression problem when we considered polynomial curve Ô¨Åtting in chapter 1 the polynomial is a speciÔ¨Åc example of a broad class of functions called linear regression models which share the property of being linear functions of the adjustable parameters and which will form the focus of this chapter the simplest form of linear regression models are also linear functions of the input variables',\n",
       " 'the polynomial is a speciÔ¨Åc example of a broad class of functions called linear regression models which share the property of being linear functions of the adjustable parameters and which will form the focus of this chapter the simplest form of linear regression models are also linear functions of the input variables however we can obtain a much more useful class of functions by taking linear combinations of a Ô¨Åxed set of nonlinear functions of the input variables known as basis functions such models are linear functions of the parameters which gives them simple analytical properties and yet can be nonlinear with respect to the input variables 137 138 3',\n",
       " 'such models are linear functions of the parameters which gives them simple analytical properties and yet can be nonlinear with respect to the input variables 137 138 3 linear models for regression given a training data set comprisingn observations xn where n 1 n together with corresponding target values tn the goal is to predict the value of t for a new value of x in the simplest approach this can be done by directly con structing an appropriate function y x whose values for new inputs x constitute the predictions for the corresponding values of t more generally from a probabilistic perspective we aim to model the predictive distributionp t x because this expresses our uncertainty about the value of t for each value of x',\n",
       " 'linear models for regression given a training data set comprisingn observations xn where n 1 n together with corresponding target values tn the goal is to predict the value of t for a new value of x in the simplest approach this can be done by directly con structing an appropriate function y x whose values for new inputs x constitute the predictions for the corresponding values of t more generally from a probabilistic perspective we aim to model the predictive distributionp t x because this expresses our uncertainty about the value of t for each value of x from this conditional dis tribution we can make predictions of t for any new value of x in such a way as to minimize the expected value of a suitably chosen loss function',\n",
       " 'in the simplest approach this can be done by directly con structing an appropriate function y x whose values for new inputs x constitute the predictions for the corresponding values of t more generally from a probabilistic perspective we aim to model the predictive distributionp t x because this expresses our uncertainty about the value of t for each value of x from this conditional dis tribution we can make predictions of t for any new value of x in such a way as to minimize the expected value of a suitably chosen loss function as discussed in sec tion 1 5 5 a common choice of loss function for real valued variables is the squared loss for which the optimal solution is given by the conditional expectation of t although linear models have signiÔ¨Åcant limitations as practical techniques for pattern recognition particularly for problems involving input spaces of high dimen sionality they have nice analytical properties and form the foundation for more so phisticated models to be discussed in later chapters',\n",
       " 'from this conditional dis tribution we can make predictions of t for any new value of x in such a way as to minimize the expected value of a suitably chosen loss function as discussed in sec tion 1 5 5 a common choice of loss function for real valued variables is the squared loss for which the optimal solution is given by the conditional expectation of t although linear models have signiÔ¨Åcant limitations as practical techniques for pattern recognition particularly for problems involving input spaces of high dimen sionality they have nice analytical properties and form the foundation for more so phisticated models to be discussed in later chapters 3 1',\n",
       " 'as discussed in sec tion 1 5 5 a common choice of loss function for real valued variables is the squared loss for which the optimal solution is given by the conditional expectation of t although linear models have signiÔ¨Åcant limitations as practical techniques for pattern recognition particularly for problems involving input spaces of high dimen sionality they have nice analytical properties and form the foundation for more so phisticated models to be discussed in later chapters 3 1 linear basis function models the simplest linear model for regression is one that involves a linear combination of the input variables y x w w 0 w1x1 wdxd 3 1 where x x1 x d t this is often simply known aslinear regression',\n",
       " '3 1 linear basis function models the simplest linear model for regression is one that involves a linear combination of the input variables y x w w 0 w1x1 wdxd 3 1 where x x1 x d t this is often simply known aslinear regression the key property of this model is that it is a linear function of the parametersw0 w d i ti s also however a linear function of the input variablesxi and this imposes signiÔ¨Åcant limitations on the model we therefore extend the class of models by considering linear combinations of Ô¨Åxed nonlinear functions of the input variables of the form y x w w0 m 1 j 1 wjœÜj x 3 2 where œÜj x are known as basis functions',\n",
       " 'the key property of this model is that it is a linear function of the parametersw0 w d i ti s also however a linear function of the input variablesxi and this imposes signiÔ¨Åcant limitations on the model we therefore extend the class of models by considering linear combinations of Ô¨Åxed nonlinear functions of the input variables of the form y x w w0 m 1 j 1 wjœÜj x 3 2 where œÜj x are known as basis functions by denoting the maximum value of the index j by m 1 the total number of parameters in this model will be m the parameter w0 allows for any Ô¨Åxed offset in the data and is sometimes called a bias parameter not to be confused with bias in a statistical sense',\n",
       " 'we therefore extend the class of models by considering linear combinations of Ô¨Åxed nonlinear functions of the input variables of the form y x w w0 m 1 j 1 wjœÜj x 3 2 where œÜj x are known as basis functions by denoting the maximum value of the index j by m 1 the total number of parameters in this model will be m the parameter w0 allows for any Ô¨Åxed offset in the data and is sometimes called a bias parameter not to be confused with bias in a statistical sense it is often convenient to deÔ¨Åne an additional dummy basis function œÜ0 x 1 so that y x w m 1 j 0 wjœÜj x wtœÜ x 3 3 where w w0 w m 1 t and œÜ œÜ0 œÜ m 1 t in many practical ap plications of pattern recognition we will apply some form of Ô¨Åxed pre processing 3 1',\n",
       " 'by denoting the maximum value of the index j by m 1 the total number of parameters in this model will be m the parameter w0 allows for any Ô¨Åxed offset in the data and is sometimes called a bias parameter not to be confused with bias in a statistical sense it is often convenient to deÔ¨Åne an additional dummy basis function œÜ0 x 1 so that y x w m 1 j 0 wjœÜj x wtœÜ x 3 3 where w w0 w m 1 t and œÜ œÜ0 œÜ m 1 t in many practical ap plications of pattern recognition we will apply some form of Ô¨Åxed pre processing 3 1 linear basis function models 139 or feature extraction to the original data variables if the original variables com prise the vector x then the features can be expressed in terms of the basis functions œÜj x',\n",
       " 'linear basis function models 139 or feature extraction to the original data variables if the original variables com prise the vector x then the features can be expressed in terms of the basis functions œÜj x by using nonlinear basis functions we allow the function y x w to be a non linear function of the input vector x functions of the form 3 2 are called linear models however because this function is linear in w it is this linearity in the pa rameters that will greatly simplify the analysis of this class of models however it also leads to some signiÔ¨Åcant limitations as we discuss in section 3 6',\n",
       " 'functions of the form 3 2 are called linear models however because this function is linear in w it is this linearity in the pa rameters that will greatly simplify the analysis of this class of models however it also leads to some signiÔ¨Åcant limitations as we discuss in section 3 6 the example of polynomial regression considered in chapter 1 is a particular example of this model in which there is a single input variablex and the basis func tions take the form of powers of x so that œÜj x xj one limitation of polynomial basis functions is that they are global functions of the input variable so that changes in one region of input space affect all other regions',\n",
       " 'the example of polynomial regression considered in chapter 1 is a particular example of this model in which there is a single input variablex and the basis func tions take the form of powers of x so that œÜj x xj one limitation of polynomial basis functions is that they are global functions of the input variable so that changes in one region of input space affect all other regions this can be resolved by dividing the input space up into regions and Ô¨Åt a different polynomial in each region leading to spline functions hastie et al 2001 there are many other possible choices for the basis functions for example œÜ j x e x p x ¬µj 2 2s2 3 4 where the ¬µj govern the locations of the basis functions in input space and the pa rameter s governs their spatial scale',\n",
       " 'this can be resolved by dividing the input space up into regions and Ô¨Åt a different polynomial in each region leading to spline functions hastie et al 2001 there are many other possible choices for the basis functions for example œÜ j x e x p x ¬µj 2 2s2 3 4 where the ¬µj govern the locations of the basis functions in input space and the pa rameter s governs their spatial scale these are usually referred to as gaussian basis functions although it should be noted that they are not required to have a prob abilistic interpretation and in particular the normalization coefÔ¨Åcient is unimportant because these basis functions will be multiplied by adaptive parameters w j',\n",
       " 'there are many other possible choices for the basis functions for example œÜ j x e x p x ¬µj 2 2s2 3 4 where the ¬µj govern the locations of the basis functions in input space and the pa rameter s governs their spatial scale these are usually referred to as gaussian basis functions although it should be noted that they are not required to have a prob abilistic interpretation and in particular the normalization coefÔ¨Åcient is unimportant because these basis functions will be multiplied by adaptive parameters w j another possibility is the sigmoidal basis function of the form œÜj x œÉ x ¬µj s 3 5 where œÉ a is the logistic sigmoid function deÔ¨Åned by œÉ a 1 1 e x p a',\n",
       " 'these are usually referred to as gaussian basis functions although it should be noted that they are not required to have a prob abilistic interpretation and in particular the normalization coefÔ¨Åcient is unimportant because these basis functions will be multiplied by adaptive parameters w j another possibility is the sigmoidal basis function of the form œÜj x œÉ x ¬µj s 3 5 where œÉ a is the logistic sigmoid function deÔ¨Åned by œÉ a 1 1 e x p a 3 6 equivalently we can use the tanh function because this is related to the logistic sigmoid by tanh a 2 œÉ a 1 and so a general linear combination of logistic sigmoid functions is equivalent to a general linear combination of tanh functions these various choices of basis function are illustrated in figure 3 1',\n",
       " '3 6 equivalently we can use the tanh function because this is related to the logistic sigmoid by tanh a 2 œÉ a 1 and so a general linear combination of logistic sigmoid functions is equivalent to a general linear combination of tanh functions these various choices of basis function are illustrated in figure 3 1 yet another possible choice of basis function is the fourier basis which leads to an expansion in sinusoidal functions each basis function represents a speciÔ¨Åc fre quency and has inÔ¨Ånite spatial extent by contrast basis functions that are localized to Ô¨Ånite regions of input space necessarily comprise a spectrum of different spatial frequencies',\n",
       " 'each basis function represents a speciÔ¨Åc fre quency and has inÔ¨Ånite spatial extent by contrast basis functions that are localized to Ô¨Ånite regions of input space necessarily comprise a spectrum of different spatial frequencies in many signal processing applications it is of interest to consider ba sis functions that are localized in both space and frequency leading to a class of functions known as wavelets these are also deÔ¨Åned to be mutually orthogonal to simplify their application wavelets are most applicable when the input values live 140 3',\n",
       " 'these are also deÔ¨Åned to be mutually orthogonal to simplify their application wavelets are most applicable when the input values live 140 3 linear models for regression 1 0 1 1 0 5 0 0 5 1 1 0 1 0 0 25 0 5 0 75 1 1 0 1 0 0 25 0 5 0 75 1 figure 3 1 examples of basis functions showing polynomials on the left gaussians of the form 3 4 in the centre and sigmoidal of the form 3 5 on the right on a regular lattice such as the successive time points in a temporal sequence or the pixels in an image useful texts on wavelets include ogden 1997 mallat 1999 and vidakovic 1999',\n",
       " 'on a regular lattice such as the successive time points in a temporal sequence or the pixels in an image useful texts on wavelets include ogden 1997 mallat 1999 and vidakovic 1999 most of the discussion in this chapter however is independent of the particular choice of basis function set and so for most of our discussion we shall not specify the particular form of the basis functions except for the purposes of numerical il lustration',\n",
       " 'useful texts on wavelets include ogden 1997 mallat 1999 and vidakovic 1999 most of the discussion in this chapter however is independent of the particular choice of basis function set and so for most of our discussion we shall not specify the particular form of the basis functions except for the purposes of numerical il lustration indeed much of our discussion will be equally applicable to the situation in which the vector œÜ x of basis functions is simply the identity œÜ x x fur thermore in order to keep the notation simple we shall focus on the case of a single target variable t however in section 3 1 5 we consider brieÔ¨Çy the modiÔ¨Åcations needed to deal with multiple target variables',\n",
       " 'most of the discussion in this chapter however is independent of the particular choice of basis function set and so for most of our discussion we shall not specify the particular form of the basis functions except for the purposes of numerical il lustration indeed much of our discussion will be equally applicable to the situation in which the vector œÜ x of basis functions is simply the identity œÜ x x fur thermore in order to keep the notation simple we shall focus on the case of a single target variable t however in section 3 1 5 we consider brieÔ¨Çy the modiÔ¨Åcations needed to deal with multiple target variables 3 1 1 maximum likelihood and least squares in chapter 1 we Ô¨Åtted polynomial functions to data sets by minimizing a sum of squares error function',\n",
       " 'indeed much of our discussion will be equally applicable to the situation in which the vector œÜ x of basis functions is simply the identity œÜ x x fur thermore in order to keep the notation simple we shall focus on the case of a single target variable t however in section 3 1 5 we consider brieÔ¨Çy the modiÔ¨Åcations needed to deal with multiple target variables 3 1 1 maximum likelihood and least squares in chapter 1 we Ô¨Åtted polynomial functions to data sets by minimizing a sum of squares error function we also showed that this error function could be motivated as the maximum likelihood solution under an assumed gaussian noise model let us return to this discussion and consider the least squares approach and its relation to maximum likelihood in more detail',\n",
       " 'we also showed that this error function could be motivated as the maximum likelihood solution under an assumed gaussian noise model let us return to this discussion and consider the least squares approach and its relation to maximum likelihood in more detail as before we assume that the target variable t is given by a deterministic func tion y x w with additive gaussian noise so that t y x w œµ 3 7 where œµ is a zero mean gaussian random variable with precision inverse variance Œ≤ thus we can write p t x w Œ≤ n t y x w Œ≤ 1 3 8 recall that if we assume a squared loss function then the optimal prediction for a new value of x will be given by the conditional mean of the target variable',\n",
       " 'thus we can write p t x w Œ≤ n t y x w Œ≤ 1 3 8 recall that if we assume a squared loss function then the optimal prediction for a new value of x will be given by the conditional mean of the target variable in thesection 1 5 5 case of a gaussian conditional distribution of the form 3 8 the conditional mean 3 1 linear basis function models 141 will be simply e t x tp t x d t y x w 3 9 note that the gaussian noise assumption implies that the conditional distribution of t given x is unimodal which may be inappropriate for some applications an ex tension to mixtures of conditional gaussian distributions which permit multimodal conditional distributions will be discussed in section 14 5 1',\n",
       " '3 9 note that the gaussian noise assumption implies that the conditional distribution of t given x is unimodal which may be inappropriate for some applications an ex tension to mixtures of conditional gaussian distributions which permit multimodal conditional distributions will be discussed in section 14 5 1 now consider a data set of inputs x x 1 xn with corresponding target values t1 t n',\n",
       " 'an ex tension to mixtures of conditional gaussian distributions which permit multimodal conditional distributions will be discussed in section 14 5 1 now consider a data set of inputs x x 1 xn with corresponding target values t1 t n we group the target variables tn into a column vector that we denote by t where the typeface is chosen to distinguish it from a single observation of a multivariate target which would be denoted t making the assumption that these data points are drawn independently from the distribution 3 8 we obtain the following expression for the likelihood function which is a function of the adjustable parameters w and Œ≤ in the form p t x w Œ≤ n n 1 n tn wtœÜ xn Œ≤ 1 3 10 where we have used 3 3',\n",
       " 'now consider a data set of inputs x x 1 xn with corresponding target values t1 t n we group the target variables tn into a column vector that we denote by t where the typeface is chosen to distinguish it from a single observation of a multivariate target which would be denoted t making the assumption that these data points are drawn independently from the distribution 3 8 we obtain the following expression for the likelihood function which is a function of the adjustable parameters w and Œ≤ in the form p t x w Œ≤ n n 1 n tn wtœÜ xn Œ≤ 1 3 10 where we have used 3 3 note that in supervised learning problems such as regres sion and classiÔ¨Åcation we are not seeking to model the distribution of the input variables',\n",
       " 'we group the target variables tn into a column vector that we denote by t where the typeface is chosen to distinguish it from a single observation of a multivariate target which would be denoted t making the assumption that these data points are drawn independently from the distribution 3 8 we obtain the following expression for the likelihood function which is a function of the adjustable parameters w and Œ≤ in the form p t x w Œ≤ n n 1 n tn wtœÜ xn Œ≤ 1 3 10 where we have used 3 3 note that in supervised learning problems such as regres sion and classiÔ¨Åcation we are not seeking to model the distribution of the input variables thus x will always appear in the set of conditioning variables and so from now on we will drop the explicitx from expressions such as p t x w Œ≤ in or der to keep the notation uncluttered',\n",
       " 'note that in supervised learning problems such as regres sion and classiÔ¨Åcation we are not seeking to model the distribution of the input variables thus x will always appear in the set of conditioning variables and so from now on we will drop the explicitx from expressions such as p t x w Œ≤ in or der to keep the notation uncluttered taking the logarithm of the likelihood function and making use of the standard form 1 46 for the univariate gaussian we have lnp t w Œ≤ n n 1 lnn tn wtœÜ xn Œ≤ 1 n 2 ln Œ≤ n 2 ln 2œÄ Œ≤ed w 3 11 where the sum of squares error function is deÔ¨Åned by ed w 1 2 n n 1 tn wtœÜ xn 2 3 12 having written down the likelihood function we can use maximum likelihood to determine w and Œ≤',\n",
       " 'taking the logarithm of the likelihood function and making use of the standard form 1 46 for the univariate gaussian we have lnp t w Œ≤ n n 1 lnn tn wtœÜ xn Œ≤ 1 n 2 ln Œ≤ n 2 ln 2œÄ Œ≤ed w 3 11 where the sum of squares error function is deÔ¨Åned by ed w 1 2 n n 1 tn wtœÜ xn 2 3 12 having written down the likelihood function we can use maximum likelihood to determine w and Œ≤ consider Ô¨Årst the maximization with respect to w as observed already in section 1 2 5 we see that maximization of the likelihood function under a conditional gaussian noise distribution for a linear model is equivalent to minimizing a sum of squares error function given by ed w',\n",
       " '3 12 having written down the likelihood function we can use maximum likelihood to determine w and Œ≤ consider Ô¨Årst the maximization with respect to w as observed already in section 1 2 5 we see that maximization of the likelihood function under a conditional gaussian noise distribution for a linear model is equivalent to minimizing a sum of squares error function given by ed w the gradient of the log likelihood function 3 11 takes the form lnp t w Œ≤ n n 1 tn wtœÜ xn œÜ xn t 3 13 142 3 linear models for regression setting this gradient to zero gives 0 n n 1 tnœÜ xn t wt n n 1 œÜ xn œÜ xn t 3 14 solving for w we obtain wml œÜtœÜ 1 œÜtt 3 15 which are known as thenormal equationsfor the least squares problem',\n",
       " 'linear models for regression setting this gradient to zero gives 0 n n 1 tnœÜ xn t wt n n 1 œÜ xn œÜ xn t 3 14 solving for w we obtain wml œÜtœÜ 1 œÜtt 3 15 which are known as thenormal equationsfor the least squares problem hereœÜ is an n m matrix called thedesign matrix whose elements are given byœÜnj œÜj xn so that œÜ œÜ0 x1 œÜ1 x1 œÜm 1 x1 œÜ0 x2 œÜ1 x2 œÜm 1 x2 œÜ0 xn œÜ1 xn œÜm 1 xn 3 16 the quantity œÜ œÜtœÜ 1 œÜt 3 17 is known as the moore penrose pseudo inverse of the matrix œÜ rao and mitra 1971 golub and van loan 1996 it can be regarded as a generalization of the notion of matrix inverse to nonsquare matrices indeed ifœÜ is square and invertible then using the property ab 1 b 1a 1 we see that œÜ œÜ 1',\n",
       " 'it can be regarded as a generalization of the notion of matrix inverse to nonsquare matrices indeed ifœÜ is square and invertible then using the property ab 1 b 1a 1 we see that œÜ œÜ 1 at this point we can gain some insight into the role of the bias parameterw0 i f we make the bias parameter explicit then the error function 3 12 becomes ed w 1 2 n n 1 tn w0 m 1 j 1 wjœÜj xn 2 3 18 setting the derivative with respect tow0 equal to zero and solving for w0 we obtain w0 t m 1 j 1 wjœÜj 3 19 where we have deÔ¨Åned t 1 n n n 1 tn œÜj 1 n n n 1 œÜj xn 3 20 thus the bias w0 compensates for the difference between the averages over the training set of the target values and the weighted sum of the averages of the basis function values',\n",
       " '3 18 setting the derivative with respect tow0 equal to zero and solving for w0 we obtain w0 t m 1 j 1 wjœÜj 3 19 where we have deÔ¨Åned t 1 n n n 1 tn œÜj 1 n n n 1 œÜj xn 3 20 thus the bias w0 compensates for the difference between the averages over the training set of the target values and the weighted sum of the averages of the basis function values we can also maximize the log likelihood function 3 11 with respect to the noise precision parameter Œ≤ giving 1 Œ≤ml 1 n n n 1 tn wt mlœÜ xn 2 3 21 3 1 linear basis function models 143 figure 3 2 geometrical interpretation of the least squares solution in ann dimensional space whose axes are the values of t1 t n',\n",
       " 'we can also maximize the log likelihood function 3 11 with respect to the noise precision parameter Œ≤ giving 1 Œ≤ml 1 n n n 1 tn wt mlœÜ xn 2 3 21 3 1 linear basis function models 143 figure 3 2 geometrical interpretation of the least squares solution in ann dimensional space whose axes are the values of t1 t n the least squares regression function is obtained by Ô¨Ånding the or thogonal projection of the data vector t onto the subspace spanned by the basis functions œÜj x in which each basis function is viewed as a vec tor œïj of length n with elements œÜj xn s t yœï1 œï2 and so we see that the inverse of the noise precision is given by the residual variance of the target values around the regression function',\n",
       " 'the least squares regression function is obtained by Ô¨Ånding the or thogonal projection of the data vector t onto the subspace spanned by the basis functions œÜj x in which each basis function is viewed as a vec tor œïj of length n with elements œÜj xn s t yœï1 œï2 and so we see that the inverse of the noise precision is given by the residual variance of the target values around the regression function 3 1 2 geometry of least squares at this point it is instructive to consider the geometrical interpretation of the least squares solution to do this we consider an n dimensional space whose axes are given by the tn so that t t1 t n t is a vector in this space',\n",
       " '3 1 2 geometry of least squares at this point it is instructive to consider the geometrical interpretation of the least squares solution to do this we consider an n dimensional space whose axes are given by the tn so that t t1 t n t is a vector in this space each basis function œÜj xn evaluated at then data points can also be represented as a vector in the same space denoted byœïj as illustrated in figure 3 2 note thatœïj corresponds to the jth column of œÜ whereas œÜ xn corresponds to the nth row of œÜ if the number m of basis functions is smaller than the number n of data points then the m vectors œÜj xn will span a linear subspace s of dimensionality m we deÔ¨Åne y to be an n dimensional vector whose nth element is given by y xn w where n 1 n',\n",
       " 'note thatœïj corresponds to the jth column of œÜ whereas œÜ xn corresponds to the nth row of œÜ if the number m of basis functions is smaller than the number n of data points then the m vectors œÜj xn will span a linear subspace s of dimensionality m we deÔ¨Åne y to be an n dimensional vector whose nth element is given by y xn w where n 1 n because y is an arbitrary linear combination of the vectors œïj it can live anywhere in the m dimensional subspace',\n",
       " 'if the number m of basis functions is smaller than the number n of data points then the m vectors œÜj xn will span a linear subspace s of dimensionality m we deÔ¨Åne y to be an n dimensional vector whose nth element is given by y xn w where n 1 n because y is an arbitrary linear combination of the vectors œïj it can live anywhere in the m dimensional subspace the sum of squares error 3 12 is then equal up to a factor of 1 2 to the squared euclidean distance between y and t thus the least squares solution for w corresponds to that choice of y that lies in subspace s and that is closest to t intuitively from figure 3 2 we anticipate that this solution corresponds to the orthogonal projection of t onto the subspace s this is indeed the case as can easily be veriÔ¨Åed by noting that the solution for y is given by œÜwml and then conÔ¨Årming that this takes the form of an orthogonal projection exercise 3 2 in practice a direct solution of the normal equations can lead to numerical difÔ¨Å culties when œÜtœÜ is close to singular',\n",
       " 'because y is an arbitrary linear combination of the vectors œïj it can live anywhere in the m dimensional subspace the sum of squares error 3 12 is then equal up to a factor of 1 2 to the squared euclidean distance between y and t thus the least squares solution for w corresponds to that choice of y that lies in subspace s and that is closest to t intuitively from figure 3 2 we anticipate that this solution corresponds to the orthogonal projection of t onto the subspace s this is indeed the case as can easily be veriÔ¨Åed by noting that the solution for y is given by œÜwml and then conÔ¨Årming that this takes the form of an orthogonal projection exercise 3 2 in practice a direct solution of the normal equations can lead to numerical difÔ¨Å culties when œÜtœÜ is close to singular in particular when two or more of the basis vectors œïj are co linear or nearly so the resulting parameter values can have large magnitudes',\n",
       " 'the sum of squares error 3 12 is then equal up to a factor of 1 2 to the squared euclidean distance between y and t thus the least squares solution for w corresponds to that choice of y that lies in subspace s and that is closest to t intuitively from figure 3 2 we anticipate that this solution corresponds to the orthogonal projection of t onto the subspace s this is indeed the case as can easily be veriÔ¨Åed by noting that the solution for y is given by œÜwml and then conÔ¨Årming that this takes the form of an orthogonal projection exercise 3 2 in practice a direct solution of the normal equations can lead to numerical difÔ¨Å culties when œÜtœÜ is close to singular in particular when two or more of the basis vectors œïj are co linear or nearly so the resulting parameter values can have large magnitudes such near degeneracies will not be uncommon when dealing with real data sets',\n",
       " 'in particular when two or more of the basis vectors œïj are co linear or nearly so the resulting parameter values can have large magnitudes such near degeneracies will not be uncommon when dealing with real data sets the resulting numerical difÔ¨Åculties can be addressed using the technique of singular value decomposition o r svd press et al 1992 bishop and nabney 2008 note that the addition of a regularization term ensures that the matrix is non singular even in the presence of degeneracies 3 1 3 sequential learning batch techniques such as the maximum likelihood solution 3 15 which in volve processing the entire training set in one go can be computationally costly for large data sets',\n",
       " 'note that the addition of a regularization term ensures that the matrix is non singular even in the presence of degeneracies 3 1 3 sequential learning batch techniques such as the maximum likelihood solution 3 15 which in volve processing the entire training set in one go can be computationally costly for large data sets as we have discussed in chapter 1 if the data set is sufÔ¨Åciently large it may be worthwhile to usesequential algorithms also known ason line algorithms 144 3 linear models for regression in which the data points are considered one at a time and the model parameters up dated after each such presentation',\n",
       " 'as we have discussed in chapter 1 if the data set is sufÔ¨Åciently large it may be worthwhile to usesequential algorithms also known ason line algorithms 144 3 linear models for regression in which the data points are considered one at a time and the model parameters up dated after each such presentation sequential learning is also appropriate for real time applications in which the data observations are arriving in a continuous stream and predictions must be made before all of the data points are seen we can obtain a sequential learning algorithm by applying the technique of stochastic gradient descent also known assequential gradient descent as follows',\n",
       " 'sequential learning is also appropriate for real time applications in which the data observations are arriving in a continuous stream and predictions must be made before all of the data points are seen we can obtain a sequential learning algorithm by applying the technique of stochastic gradient descent also known assequential gradient descent as follows if the error function comprises a sum over data points e n en then after presen tation of pattern n the stochastic gradient descent algorithm updates the parameter vector w using w œÑ 1 w œÑ Œ∑ en 3 22 where œÑ denotes the iteration number and Œ∑ is a learning rate parameter we shall discuss the choice of value forŒ∑ shortly the value ofw is initialized to some starting vector w 0',\n",
       " 'we shall discuss the choice of value forŒ∑ shortly the value ofw is initialized to some starting vector w 0 for the case of the sum of squares error function 3 12 this gives w œÑ 1 w œÑ Œ∑ tn w œÑ tœÜn œÜn 3 23 where œÜn œÜ xn this is known as least mean squares or the lms algorithm the value of Œ∑ needs to be chosen with care to ensure that the algorithm converges bishop and nabney 2008 3 1 4 regularized least squares in section 1 1 we introduced the idea of adding a regularization term to an error function in order to control over Ô¨Åtting so that the total error function to be minimized takes the form ed w Œªew w 3 24 where Œª is the regularization coefÔ¨Åcient that controls the relative importance of the data dependent error ed w and the regularization term ew w',\n",
       " 'the value of Œ∑ needs to be chosen with care to ensure that the algorithm converges bishop and nabney 2008 3 1 4 regularized least squares in section 1 1 we introduced the idea of adding a regularization term to an error function in order to control over Ô¨Åtting so that the total error function to be minimized takes the form ed w Œªew w 3 24 where Œª is the regularization coefÔ¨Åcient that controls the relative importance of the data dependent error ed w and the regularization term ew w one of the sim plest forms of regularizer is given by the sum of squares of the weight vector ele ments ew w 1 2wtw',\n",
       " '3 1 4 regularized least squares in section 1 1 we introduced the idea of adding a regularization term to an error function in order to control over Ô¨Åtting so that the total error function to be minimized takes the form ed w Œªew w 3 24 where Œª is the regularization coefÔ¨Åcient that controls the relative importance of the data dependent error ed w and the regularization term ew w one of the sim plest forms of regularizer is given by the sum of squares of the weight vector ele ments ew w 1 2wtw 3 25 if we also consider the sum of squares error function given by e w 1 2 n n 1 tn wtœÜ xn 2 3 26 then the total error function becomes 1 2 n n 1 tn wtœÜ xn 2 Œª 2wtw',\n",
       " 'one of the sim plest forms of regularizer is given by the sum of squares of the weight vector ele ments ew w 1 2wtw 3 25 if we also consider the sum of squares error function given by e w 1 2 n n 1 tn wtœÜ xn 2 3 26 then the total error function becomes 1 2 n n 1 tn wtœÜ xn 2 Œª 2wtw 3 27 this particular choice of regularizer is known in the machine learning literature as weight decaybecause in sequential learning algorithms it encourages weight values to decay towards zero unless supported by the data in statistics it provides an ex ample of a parameter shrinkagemethod because it shrinks parameter values towards 3 1',\n",
       " '3 27 this particular choice of regularizer is known in the machine learning literature as weight decaybecause in sequential learning algorithms it encourages weight values to decay towards zero unless supported by the data in statistics it provides an ex ample of a parameter shrinkagemethod because it shrinks parameter values towards 3 1 linear basis function models 145 q 0 5 q 1 q 2 q 4 figure 3 3 contours of the regularization term in 3 29 for various values of the parameter q zero it has the advantage that the error function remains a quadratic function of w and so its exact minimizer can be found in closed form speciÔ¨Åcally setting the gradient of 3 27 with respect to w to zero and solving for w as before we obtain w Œªi œÜtœÜ 1 œÜtt',\n",
       " 'it has the advantage that the error function remains a quadratic function of w and so its exact minimizer can be found in closed form speciÔ¨Åcally setting the gradient of 3 27 with respect to w to zero and solving for w as before we obtain w Œªi œÜtœÜ 1 œÜtt 3 28 this represents a simple extension of the least squares solution 3 15 a more general regularizer is sometimes used for which the regularized error takes the form 1 2 n n 1 tn wtœÜ xn 2 Œª 2 m j 1 wj q 3 29 where q 2 corresponds to the quadratic regularizer 3 27 figure 3 3 shows con tours of the regularization function for different values of q the case of q 1 is know as the lasso in the statistics literature tibshirani 1996',\n",
       " 'figure 3 3 shows con tours of the regularization function for different values of q the case of q 1 is know as the lasso in the statistics literature tibshirani 1996 it has the property that if Œª is sufÔ¨Åciently large some of the coefÔ¨Åcients wj are driven to zero leading to a sparse model in which the corresponding basis functions play no role to see this we Ô¨Årst note that minimizing 3 29 is equivalent to minimizing the unregularized sum of squares error 3 12 subject to the constraintexercise 3 5 m j 1 wj q Œ∑ 3 30 for an appropriate value of the parameterŒ∑ where the two approaches can be related using lagrange multipliers the origin of the sparsity can be seen from figure 3 4 appendix e which shows that the minimum of the error function subject to the constraint 3 30',\n",
       " 'to see this we Ô¨Årst note that minimizing 3 29 is equivalent to minimizing the unregularized sum of squares error 3 12 subject to the constraintexercise 3 5 m j 1 wj q Œ∑ 3 30 for an appropriate value of the parameterŒ∑ where the two approaches can be related using lagrange multipliers the origin of the sparsity can be seen from figure 3 4 appendix e which shows that the minimum of the error function subject to the constraint 3 30 as Œª is increased so an increasing number of parameters are driven to zero regularization allows complex models to be trained on data sets of limited size without severe over Ô¨Åtting essentially by limiting the effective model complexity',\n",
       " 'as Œª is increased so an increasing number of parameters are driven to zero regularization allows complex models to be trained on data sets of limited size without severe over Ô¨Åtting essentially by limiting the effective model complexity however the problem of determining the optimal model complexity is then shifted from one of Ô¨Ånding the appropriate number of basis functions to one of determining a suitable value of the regularization coefÔ¨Åcient Œª we shall return to the issue of model complexity later in this chapter 146 3',\n",
       " 'we shall return to the issue of model complexity later in this chapter 146 3 linear models for regression figure 3 4 plot of the contours of the unregularized error function blue along with the constraint re gion 3 30 for the quadratic regular izer q 2 on the left and the lasso regularizer q 1 on the right in which the optimum value for the pa rameter vector w is denoted by w the lasso gives a sparse solution in which w 1 0 w1 w2 w w1 w2 w for the remainder of this chapter we shall focus on the quadratic regularizer 3 27 both for its practical importance and its analytical tractability 3 1 5 multiple outputs so far we have considered the case of a single target variablet',\n",
       " 'w1 w2 w w1 w2 w for the remainder of this chapter we shall focus on the quadratic regularizer 3 27 both for its practical importance and its analytical tractability 3 1 5 multiple outputs so far we have considered the case of a single target variablet in some applica tions we may wish to predict k 1 target variables which we denote collectively by the target vectort this could be done by introducing a different set of basis func tions for each component oft leading to multiple independent regression problems',\n",
       " 'in some applica tions we may wish to predict k 1 target variables which we denote collectively by the target vectort this could be done by introducing a different set of basis func tions for each component oft leading to multiple independent regression problems however a more interesting and more common approach is to use the same set of basis functions to model all of the components of the target vector so that y x w wtœÜ x 3 31 where y is a k dimensional column vector w is an m k matrix of parameters and œÜ x is an m dimensional column vector with elements œÜj x with œÜ0 x 1 as before suppose we take the conditional distribution of the target vector to be an isotropic gaussian of the form p t x w Œ≤ n t wtœÜ x Œ≤ 1i',\n",
       " 'however a more interesting and more common approach is to use the same set of basis functions to model all of the components of the target vector so that y x w wtœÜ x 3 31 where y is a k dimensional column vector w is an m k matrix of parameters and œÜ x is an m dimensional column vector with elements œÜj x with œÜ0 x 1 as before suppose we take the conditional distribution of the target vector to be an isotropic gaussian of the form p t x w Œ≤ n t wtœÜ x Œ≤ 1i 3 32 if we have a set of observations t1 tn we can combine these into a matrix t of size n k such that the nth row is given by tt n similarly we can combine the input vectors x1 xn into a matrix x',\n",
       " 'suppose we take the conditional distribution of the target vector to be an isotropic gaussian of the form p t x w Œ≤ n t wtœÜ x Œ≤ 1i 3 32 if we have a set of observations t1 tn we can combine these into a matrix t of size n k such that the nth row is given by tt n similarly we can combine the input vectors x1 xn into a matrix x the log likelihood function is then given by lnp t x w Œ≤ n n 1 lnn tn wtœÜ xn Œ≤ 1i nk 2 ln Œ≤ 2œÄ Œ≤ 2 n n 1 tn wtœÜ xn 2 3 33 3 2 the bias variance decomposition 147 as before we can maximize this function with respect to w giving wml œÜtœÜ 1 œÜtt',\n",
       " '3 33 3 2 the bias variance decomposition 147 as before we can maximize this function with respect to w giving wml œÜtœÜ 1 œÜtt 3 34 if we examine this result for each target variable tk w eh a v e wk œÜtœÜ 1 œÜttk œÜ tk 3 35 where tk is an n dimensional column vector with components tnk for n 1 n thus the solution to the regression problem decouples between the different target variables and we need only compute a single pseudo inverse matrix œÜ which is shared by all of the vectors wk the extension to general gaussian noise distributions having arbitrary covari ance matrices is straightforward again this leads to a decoupling into k inde exercise 3 6 pendent regression problems',\n",
       " 'the extension to general gaussian noise distributions having arbitrary covari ance matrices is straightforward again this leads to a decoupling into k inde exercise 3 6 pendent regression problems this result is unsurprising because the parameters w deÔ¨Åne only the mean of the gaussian noise distribution and we know from sec tion 2 3 4 that the maximum likelihood solution for the mean of a multivariate gaus sian is independent of the covariance from now on we shall therefore consider a single target variable t for simplicity 3 2 the bias variance decomposition so far in our discussion of linear models for regression we have assumed that the form and number of basis functions are both Ô¨Åxed',\n",
       " '3 2 the bias variance decomposition so far in our discussion of linear models for regression we have assumed that the form and number of basis functions are both Ô¨Åxed as we have seen in chapter 1 the use of maximum likelihood or equivalently least squares can lead to severe over Ô¨Åtting if complex models are trained using data sets of limited size however limiting the number of basis functions in order to avoid over Ô¨Åtting has the side effect of limiting the Ô¨Çexibility of the model to capture interesting and important trends in the data although the introduction of regularization terms can control over Ô¨Åtting for models with many parameters this raises the question of how to determine a suitable value for the regularization coefÔ¨Åcient Œª',\n",
       " 'however limiting the number of basis functions in order to avoid over Ô¨Åtting has the side effect of limiting the Ô¨Çexibility of the model to capture interesting and important trends in the data although the introduction of regularization terms can control over Ô¨Åtting for models with many parameters this raises the question of how to determine a suitable value for the regularization coefÔ¨Åcient Œª seeking the solution that minimizes the regularized error function with respect to both the weight vector w and the regularization coefÔ¨Åcient Œª is clearly not the right approach since this leads to the unregularized solution with Œª 0',\n",
       " 'although the introduction of regularization terms can control over Ô¨Åtting for models with many parameters this raises the question of how to determine a suitable value for the regularization coefÔ¨Åcient Œª seeking the solution that minimizes the regularized error function with respect to both the weight vector w and the regularization coefÔ¨Åcient Œª is clearly not the right approach since this leads to the unregularized solution with Œª 0 as we have seen in earlier chapters the phenomenon of over Ô¨Åtting is really an unfortunate property of maximum likelihood and does not arise when we marginalize over parameters in a bayesian setting in this chapter we shall consider the bayesian view of model complexity in some depth',\n",
       " 'as we have seen in earlier chapters the phenomenon of over Ô¨Åtting is really an unfortunate property of maximum likelihood and does not arise when we marginalize over parameters in a bayesian setting in this chapter we shall consider the bayesian view of model complexity in some depth before doing so however it is instructive to consider a frequentist viewpoint of the model complexity issue known as thebias variance trade off although we shall introduce this concept in the context of linear basis function models where it is easy to illustrate the ideas using simple examples the discussion has more general applicability',\n",
       " 'before doing so however it is instructive to consider a frequentist viewpoint of the model complexity issue known as thebias variance trade off although we shall introduce this concept in the context of linear basis function models where it is easy to illustrate the ideas using simple examples the discussion has more general applicability in section 1 5 5 when we discussed decision theory for regression problems we considered various loss functions each of which leads to a corresponding optimal prediction once we are given the conditional distributionp t x a popular choice is 148 3',\n",
       " 'in section 1 5 5 when we discussed decision theory for regression problems we considered various loss functions each of which leads to a corresponding optimal prediction once we are given the conditional distributionp t x a popular choice is 148 3 linear models for regression the squared loss function for which the optimal prediction is given by the conditional expectation which we denote by h x and which is given by h x e t x tp t x d t 3 36 at this point it is worth distinguishing between the squared loss function arising from decision theory and the sum of squares error function that arose in the maxi mum likelihood estimation of model parameters',\n",
       " 'a popular choice is 148 3 linear models for regression the squared loss function for which the optimal prediction is given by the conditional expectation which we denote by h x and which is given by h x e t x tp t x d t 3 36 at this point it is worth distinguishing between the squared loss function arising from decision theory and the sum of squares error function that arose in the maxi mum likelihood estimation of model parameters we might use more sophisticated techniques than least squares for example regularization or a fully bayesian ap proach to determine the conditional distribution p t x these can all be combined with the squared loss function for the purpose of making predictions',\n",
       " 'we might use more sophisticated techniques than least squares for example regularization or a fully bayesian ap proach to determine the conditional distribution p t x these can all be combined with the squared loss function for the purpose of making predictions we showed in section 1 5 5 that the expected squared loss can be written in the form e l y x h x 2 p x dx h x t 2p x t d xdt 3 37 recall that the second term which is independent of y x arises from the intrinsic noise on the data and represents the minimum achievable value of the expected loss the Ô¨Årst term depends on our choice for the function y x and we will seek a so lution for y x which makes this term a minimum because it is nonnegative the smallest that we can hope to make this term is zero',\n",
       " 'the Ô¨Årst term depends on our choice for the function y x and we will seek a so lution for y x which makes this term a minimum because it is nonnegative the smallest that we can hope to make this term is zero if we had an unlimited supply of data and unlimited computational resources we could in principle Ô¨Ånd the regres sion function h x to any desired degree of accuracy and this would represent the optimal choice for y x however in practice we have a data set d containing only a Ô¨Ånite number n of data points and consequently we do not know the regression function h x exactly',\n",
       " 'if we had an unlimited supply of data and unlimited computational resources we could in principle Ô¨Ånd the regres sion function h x to any desired degree of accuracy and this would represent the optimal choice for y x however in practice we have a data set d containing only a Ô¨Ånite number n of data points and consequently we do not know the regression function h x exactly if we model the h x using a parametric function y x w governed by a pa rameter vector w then from a bayesian perspective the uncertainty in our model is expressed through a posterior distribution overw a frequentist treatment however involves making a point estimate of w based on the data set d and tries instead to interpret the uncertainty of this estimate through the following thought experi ment',\n",
       " 'if we model the h x using a parametric function y x w governed by a pa rameter vector w then from a bayesian perspective the uncertainty in our model is expressed through a posterior distribution overw a frequentist treatment however involves making a point estimate of w based on the data set d and tries instead to interpret the uncertainty of this estimate through the following thought experi ment suppose we had a large number of data sets each of size n and each drawn independently from the distribution p t x for any given data set d we can run our learning algorithm and obtain a prediction function y x d different data sets from the ensemble will give different functions and consequently different values of the squared loss',\n",
       " 'for any given data set d we can run our learning algorithm and obtain a prediction function y x d different data sets from the ensemble will give different functions and consequently different values of the squared loss the performance of a particular learning algorithm is then assessed by taking the average over this ensemble of data sets consider the integrand of the Ô¨Årst term in 3 37 which for a particular data set d takes the form y x d h x 2 3 38 because this quantity will be dependent on the particular data setd we take its aver age over the ensemble of data sets if we add and subtract the quantity ed y x d 3 2',\n",
       " '3 38 because this quantity will be dependent on the particular data setd we take its aver age over the ensemble of data sets if we add and subtract the quantity ed y x d 3 2 the bias variance decomposition 149 inside the braces and then expand we obtain y x d ed y x d ed y x d h x 2 y x d ed y x d 2 ed y x d h x 2 2 y x d ed y x d ed y x d h x 3 39 we now take the expectation of this expression with respect to d and note that the Ô¨Ånal term will vanish giving ed y x d h x 2 ed y x d h x 2 bias 2 ed y x d ed y x d 2 variance 3 40 we see that the expected squared difference between y x d and the regression function h x can be expressed as the sum of two terms',\n",
       " '3 39 we now take the expectation of this expression with respect to d and note that the Ô¨Ånal term will vanish giving ed y x d h x 2 ed y x d h x 2 bias 2 ed y x d ed y x d 2 variance 3 40 we see that the expected squared difference between y x d and the regression function h x can be expressed as the sum of two terms the Ô¨Årst term called the squared bias represents the extent to which the average prediction over all data sets differs from the desired regression function the second term called the variance measures the extent to which the solutions for individual data sets vary around their average and hence this measures the extent to which the functiony x d is sensitive to the particular choice of data set',\n",
       " 'the Ô¨Årst term called the squared bias represents the extent to which the average prediction over all data sets differs from the desired regression function the second term called the variance measures the extent to which the solutions for individual data sets vary around their average and hence this measures the extent to which the functiony x d is sensitive to the particular choice of data set we shall provide some intuition to support these deÔ¨Ånitions shortly when we consider a simple example so far we have considered a single input valuex',\n",
       " 'we shall provide some intuition to support these deÔ¨Ånitions shortly when we consider a simple example so far we have considered a single input valuex if we substitute this expansion back into 3 37 we obtain the following decomposition of the expected squared loss expected loss bias 2 variance noise 3 41 where bias 2 ed y x d h x 2p x dx 3 42 variance ed y x d ed y x d 2 p x dx 3 43 noise h x t 2p x t d xdt 3 44 and the bias and variance terms now refer to integrated quantities our goal is to minimize the expected loss which we have decomposed into the sum of a squared bias a variance and a constant noise term',\n",
       " 'if we substitute this expansion back into 3 37 we obtain the following decomposition of the expected squared loss expected loss bias 2 variance noise 3 41 where bias 2 ed y x d h x 2p x dx 3 42 variance ed y x d ed y x d 2 p x dx 3 43 noise h x t 2p x t d xdt 3 44 and the bias and variance terms now refer to integrated quantities our goal is to minimize the expected loss which we have decomposed into the sum of a squared bias a variance and a constant noise term as we shall see there is a trade off between bias and variance with very Ô¨Çexible models having low bias and high variance and relatively rigid models having high bias and low variance',\n",
       " 'our goal is to minimize the expected loss which we have decomposed into the sum of a squared bias a variance and a constant noise term as we shall see there is a trade off between bias and variance with very Ô¨Çexible models having low bias and high variance and relatively rigid models having high bias and low variance the model with the optimal predictive capability is the one that leads to the best balance between bias and variance this is illustrated by considering the sinusoidal data set from chapter 1 here we generate 100 data sets each containing n 2 5appendix a data points independently from the sinusoidal curve h x s i n 2œÄx the data sets are indexed by l 1 l where l 100 and for each data set d l we 150 3',\n",
       " 'here we generate 100 data sets each containing n 2 5appendix a data points independently from the sinusoidal curve h x s i n 2œÄx the data sets are indexed by l 1 l where l 100 and for each data set d l we 150 3 linear models for regression x t lnŒª 2 6 0 1 1 0 1 x t 0 1 1 0 1 x t lnŒª 0 31 0 1 1 0 1 x t 0 1 1 0 1 x t lnŒª 2 4 0 1 1 0 1 x t 0 1 1 0 1 figure 3 5 illustration of the dependence of bias and variance on model complexity governed by a regulariza tion parameter Œª using the sinusoidal data set from chapter 1 there arel 100data sets each havingn 2 5 data points and there are 24 gaussian basis functions in the model so that the total number of parameters is m 2 5 including the bias parameter',\n",
       " 'linear models for regression x t lnŒª 2 6 0 1 1 0 1 x t 0 1 1 0 1 x t lnŒª 0 31 0 1 1 0 1 x t 0 1 1 0 1 x t lnŒª 2 4 0 1 1 0 1 x t 0 1 1 0 1 figure 3 5 illustration of the dependence of bias and variance on model complexity governed by a regulariza tion parameter Œª using the sinusoidal data set from chapter 1 there arel 100data sets each havingn 2 5 data points and there are 24 gaussian basis functions in the model so that the total number of parameters is m 2 5 including the bias parameter the left column shows the result of Ô¨Åtting the model to the data sets for various values of ln Œª for clarity only 20 of the 100 Ô¨Åts are shown',\n",
       " 'there arel 100data sets each havingn 2 5 data points and there are 24 gaussian basis functions in the model so that the total number of parameters is m 2 5 including the bias parameter the left column shows the result of Ô¨Åtting the model to the data sets for various values of ln Œª for clarity only 20 of the 100 Ô¨Åts are shown the right column shows the corresponding average of the 100 Ô¨Åts red along with the sinusoidal function from which the data sets were generated green 3 2 the bias variance decomposition 151 figure 3 6 plot of squared bias and variance together with their sum correspond ing to the results shown in fig ure 3 5 also shown is the average test set error for a test data set size of 1000 points',\n",
       " 'the bias variance decomposition 151 figure 3 6 plot of squared bias and variance together with their sum correspond ing to the results shown in fig ure 3 5 also shown is the average test set error for a test data set size of 1000 points the minimum value of bias 2 variance occurs around ln Œª 0 31 which is close to the value that gives the minimum error on the test data lnŒª 3 2 1 0 1 2 0 0 03 0 06 0 09 0 12 0 15 bias 2 variance bias 2 variance test error Ô¨Åt a model with 24 gaussian basis functions by minimizing the regularized error function 3 27 to give a prediction function y l x as shown in figure 3 5',\n",
       " 'the minimum value of bias 2 variance occurs around ln Œª 0 31 which is close to the value that gives the minimum error on the test data lnŒª 3 2 1 0 1 2 0 0 03 0 06 0 09 0 12 0 15 bias 2 variance bias 2 variance test error Ô¨Åt a model with 24 gaussian basis functions by minimizing the regularized error function 3 27 to give a prediction function y l x as shown in figure 3 5 the top row corresponds to a large value of the regularization coefÔ¨ÅcientŒª that gives low variance because the red curves in the left plot look similar but high bias because the two curves in the right plot are very different',\n",
       " 'lnŒª 3 2 1 0 1 2 0 0 03 0 06 0 09 0 12 0 15 bias 2 variance bias 2 variance test error Ô¨Åt a model with 24 gaussian basis functions by minimizing the regularized error function 3 27 to give a prediction function y l x as shown in figure 3 5 the top row corresponds to a large value of the regularization coefÔ¨ÅcientŒª that gives low variance because the red curves in the left plot look similar but high bias because the two curves in the right plot are very different conversely on the bottom row for which Œª is small there is large variance shown by the high variability between the red curves in the left plot but low bias shown by the good Ô¨Åt between the average model Ô¨Åt and the original sinusoidal function',\n",
       " 'the top row corresponds to a large value of the regularization coefÔ¨ÅcientŒª that gives low variance because the red curves in the left plot look similar but high bias because the two curves in the right plot are very different conversely on the bottom row for which Œª is small there is large variance shown by the high variability between the red curves in the left plot but low bias shown by the good Ô¨Åt between the average model Ô¨Åt and the original sinusoidal function note that the result of averaging many solutions for the complex model with m 2 5 is a very good Ô¨Åt to the regression function which suggests that averaging may be a beneÔ¨Åcial procedure',\n",
       " 'conversely on the bottom row for which Œª is small there is large variance shown by the high variability between the red curves in the left plot but low bias shown by the good Ô¨Åt between the average model Ô¨Åt and the original sinusoidal function note that the result of averaging many solutions for the complex model with m 2 5 is a very good Ô¨Åt to the regression function which suggests that averaging may be a beneÔ¨Åcial procedure indeed a weighted averaging of multiple solutions lies at the heart of a bayesian approach although the averaging is with respect to the posterior distribution of parameters not with respect to multiple data sets we can also examine the bias variance trade off quantitatively for this example',\n",
       " 'indeed a weighted averaging of multiple solutions lies at the heart of a bayesian approach although the averaging is with respect to the posterior distribution of parameters not with respect to multiple data sets we can also examine the bias variance trade off quantitatively for this example the average prediction is estimated from y x 1 l l l 1 y l x 3 45 and the integrated squared bias and integrated variance are then given by bias 2 1 n n n 1 y xn h xn 2 3 46 variance 1 n n n 1 1 l l l 1 y l xn y xn 2 3 47 where the integral over x weighted by the distribution p x is approximated by a Ô¨Ånite sum over data points drawn from that distribution these quantities along with their sum are plotted as a function of lnŒª in figure 3 6',\n",
       " 'the average prediction is estimated from y x 1 l l l 1 y l x 3 45 and the integrated squared bias and integrated variance are then given by bias 2 1 n n n 1 y xn h xn 2 3 46 variance 1 n n n 1 1 l l l 1 y l xn y xn 2 3 47 where the integral over x weighted by the distribution p x is approximated by a Ô¨Ånite sum over data points drawn from that distribution these quantities along with their sum are plotted as a function of lnŒª in figure 3 6 we see that small values of Œª allow the model to become Ô¨Ånely tuned to the noise on each individual 152 3 linear models for regression data set leading to large variance conversely a large value of Œª pulls the weight parameters towards zero leading to large bias',\n",
       " 'linear models for regression data set leading to large variance conversely a large value of Œª pulls the weight parameters towards zero leading to large bias although the bias variance decomposition may provide some interesting in sights into the model complexity issue from a frequentist perspective it is of lim ited practical value because the bias variance decomposition is based on averages with respect to ensembles of data sets whereas in practice we have only the single observed data set if we had a large number of independent training sets of a given size we would be better off combining them into a single large training set which of course would reduce the level of over Ô¨Åtting for a given model complexity',\n",
       " 'although the bias variance decomposition may provide some interesting in sights into the model complexity issue from a frequentist perspective it is of lim ited practical value because the bias variance decomposition is based on averages with respect to ensembles of data sets whereas in practice we have only the single observed data set if we had a large number of independent training sets of a given size we would be better off combining them into a single large training set which of course would reduce the level of over Ô¨Åtting for a given model complexity given these limitations we turn in the next section to a bayesian treatment of linear basis function models which not only provides powerful insights into the issues of over Ô¨Åtting but which also leads to practical techniques for addressing the question model complexity',\n",
       " 'if we had a large number of independent training sets of a given size we would be better off combining them into a single large training set which of course would reduce the level of over Ô¨Åtting for a given model complexity given these limitations we turn in the next section to a bayesian treatment of linear basis function models which not only provides powerful insights into the issues of over Ô¨Åtting but which also leads to practical techniques for addressing the question model complexity 3 3 bayesian linear regression in our discussion of maximum likelihood for setting the parameters of a linear re gression model we have seen that the effective model complexity governed by the number of basis functions needs to be controlled according to the size of the data set',\n",
       " '3 3 bayesian linear regression in our discussion of maximum likelihood for setting the parameters of a linear re gression model we have seen that the effective model complexity governed by the number of basis functions needs to be controlled according to the size of the data set adding a regularization term to the log likelihood function means the effective model complexity can then be controlled by the value of the regularization coefÔ¨Å cient although the choice of the number and form of the basis functions is of course still important in determining the overall behaviour of the model',\n",
       " 'bayesian linear regression in our discussion of maximum likelihood for setting the parameters of a linear re gression model we have seen that the effective model complexity governed by the number of basis functions needs to be controlled according to the size of the data set adding a regularization term to the log likelihood function means the effective model complexity can then be controlled by the value of the regularization coefÔ¨Å cient although the choice of the number and form of the basis functions is of course still important in determining the overall behaviour of the model this leaves the issue of deciding the appropriate model complexity for the par ticular problem which cannot be decided simply by maximizing the likelihood func tion because this always leads to excessively complex models and over Ô¨Åtting',\n",
       " 'adding a regularization term to the log likelihood function means the effective model complexity can then be controlled by the value of the regularization coefÔ¨Å cient although the choice of the number and form of the basis functions is of course still important in determining the overall behaviour of the model this leaves the issue of deciding the appropriate model complexity for the par ticular problem which cannot be decided simply by maximizing the likelihood func tion because this always leads to excessively complex models and over Ô¨Åtting in dependent hold out data can be used to determine model complexity as discussed in section 1 3 but this can be both computationally expensive and wasteful of valu able data',\n",
       " 'this leaves the issue of deciding the appropriate model complexity for the par ticular problem which cannot be decided simply by maximizing the likelihood func tion because this always leads to excessively complex models and over Ô¨Åtting in dependent hold out data can be used to determine model complexity as discussed in section 1 3 but this can be both computationally expensive and wasteful of valu able data we therefore turn to a bayesian treatment of linear regression which will avoid the over Ô¨Åtting problem of maximum likelihood and which will also lead to automatic methods of determining model complexity using the training data alone',\n",
       " 'in dependent hold out data can be used to determine model complexity as discussed in section 1 3 but this can be both computationally expensive and wasteful of valu able data we therefore turn to a bayesian treatment of linear regression which will avoid the over Ô¨Åtting problem of maximum likelihood and which will also lead to automatic methods of determining model complexity using the training data alone again for simplicity we will focus on the case of a single target variable t ex tension to multiple target variables is straightforward and follows the discussion of section 3 1 5',\n",
       " 'we therefore turn to a bayesian treatment of linear regression which will avoid the over Ô¨Åtting problem of maximum likelihood and which will also lead to automatic methods of determining model complexity using the training data alone again for simplicity we will focus on the case of a single target variable t ex tension to multiple target variables is straightforward and follows the discussion of section 3 1 5 3 3 1 parameter distribution we begin our discussion of the bayesian treatment of linear regression by in troducing a prior probability distribution over the model parameters w for the mo ment we shall treat the noise precision parameter Œ≤ as a known constant',\n",
       " 'again for simplicity we will focus on the case of a single target variable t ex tension to multiple target variables is straightforward and follows the discussion of section 3 1 5 3 3 1 parameter distribution we begin our discussion of the bayesian treatment of linear regression by in troducing a prior probability distribution over the model parameters w for the mo ment we shall treat the noise precision parameter Œ≤ as a known constant first note that the likelihood functionp t w deÔ¨Åned by 3 10 is the exponential of a quadratic function of w the corresponding conjugate prior is therefore given by a gaussian distribution of the form p w n w m0 s0 3 48 having mean m0 and covariance s0 3 3',\n",
       " 'first note that the likelihood functionp t w deÔ¨Åned by 3 10 is the exponential of a quadratic function of w the corresponding conjugate prior is therefore given by a gaussian distribution of the form p w n w m0 s0 3 48 having mean m0 and covariance s0 3 3 bayesian linear regression 153 next we compute the posterior distribution which is proportional to the product of the likelihood function and the prior due to the choice of a conjugate gaus sian prior distribution the posterior will also be gaussian we can evaluate this distribution by the usual procedure of completing the square in the exponential and then Ô¨Ånding the normalization coefÔ¨Åcient using the standard result for a normalized gaussian',\n",
       " 'due to the choice of a conjugate gaus sian prior distribution the posterior will also be gaussian we can evaluate this distribution by the usual procedure of completing the square in the exponential and then Ô¨Ånding the normalization coefÔ¨Åcient using the standard result for a normalized gaussian however we have already done the necessary work in deriving the gen exercise 3 7 eral result 2 116 which allows us to write down the posterior distribution directly in the form p w t n w m n sn 3 49 where mn sn s 1 0 m0 Œ≤œÜtt 3 50 s 1 n s 1 0 Œ≤œÜtœÜ 3 51 note that because the posterior distribution is gaussian its mode coincides with its mean thus the maximum posterior weight vector is simply given bywmap mn',\n",
       " '3 51 note that because the posterior distribution is gaussian its mode coincides with its mean thus the maximum posterior weight vector is simply given bywmap mn if we consider an inÔ¨Ånitely broad prior s0 Œ± 1i with Œ± 0 the mean mn of the posterior distribution reduces to the maximum likelihood value wml given by 3 15 similarly if n 0 then the posterior distribution reverts to the prior furthermore if data points arrive sequentially then the posterior distribution at any stage acts as the prior distribution for the subsequent data point such that the new posterior distribution is again given by 3 49 exercise 3 8 for the remainder of this chapter we shall consider a particular form of gaus sian prior in order to simplify the treatment',\n",
       " 'similarly if n 0 then the posterior distribution reverts to the prior furthermore if data points arrive sequentially then the posterior distribution at any stage acts as the prior distribution for the subsequent data point such that the new posterior distribution is again given by 3 49 exercise 3 8 for the remainder of this chapter we shall consider a particular form of gaus sian prior in order to simplify the treatment speciÔ¨Åcally we consider a zero mean isotropic gaussian governed by a single precision parameter Œ± so that p w Œ± n w 0 Œ± 1i 3 52 and the corresponding posterior distribution over w is then given by 3 49 with mn Œ≤sn œÜtt 3 53 s 1 n Œ±i Œ≤œÜtœÜ',\n",
       " 'furthermore if data points arrive sequentially then the posterior distribution at any stage acts as the prior distribution for the subsequent data point such that the new posterior distribution is again given by 3 49 exercise 3 8 for the remainder of this chapter we shall consider a particular form of gaus sian prior in order to simplify the treatment speciÔ¨Åcally we consider a zero mean isotropic gaussian governed by a single precision parameter Œ± so that p w Œ± n w 0 Œ± 1i 3 52 and the corresponding posterior distribution over w is then given by 3 49 with mn Œ≤sn œÜtt 3 53 s 1 n Œ±i Œ≤œÜtœÜ 3 54 the log of the posterior distribution is given by the sum of the log likelihood and the log of the prior and as a function of w takes the form lnp w t Œ≤ 2 n n 1 tn wtœÜ xn 2 Œ± 2 wtw c o n s t 3 55 maximization of this posterior distribution with respect to w is therefore equiva lent to the minimization of the sum of squares error function with the addition of a quadratic regularization term corresponding to 3 27 with Œª Œ± Œ≤',\n",
       " 'speciÔ¨Åcally we consider a zero mean isotropic gaussian governed by a single precision parameter Œ± so that p w Œ± n w 0 Œ± 1i 3 52 and the corresponding posterior distribution over w is then given by 3 49 with mn Œ≤sn œÜtt 3 53 s 1 n Œ±i Œ≤œÜtœÜ 3 54 the log of the posterior distribution is given by the sum of the log likelihood and the log of the prior and as a function of w takes the form lnp w t Œ≤ 2 n n 1 tn wtœÜ xn 2 Œ± 2 wtw c o n s t 3 55 maximization of this posterior distribution with respect to w is therefore equiva lent to the minimization of the sum of squares error function with the addition of a quadratic regularization term corresponding to 3 27 with Œª Œ± Œ≤ we can illustrate bayesian learning in a linear basis function model as well as the sequential update of a posterior distribution using a simple example involving straight line Ô¨Åtting',\n",
       " '3 54 the log of the posterior distribution is given by the sum of the log likelihood and the log of the prior and as a function of w takes the form lnp w t Œ≤ 2 n n 1 tn wtœÜ xn 2 Œ± 2 wtw c o n s t 3 55 maximization of this posterior distribution with respect to w is therefore equiva lent to the minimization of the sum of squares error function with the addition of a quadratic regularization term corresponding to 3 27 with Œª Œ± Œ≤ we can illustrate bayesian learning in a linear basis function model as well as the sequential update of a posterior distribution using a simple example involving straight line Ô¨Åtting consider a single input variable x a single target variable t and 154 3 linear models for regression a linear model of the form y x w w0 w1x',\n",
       " 'consider a single input variable x a single target variable t and 154 3 linear models for regression a linear model of the form y x w w0 w1x because this has just two adap tive parameters we can plot the prior and posterior distributions directly in parameter space we generate synthetic data from the functionf x a a 0 a1x with param eter values a0 0 3 and a1 0 5 by Ô¨Årst choosing values of xn from the uniform distribution u x 1 1 then evaluatingf xn a and Ô¨Ånally adding gaussian noise with standard deviation of 0 2 to obtain the target values tn our goal is to recover the values of a0 and a1 from such data and we will explore the dependence on the size of the data set',\n",
       " 'we generate synthetic data from the functionf x a a 0 a1x with param eter values a0 0 3 and a1 0 5 by Ô¨Årst choosing values of xn from the uniform distribution u x 1 1 then evaluatingf xn a and Ô¨Ånally adding gaussian noise with standard deviation of 0 2 to obtain the target values tn our goal is to recover the values of a0 and a1 from such data and we will explore the dependence on the size of the data set we assume here that the noise variance is known and hence we set the precision parameter to its true value Œ≤ 1 0 2 2 2 5 similarly we Ô¨Åx the parameter Œ± to 2 0 we shall shortly discuss strategies for determining Œ± and Œ≤ from the training data',\n",
       " 'similarly we Ô¨Åx the parameter Œ± to 2 0 we shall shortly discuss strategies for determining Œ± and Œ≤ from the training data figure 3 7 shows the results of bayesian learning in this model as the size of the data set is increased and demonstrates the sequential nature of bayesian learning in which the current posterior distribution forms the prior when a new data point is observed it is worth taking time to study this Ô¨Ågure in detail as it illustrates several important aspects of bayesian inference the Ô¨Årst row of this Ô¨Ågure corresponds to the situation before any data points are observed and shows a plot of the prior distribution in w space together with six samples of the function y x w in which the values of w are drawn from the prior',\n",
       " 'it is worth taking time to study this Ô¨Ågure in detail as it illustrates several important aspects of bayesian inference the Ô¨Årst row of this Ô¨Ågure corresponds to the situation before any data points are observed and shows a plot of the prior distribution in w space together with six samples of the function y x w in which the values of w are drawn from the prior in the second row we see the situation after observing a single data point the location x t of the data point is shown by a blue circle in the right hand column',\n",
       " 'in the second row we see the situation after observing a single data point the location x t of the data point is shown by a blue circle in the right hand column in the left hand column is a plot of the likelihood function p t x w for this data point as a function of w note that the likelihood function provides a soft constraint that the line must pass close to the data point where close is determined by the noise precision Œ≤ for comparison the true parameter values a 0 0 3 and a1 0 5 used to generate the data set are shown by a white cross in the plots in the left column of figure 3 7 when we multiply this likelihood function by the prior from the top row and normalize we obtain the posterior distribution shown in the middle plot on the second row',\n",
       " 'for comparison the true parameter values a 0 0 3 and a1 0 5 used to generate the data set are shown by a white cross in the plots in the left column of figure 3 7 when we multiply this likelihood function by the prior from the top row and normalize we obtain the posterior distribution shown in the middle plot on the second row sam ples of the regression function y x w obtained by drawing samples of w from this posterior distribution are shown in the right hand plot note that these sample lines all pass close to the data point the third row of this Ô¨Ågure shows the effect of ob serving a second data point again shown by a blue circle in the plot in the right hand column the corresponding likelihood function for this second data point alone is shown in the left plot',\n",
       " 'the third row of this Ô¨Ågure shows the effect of ob serving a second data point again shown by a blue circle in the plot in the right hand column the corresponding likelihood function for this second data point alone is shown in the left plot when we multiply this likelihood function by the posterior distribution from the second row we obtain the posterior distribution shown in the middle plot of the third row note that this is exactly the same posterior distribution as would be obtained by combining the original prior with the likelihood function for the two data points this posterior has now been inÔ¨Çuenced by two data points and because two points are sufÔ¨Åcient to deÔ¨Åne a line this already gives a relatively compact posterior distribution',\n",
       " 'note that this is exactly the same posterior distribution as would be obtained by combining the original prior with the likelihood function for the two data points this posterior has now been inÔ¨Çuenced by two data points and because two points are sufÔ¨Åcient to deÔ¨Åne a line this already gives a relatively compact posterior distribution samples from this posterior distribution give rise to the functions shown in red in the third column and we see that these functions pass close to both of the data points the fourth row shows the effect of observing a total of 20 data points the left hand plot shows the likelihood function for the 20 th data point alone and the middle plot shows the resulting posterior distribution that has now absorbed information from all 20 observations',\n",
       " 'the fourth row shows the effect of observing a total of 20 data points the left hand plot shows the likelihood function for the 20 th data point alone and the middle plot shows the resulting posterior distribution that has now absorbed information from all 20 observations note how the posterior is much sharper than in the third row in the limit of an inÔ¨Ånite number of data points the 3 3 bayesian linear regression 155 figure 3 7 illustration of sequential bayesian learning for a simple linear model of the formy x w w0 w1x a detailed description of this Ô¨Ågure is given in the text 156 3 linear models for regression posterior distribution would become a delta function centred on the true parameter values shown by the white cross',\n",
       " '156 3 linear models for regression posterior distribution would become a delta function centred on the true parameter values shown by the white cross other forms of prior over the parameters can be considered for instance we can generalize the gaussian prior to give p w Œ± q 2 Œ± 2 1 q 1 Œ≥ 1 q m exp Œ± 2 m j 1 wj q 3 56 in which q 2 corresponds to the gaussian distribution and only in this case is the prior conjugate to the likelihood function 3 10 finding the maximum of the poste rior distribution overw corresponds to minimization of the regularized error function 3 29 in the case of the gaussian prior the mode of the posterior distribution was equal to the mean although this will no longer hold if q 2',\n",
       " 'finding the maximum of the poste rior distribution overw corresponds to minimization of the regularized error function 3 29 in the case of the gaussian prior the mode of the posterior distribution was equal to the mean although this will no longer hold if q 2 3 3 2 predictive distribution in practice we are not usually interested in the value of w itself but rather in making predictions of t for new values of x this requires that we evaluate the predictive distributiondeÔ¨Åned by p t t Œ± Œ≤ p t w Œ≤ p w t Œ± Œ≤ d w 3 57 in which t is the vector of target values from the training set and we have omitted the corresponding input vectors from the right hand side of the conditioning statements to simplify the notation',\n",
       " '3 3 2 predictive distribution in practice we are not usually interested in the value of w itself but rather in making predictions of t for new values of x this requires that we evaluate the predictive distributiondeÔ¨Åned by p t t Œ± Œ≤ p t w Œ≤ p w t Œ± Œ≤ d w 3 57 in which t is the vector of target values from the training set and we have omitted the corresponding input vectors from the right hand side of the conditioning statements to simplify the notation the conditional distribution p t x w Œ≤ of the target vari able is given by 3 8 and the posterior weight distribution is given by 3 49',\n",
       " 'this requires that we evaluate the predictive distributiondeÔ¨Åned by p t t Œ± Œ≤ p t w Œ≤ p w t Œ± Œ≤ d w 3 57 in which t is the vector of target values from the training set and we have omitted the corresponding input vectors from the right hand side of the conditioning statements to simplify the notation the conditional distribution p t x w Œ≤ of the target vari able is given by 3 8 and the posterior weight distribution is given by 3 49 we see that 3 57 involves the convolution of two gaussian distributions and so making use of the result 2 115 from section 8 1 4 we see that the predictive distribution takes the formexercise 3 10 p t x t Œ± Œ≤ n t m t n œÜ x œÉ 2 n x 3 58 where the variance œÉ2 n x of the predictive distribution is given by œÉ2 n x 1 Œ≤ œÜ x tsn œÜ x',\n",
       " 'the conditional distribution p t x w Œ≤ of the target vari able is given by 3 8 and the posterior weight distribution is given by 3 49 we see that 3 57 involves the convolution of two gaussian distributions and so making use of the result 2 115 from section 8 1 4 we see that the predictive distribution takes the formexercise 3 10 p t x t Œ± Œ≤ n t m t n œÜ x œÉ 2 n x 3 58 where the variance œÉ2 n x of the predictive distribution is given by œÉ2 n x 1 Œ≤ œÜ x tsn œÜ x 3 59 the Ô¨Årst term in 3 59 represents the noise on the data whereas the second term reÔ¨Çects the uncertainty associated with the parameters w because the noise process and the distribution of w are independent gaussians their variances are additive',\n",
       " 'we see that 3 57 involves the convolution of two gaussian distributions and so making use of the result 2 115 from section 8 1 4 we see that the predictive distribution takes the formexercise 3 10 p t x t Œ± Œ≤ n t m t n œÜ x œÉ 2 n x 3 58 where the variance œÉ2 n x of the predictive distribution is given by œÉ2 n x 1 Œ≤ œÜ x tsn œÜ x 3 59 the Ô¨Årst term in 3 59 represents the noise on the data whereas the second term reÔ¨Çects the uncertainty associated with the parameters w because the noise process and the distribution of w are independent gaussians their variances are additive note that as additional data points are observed the posterior distribution becomes narrower as a consequence it can be shown qazaz et al 1997 that œÉ 2 n 1 x œÉ2 n x',\n",
       " 'note that as additional data points are observed the posterior distribution becomes narrower as a consequence it can be shown qazaz et al 1997 that œÉ 2 n 1 x œÉ2 n x in the limit n the second term in 3 59 goes to zero and the varianceexercise 3 11 of the predictive distribution arises solely from the additive noise governed by the parameter Œ≤ as an illustration of the predictive distribution for bayesian linear regression models let us return to the synthetic sinusoidal data set of section 1 1 in figure 3 8 3 3',\n",
       " 'as an illustration of the predictive distribution for bayesian linear regression models let us return to the synthetic sinusoidal data set of section 1 1 in figure 3 8 3 3 bayesian linear regression 157 x t 0 1 1 0 1 x t 0 1 1 0 1 x t 0 1 1 0 1 x t 0 1 1 0 1 figure 3 8 examples of the predictive distribution 3 58 for a model consisting of 9 gaussian basis functions of the form 3 4 using the synthetic sinusoidal data set of section 1 1 see the text for a detailed discussion we Ô¨Åt a model comprising a linear combination of gaussian basis functions to data sets of various sizes and then look at the corresponding posterior distributions here the green curves correspond to the function sin 2œÄx from which the data points were generated with the addition of gaussian noise',\n",
       " 'we Ô¨Åt a model comprising a linear combination of gaussian basis functions to data sets of various sizes and then look at the corresponding posterior distributions here the green curves correspond to the function sin 2œÄx from which the data points were generated with the addition of gaussian noise data sets of size n 1 n 2 n 4 and n 2 5 are shown in the four plots by the blue circles for each plot the red curve shows the mean of the corresponding gaussian predictive distribution and the red shaded region spans one standard deviation either side of the mean note that the predictive uncertainty depends on x and is smallest in the neighbourhood of the data points also note that the level of uncertainty decreases as more data points are observed',\n",
       " 'note that the predictive uncertainty depends on x and is smallest in the neighbourhood of the data points also note that the level of uncertainty decreases as more data points are observed the plots in figure 3 8 only show the point wise predictive variance as a func tion of x in order to gain insight into the covariance between the predictions at different values of x we can draw samples from the posterior distribution over w and then plot the corresponding functions y x w as shown in figure 3 9 158 3 linear models for regression x t 0 1 1 0 1 x t 0 1 1 0 1 x t 0 1 1 0 1 x t 0 1 1 0 1 figure 3 9 plots of the function y x w using samples from the posterior distributions overw corresponding to the plots in figure 3 8',\n",
       " '158 3 linear models for regression x t 0 1 1 0 1 x t 0 1 1 0 1 x t 0 1 1 0 1 x t 0 1 1 0 1 figure 3 9 plots of the function y x w using samples from the posterior distributions overw corresponding to the plots in figure 3 8 if we used localized basis functions such as gaussians then in regions away from the basis function centres the contribution from the second term in the predic tive variance 3 59 will go to zero leaving only the noise contribution Œ≤ 1 thus the model becomes very conÔ¨Ådent in its predictions when extrapolating outside the region occupied by the basis functions which is generally an undesirable behaviour',\n",
       " 'if we used localized basis functions such as gaussians then in regions away from the basis function centres the contribution from the second term in the predic tive variance 3 59 will go to zero leaving only the noise contribution Œ≤ 1 thus the model becomes very conÔ¨Ådent in its predictions when extrapolating outside the region occupied by the basis functions which is generally an undesirable behaviour this problem can be avoided by adopting an alternative bayesian approach to re gression known as a gaussian process section 6 4 note that if both w and Œ≤ are treated as unknown then we can introduce a conjugate prior distribution p w Œ≤ that from the discussion in section 2 3 6 will be given by a gaussian gamma distribution denison et al 2002',\n",
       " 'thus the model becomes very conÔ¨Ådent in its predictions when extrapolating outside the region occupied by the basis functions which is generally an undesirable behaviour this problem can be avoided by adopting an alternative bayesian approach to re gression known as a gaussian process section 6 4 note that if both w and Œ≤ are treated as unknown then we can introduce a conjugate prior distribution p w Œ≤ that from the discussion in section 2 3 6 will be given by a gaussian gamma distribution denison et al 2002 in this case theexercise 3 12 predictive distribution is a student s t distribution exercise 3 13 3 3',\n",
       " 'this problem can be avoided by adopting an alternative bayesian approach to re gression known as a gaussian process section 6 4 note that if both w and Œ≤ are treated as unknown then we can introduce a conjugate prior distribution p w Œ≤ that from the discussion in section 2 3 6 will be given by a gaussian gamma distribution denison et al 2002 in this case theexercise 3 12 predictive distribution is a student s t distribution exercise 3 13 3 3 bayesian linear regression 159 figure 3 10 the equivalent ker nel k x x for the gaussian basis functions in figure 3 1 shown as a plot of x versus x together with three slices through this matrix cor responding to three different values of x',\n",
       " 'in this case theexercise 3 12 predictive distribution is a student s t distribution exercise 3 13 3 3 bayesian linear regression 159 figure 3 10 the equivalent ker nel k x x for the gaussian basis functions in figure 3 1 shown as a plot of x versus x together with three slices through this matrix cor responding to three different values of x the data set used to generate this kernel comprised 200 values of x equally spaced over the interval 1 1 3 3 3 equivalent kernel the posterior mean solution 3 53 for the linear basis function model has an in teresting interpretation that will set the stage for kernel methods including gaussian processes',\n",
       " 'the data set used to generate this kernel comprised 200 values of x equally spaced over the interval 1 1 3 3 3 equivalent kernel the posterior mean solution 3 53 for the linear basis function model has an in teresting interpretation that will set the stage for kernel methods including gaussian processes if we substitute 3 53 into the expression 3 3 we see that the predictivechapter 6 mean can be written in the form y x mn mt n œÜ x Œ≤œÜ x tsn œÜtt n n 1 Œ≤œÜ x tsn œÜ xn tn 3 60 where sn is deÔ¨Åned by 3 51',\n",
       " '3 3 3 equivalent kernel the posterior mean solution 3 53 for the linear basis function model has an in teresting interpretation that will set the stage for kernel methods including gaussian processes if we substitute 3 53 into the expression 3 3 we see that the predictivechapter 6 mean can be written in the form y x mn mt n œÜ x Œ≤œÜ x tsn œÜtt n n 1 Œ≤œÜ x tsn œÜ xn tn 3 60 where sn is deÔ¨Åned by 3 51 thus the mean of the predictive distribution at a point x is given by a linear combination of the training set target variables tn so that we can write y x mn n n 1 k x xn tn 3 61 where the function k x x Œ≤œÜ x tsn œÜ x 3 62 is known as thesmoother matrixor the equivalent kernel',\n",
       " 'if we substitute 3 53 into the expression 3 3 we see that the predictivechapter 6 mean can be written in the form y x mn mt n œÜ x Œ≤œÜ x tsn œÜtt n n 1 Œ≤œÜ x tsn œÜ xn tn 3 60 where sn is deÔ¨Åned by 3 51 thus the mean of the predictive distribution at a point x is given by a linear combination of the training set target variables tn so that we can write y x mn n n 1 k x xn tn 3 61 where the function k x x Œ≤œÜ x tsn œÜ x 3 62 is known as thesmoother matrixor the equivalent kernel regression functions such as this which make predictions by taking linear combinations of the training set target values are known aslinear smoothers note that the equivalent kernel depends on the input values xn from the data set because these appear in the deÔ¨Ånition of sn',\n",
       " 'regression functions such as this which make predictions by taking linear combinations of the training set target values are known aslinear smoothers note that the equivalent kernel depends on the input values xn from the data set because these appear in the deÔ¨Ånition of sn the equivalent kernel is illustrated for the case of gaussian basis functions in figure 3 10 in which the kernel functions k x x have been plotted as a function of x for three different values of x we see that they are localized around x and so the mean of the predictive distribution at x given by y x mn is obtained by forming a weighted combination of the target values in which data points close tox are given higher weight than points further removed from x',\n",
       " 'the equivalent kernel is illustrated for the case of gaussian basis functions in figure 3 10 in which the kernel functions k x x have been plotted as a function of x for three different values of x we see that they are localized around x and so the mean of the predictive distribution at x given by y x mn is obtained by forming a weighted combination of the target values in which data points close tox are given higher weight than points further removed from x intuitively it seems reasonable that we should weight local evidence more strongly than distant evidence note that this localization property holds not only for the localized gaussian basis functions but also for the nonlocal polynomial and sigmoidal basis functions as illustrated in figure 3 11 160 3',\n",
       " 'note that this localization property holds not only for the localized gaussian basis functions but also for the nonlocal polynomial and sigmoidal basis functions as illustrated in figure 3 11 160 3 linear models for regression figure 3 11 examples of equiva lent kernels k x x for x 0 plotted as a function of x corre sponding left to the polynomial ba sis functions and right to the sig moidal basis functions shown in fig ure 3 1 note that these are local ized functions of x even though the corresponding basis functions are nonlocal',\n",
       " 'linear models for regression figure 3 11 examples of equiva lent kernels k x x for x 0 plotted as a function of x corre sponding left to the polynomial ba sis functions and right to the sig moidal basis functions shown in fig ure 3 1 note that these are local ized functions of x even though the corresponding basis functions are nonlocal 1 0 1 0 0 02 0 04 1 0 1 0 0 02 0 04 further insight into the role of the equivalent kernel can be obtained by consid ering the covariance between y x and y x which is given by cov y x y x c o v œÜ x tw wtœÜ x œÜ x tsn œÜ x Œ≤ 1k x x 3 63 where we have made use of 3 49 and 3 62',\n",
       " 'note that these are local ized functions of x even though the corresponding basis functions are nonlocal 1 0 1 0 0 02 0 04 1 0 1 0 0 02 0 04 further insight into the role of the equivalent kernel can be obtained by consid ering the covariance between y x and y x which is given by cov y x y x c o v œÜ x tw wtœÜ x œÜ x tsn œÜ x Œ≤ 1k x x 3 63 where we have made use of 3 49 and 3 62 from the form of the equivalent kernel we see that the predictive mean at nearby points will be highly correlated whereas for more distant pairs of points the correlation will be smaller the predictive distribution shown in figure 3 8 allows us to visualize the point wise uncertainty in the predictions governed by 3 59',\n",
       " 'from the form of the equivalent kernel we see that the predictive mean at nearby points will be highly correlated whereas for more distant pairs of points the correlation will be smaller the predictive distribution shown in figure 3 8 allows us to visualize the point wise uncertainty in the predictions governed by 3 59 however by drawing sam ples from the posterior distribution over w and plotting the corresponding model functions y x w as in figure 3 9 we are visualizing the joint uncertainty in the posterior distribution between the y values at two or more x values as governed by the equivalent kernel the formulation of linear regression in terms of a kernel function suggests an alternative approach to regression as follows',\n",
       " 'however by drawing sam ples from the posterior distribution over w and plotting the corresponding model functions y x w as in figure 3 9 we are visualizing the joint uncertainty in the posterior distribution between the y values at two or more x values as governed by the equivalent kernel the formulation of linear regression in terms of a kernel function suggests an alternative approach to regression as follows instead of introducing a set of basis functions which implicitly determines an equivalent kernel we can instead deÔ¨Åne a localized kernel directly and use this to make predictions for new input vectors x given the observed training set',\n",
       " 'the formulation of linear regression in terms of a kernel function suggests an alternative approach to regression as follows instead of introducing a set of basis functions which implicitly determines an equivalent kernel we can instead deÔ¨Åne a localized kernel directly and use this to make predictions for new input vectors x given the observed training set this leads to a practical framework for regression and classiÔ¨Åcation called gaussian processes which will be discussed in detail in section 6 4 we have seen that the effective kernel deÔ¨Ånes the weights by which the training set target values are combined in order to make a prediction at a new value ofx and it can be shown that these weights sum to one in other words n n 1 k x xn 1 3 64 for all values of x',\n",
       " 'this leads to a practical framework for regression and classiÔ¨Åcation called gaussian processes which will be discussed in detail in section 6 4 we have seen that the effective kernel deÔ¨Ånes the weights by which the training set target values are combined in order to make a prediction at a new value ofx and it can be shown that these weights sum to one in other words n n 1 k x xn 1 3 64 for all values of x this intuitively pleasing result can easily be proven informallyexercise 3 14 by noting that the summation is equivalent to considering the predictive mean ÀÜy x for a set of target data in which tn 1 for all n provided the basis functions are linearly independent that there are more data points than basis functions and that one of the basis functions is constant corresponding to the bias parameter then it is clear that we can Ô¨Åt the training data exactly and hence that the predictive mean will 3 4',\n",
       " 'we have seen that the effective kernel deÔ¨Ånes the weights by which the training set target values are combined in order to make a prediction at a new value ofx and it can be shown that these weights sum to one in other words n n 1 k x xn 1 3 64 for all values of x this intuitively pleasing result can easily be proven informallyexercise 3 14 by noting that the summation is equivalent to considering the predictive mean ÀÜy x for a set of target data in which tn 1 for all n provided the basis functions are linearly independent that there are more data points than basis functions and that one of the basis functions is constant corresponding to the bias parameter then it is clear that we can Ô¨Åt the training data exactly and hence that the predictive mean will 3 4 bayesian model comparison 161 be simply ÀÜy x 1 from which we obtain 3 64',\n",
       " 'this intuitively pleasing result can easily be proven informallyexercise 3 14 by noting that the summation is equivalent to considering the predictive mean ÀÜy x for a set of target data in which tn 1 for all n provided the basis functions are linearly independent that there are more data points than basis functions and that one of the basis functions is constant corresponding to the bias parameter then it is clear that we can Ô¨Åt the training data exactly and hence that the predictive mean will 3 4 bayesian model comparison 161 be simply ÀÜy x 1 from which we obtain 3 64 note that the kernel function can be negative as well as positive so although it satisÔ¨Åes a summation constraint the corresponding predictions are not necessarily convex combinations of the training set target variables',\n",
       " 'bayesian model comparison 161 be simply ÀÜy x 1 from which we obtain 3 64 note that the kernel function can be negative as well as positive so although it satisÔ¨Åes a summation constraint the corresponding predictions are not necessarily convex combinations of the training set target variables finally we note that the equivalent kernel 3 62 satisÔ¨Åes an important property shared by kernel functions in general namely that it can be expressed in the form anchapter 6 inner product with respect to a vector œà x of nonlinear functions so that k x z œà x tœà z 3 65 where œà x Œ≤1 2s1 2 n œÜ x 3 4',\n",
       " 'finally we note that the equivalent kernel 3 62 satisÔ¨Åes an important property shared by kernel functions in general namely that it can be expressed in the form anchapter 6 inner product with respect to a vector œà x of nonlinear functions so that k x z œà x tœà z 3 65 where œà x Œ≤1 2s1 2 n œÜ x 3 4 bayesian model comparison in chapter 1 we highlighted the problem of over Ô¨Åtting as well as the use of cross validation as a technique for setting the values of regularization parameters or for choosing between alternative models here we consider the problem of model se lection from a bayesian perspective',\n",
       " 'bayesian model comparison in chapter 1 we highlighted the problem of over Ô¨Åtting as well as the use of cross validation as a technique for setting the values of regularization parameters or for choosing between alternative models here we consider the problem of model se lection from a bayesian perspective in this section our discussion will be very general and then in section 3 5 we shall see how these ideas can be applied to the determination of regularization parameters in linear regression as we shall see the over Ô¨Åtting associated with maximum likelihood can be avoided by marginalizing summing or integrating over the model parameters in stead of making point estimates of their values',\n",
       " 'in this section our discussion will be very general and then in section 3 5 we shall see how these ideas can be applied to the determination of regularization parameters in linear regression as we shall see the over Ô¨Åtting associated with maximum likelihood can be avoided by marginalizing summing or integrating over the model parameters in stead of making point estimates of their values models can then be compared di rectly on the training data without the need for a validation set this allows all available data to be used for training and avoids the multiple training runs for each model associated with cross validation it also allows multiple complexity parame ters to be determined simultaneously as part of the training process',\n",
       " 'this allows all available data to be used for training and avoids the multiple training runs for each model associated with cross validation it also allows multiple complexity parame ters to be determined simultaneously as part of the training process for example in chapter 7 we shall introduce the relevance vector machine which is a bayesian model having one complexity parameter for every training data point the bayesian view of model comparison simply involves the use of probabilities to represent uncertainty in the choice of model along with a consistent application of the sum and product rules of probability suppose we wish to compare a set of l models m i where i 1 l',\n",
       " 'the bayesian view of model comparison simply involves the use of probabilities to represent uncertainty in the choice of model along with a consistent application of the sum and product rules of probability suppose we wish to compare a set of l models m i where i 1 l here a model refers to a probability distribution over the observed data d in the case of the polynomial curve Ô¨Åtting problem the distribution is deÔ¨Åned over the set of target values t while the set of input values x is assumed to be known other types of model deÔ¨Åne a joint distributions over x and t we shall suppose that the data is generated from one of these models but wesection 1 5 4 are uncertain which one our uncertainty is expressed through a prior probability distribution p mi',\n",
       " 'other types of model deÔ¨Åne a joint distributions over x and t we shall suppose that the data is generated from one of these models but wesection 1 5 4 are uncertain which one our uncertainty is expressed through a prior probability distribution p mi given a training set d we then wish to evaluate the posterior distribution p mi d p mi p d mi 3 66 the prior allows us to express a preference for different models let us simply assume that all models are given equal prior probability the interesting term is the model evidencep d mi which expresses the preference shown by the data for 162 3 linear models for regression different models and we shall examine this term in more detail shortly',\n",
       " 'the interesting term is the model evidencep d mi which expresses the preference shown by the data for 162 3 linear models for regression different models and we shall examine this term in more detail shortly the model evidence is sometimes also called the marginal likelihood because it can be viewed as a likelihood function over the space of models in which the parameters have been marginalized out the ratio of model evidencesp d mi p d mj for two models is known as a bayes factor kass and raftery 1995 once we know the posterior distribution over models the predictive distribution is given from the sum and product rules by p t x d l i 1 p t x mi d p mi d',\n",
       " 'the ratio of model evidencesp d mi p d mj for two models is known as a bayes factor kass and raftery 1995 once we know the posterior distribution over models the predictive distribution is given from the sum and product rules by p t x d l i 1 p t x mi d p mi d 3 67 this is an example of a mixture distributionin which the overall predictive distribu tion is obtained by averaging the predictive distributionsp t x mi d of individual models weighted by the posterior probabilities p mi d of those models',\n",
       " 'once we know the posterior distribution over models the predictive distribution is given from the sum and product rules by p t x d l i 1 p t x mi d p mi d 3 67 this is an example of a mixture distributionin which the overall predictive distribu tion is obtained by averaging the predictive distributionsp t x mi d of individual models weighted by the posterior probabilities p mi d of those models for in stance if we have two models that are a posteriori equally likely and one predicts a narrow distribution around t a while the other predicts a narrow distribution around t b the overall predictive distribution will be a bimodal distribution with modes at t a and t b not a single model at t a b 2',\n",
       " '3 67 this is an example of a mixture distributionin which the overall predictive distribu tion is obtained by averaging the predictive distributionsp t x mi d of individual models weighted by the posterior probabilities p mi d of those models for in stance if we have two models that are a posteriori equally likely and one predicts a narrow distribution around t a while the other predicts a narrow distribution around t b the overall predictive distribution will be a bimodal distribution with modes at t a and t b not a single model at t a b 2 a simple approximation to model averaging is to use the single most probable model alone to make predictions this is known as model selection',\n",
       " 'a simple approximation to model averaging is to use the single most probable model alone to make predictions this is known as model selection for a model governed by a set of parameters w the model evidence is given from the sum and product rules of probability by p d mi p d w mi p w mi d w 3 68 from a sampling perspective the marginal likelihood can be viewed as the proba chapter 11 bility of generating the data set d from a model whose parameters are sampled at random from the prior it is also interesting to note that the evidence is precisely the normalizing term that appears in the denominator in bayes theorem when evaluating the posterior distribution over parameters because p w d m i p d w mi p w mi p d mi',\n",
       " 'for a model governed by a set of parameters w the model evidence is given from the sum and product rules of probability by p d mi p d w mi p w mi d w 3 68 from a sampling perspective the marginal likelihood can be viewed as the proba chapter 11 bility of generating the data set d from a model whose parameters are sampled at random from the prior it is also interesting to note that the evidence is precisely the normalizing term that appears in the denominator in bayes theorem when evaluating the posterior distribution over parameters because p w d m i p d w mi p w mi p d mi 3 69 we can obtain some insight into the model evidence by making a simple approx imation to the integral over parameters',\n",
       " 'it is also interesting to note that the evidence is precisely the normalizing term that appears in the denominator in bayes theorem when evaluating the posterior distribution over parameters because p w d m i p d w mi p w mi p d mi 3 69 we can obtain some insight into the model evidence by making a simple approx imation to the integral over parameters consider Ô¨Årst the case of a model having a single parameter w the posterior distribution over parameters is proportional to p d w p w where we omit the dependence on the model mi to keep the notation uncluttered',\n",
       " '3 69 we can obtain some insight into the model evidence by making a simple approx imation to the integral over parameters consider Ô¨Årst the case of a model having a single parameter w the posterior distribution over parameters is proportional to p d w p w where we omit the dependence on the model mi to keep the notation uncluttered if we assume that the posterior distribution is sharply peaked around the most probable value wmap with width wposterior then we can approximate the in tegral by the value of the integrand at its maximum times the width of the peak if we further assume that the prior is Ô¨Çat with width wprior so that p w 1 wprior then we have p d p d w p w d w p d wmap wposterior wprior 3 70 3 4',\n",
       " 'if we assume that the posterior distribution is sharply peaked around the most probable value wmap with width wposterior then we can approximate the in tegral by the value of the integrand at its maximum times the width of the peak if we further assume that the prior is Ô¨Çat with width wprior so that p w 1 wprior then we have p d p d w p w d w p d wmap wposterior wprior 3 70 3 4 bayesian model comparison 163 figure 3 12 we can obtain a rough approximation to the model evidence if we assume that the posterior distribution over parame ters is sharply peaked around its mode wmap wposterior wprior wmap w and so taking logs we obtain lnp d ln p d wmap l n wposterior wprior 3 71 this approximation is illustrated in figure 3 12',\n",
       " 'wposterior wprior wmap w and so taking logs we obtain lnp d ln p d wmap l n wposterior wprior 3 71 this approximation is illustrated in figure 3 12 the Ô¨Årst term represents the Ô¨Åt to the data given by the most probable parameter values and for a Ô¨Çat prior this would correspond to the log likelihood the second term penalizes the model according to its complexity because wposterior wprior this term is negative and it increases in magnitude as the ratio wposterior wprior gets smaller thus if parameters are Ô¨Ånely tuned to the data in the posterior distribution then the penalty term is large for a model having a set ofm parameters we can make a similar approximation for each parameter in turn',\n",
       " 'thus if parameters are Ô¨Ånely tuned to the data in the posterior distribution then the penalty term is large for a model having a set ofm parameters we can make a similar approximation for each parameter in turn assuming that all parameters have the same ratio of wposterior wprior we obtain lnp d lnp d wmap m ln wposterior wprior 3 72 thus in this very simple approximation the size of the complexity penalty increases linearly with the number m of adaptive parameters in the model',\n",
       " 'assuming that all parameters have the same ratio of wposterior wprior we obtain lnp d lnp d wmap m ln wposterior wprior 3 72 thus in this very simple approximation the size of the complexity penalty increases linearly with the number m of adaptive parameters in the model as we increase the complexity of the model the Ô¨Årst term will typically decrease because a more complex model is better able to Ô¨Åt the data whereas the second term will increase due to the dependence on m the optimal model complexity as determined by the maximum evidence will be given by a trade off between these two competing terms',\n",
       " '3 72 thus in this very simple approximation the size of the complexity penalty increases linearly with the number m of adaptive parameters in the model as we increase the complexity of the model the Ô¨Årst term will typically decrease because a more complex model is better able to Ô¨Åt the data whereas the second term will increase due to the dependence on m the optimal model complexity as determined by the maximum evidence will be given by a trade off between these two competing terms we shall later develop a more reÔ¨Åned version of this approximation based on a gaussian approximation to the posterior distribution section 4 4 1 we can gain further insight into bayesian model comparison and understand how the marginal likelihood can favour models of intermediate complexity by con sidering figure 3 13',\n",
       " 'as we increase the complexity of the model the Ô¨Årst term will typically decrease because a more complex model is better able to Ô¨Åt the data whereas the second term will increase due to the dependence on m the optimal model complexity as determined by the maximum evidence will be given by a trade off between these two competing terms we shall later develop a more reÔ¨Åned version of this approximation based on a gaussian approximation to the posterior distribution section 4 4 1 we can gain further insight into bayesian model comparison and understand how the marginal likelihood can favour models of intermediate complexity by con sidering figure 3 13 here the horizontal axis is a one dimensional representation of the space of possible data sets so that each point on this axis corresponds to a speciÔ¨Åc data set',\n",
       " 'we shall later develop a more reÔ¨Åned version of this approximation based on a gaussian approximation to the posterior distribution section 4 4 1 we can gain further insight into bayesian model comparison and understand how the marginal likelihood can favour models of intermediate complexity by con sidering figure 3 13 here the horizontal axis is a one dimensional representation of the space of possible data sets so that each point on this axis corresponds to a speciÔ¨Åc data set we now consider three models m1 m2 and m3 of successively increasing complexity imagine running these models generatively to produce exam ple data sets and then looking at the distribution of data sets that result any given 164 3',\n",
       " 'imagine running these models generatively to produce exam ple data sets and then looking at the distribution of data sets that result any given 164 3 linear models for regression figure 3 13 schematic illustration of the distribution of data sets for three models of different com plexity in which m1 is the simplest and m3 is the most complex note that the dis tributions are normalized in this example for the partic ular observed data set d0 the model m2 with intermedi ate complexity has the largest evidence p d dd0 m1 m2 m3 model can generate a variety of different data sets since the parameters are governed by a prior probability distribution and for any choice of the parameters there may be random noise on the target variables',\n",
       " 'in this example for the partic ular observed data set d0 the model m2 with intermedi ate complexity has the largest evidence p d dd0 m1 m2 m3 model can generate a variety of different data sets since the parameters are governed by a prior probability distribution and for any choice of the parameters there may be random noise on the target variables to generate a particular data set from a spe ciÔ¨Åc model we Ô¨Årst choose the values of the parameters from their prior distribution p w and then for these parameter values we sample the data from p d w a sim ple model for example based on a Ô¨Årst order polynomial has little variability and so will generate data sets that are fairly similar to each other',\n",
       " 'to generate a particular data set from a spe ciÔ¨Åc model we Ô¨Årst choose the values of the parameters from their prior distribution p w and then for these parameter values we sample the data from p d w a sim ple model for example based on a Ô¨Årst order polynomial has little variability and so will generate data sets that are fairly similar to each other its distribution p d is therefore conÔ¨Åned to a relatively small region of the horizontal axis by contrast a complex model such as a ninth order polynomial can generate a great variety of different data sets and so its distribution p d is spread over a large region of the space of data sets',\n",
       " 'its distribution p d is therefore conÔ¨Åned to a relatively small region of the horizontal axis by contrast a complex model such as a ninth order polynomial can generate a great variety of different data sets and so its distribution p d is spread over a large region of the space of data sets because the distributions p d mi are normalized we see that the particular data set d0 can have the highest value of the evidence for the model of intermediate complexity essentially the simpler model cannot Ô¨Åt the data well whereas the more complex model spreads its predictive probability over too broad a range of data sets and so assigns relatively small probability to any one of them',\n",
       " 'because the distributions p d mi are normalized we see that the particular data set d0 can have the highest value of the evidence for the model of intermediate complexity essentially the simpler model cannot Ô¨Åt the data well whereas the more complex model spreads its predictive probability over too broad a range of data sets and so assigns relatively small probability to any one of them implicit in the bayesian model comparison framework is the assumption that the true distribution from which the data are generated is contained within the set of models under consideration provided this is so we can show that bayesian model comparison will on average favour the correct model to see this consider two models m1 and m2 in which the truth corresponds to m1',\n",
       " 'provided this is so we can show that bayesian model comparison will on average favour the correct model to see this consider two models m1 and m2 in which the truth corresponds to m1 for a given Ô¨Ånite data set it is possible for the bayes factor to be larger for the incorrect model however if we average the bayes factor over the distribution of data sets we obtain the expected bayes factor in the form p d m1 l np d m1 p d m2 dd 3 73 where the average has been taken with respect to the true distribution of the data this quantity is an example of thekullback leibler divergence and satisÔ¨Åes the prop section 1 6 1 erty of always being positive unless the two distributions are equal in which case it is zero thus on average the bayes factor will always favour the correct model',\n",
       " ...]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_chunks = [advanced_text_preprocessing(chunk) for chunk in all_chunks]\n",
    "preprocessed_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a17776bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_chunks(chunks, metadata):\n",
    "    \"\"\"Analyze chunk quality and distribution\"\"\"\n",
    "    \n",
    "    chunk_lengths = [len(chunk) for chunk in chunks]\n",
    "    \n",
    "    print(\"üìä CHUNK ANALYSIS\")\n",
    "    print(f\"Total Chunks: {len(chunks)}\")\n",
    "    print(f\"Average Length: {np.mean(chunk_lengths):.0f} characters\")\n",
    "    print(f\"Min Length: {min(chunk_lengths)} characters\")\n",
    "    print(f\"Max Length: {max(chunk_lengths)} characters\")\n",
    "    print(f\"Median Length: {np.median(chunk_lengths):.0f} characters\")\n",
    "    \n",
    "    # Show distribution by document\n",
    "    doc_chunk_counts = {}\n",
    "    for meta in metadata:\n",
    "        doc_name = meta['source_file']\n",
    "        doc_chunk_counts[doc_name] = doc_chunk_counts.get(doc_name, 0) + 1\n",
    "    \n",
    "    print(f\"\\nTop documents by chunk count:\")\n",
    "    sorted_docs = sorted(doc_chunk_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    for doc, count in sorted_docs[:5]:\n",
    "        print(f\"  {doc}: {count} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c89be025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä CHUNK ANALYSIS\n",
      "Total Chunks: 12561\n",
      "Average Length: 760 characters\n",
      "Min Length: 53 characters\n",
      "Max Length: 39826 characters\n",
      "Median Length: 741 characters\n",
      "\n",
      "Top documents by chunk count:\n",
      "  Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf: 4281 chunks\n",
      "  Deep Learning by Ian Goodfellow, Yoshua Bengio, Aaron Courville.pdf: 3981 chunks\n",
      "  Hands_On_Machine_Learning_with_Scikit_Learn_and_TensorFlow.pdf: 2404 chunks\n",
      "  understanding-machine-learning-theory-algorithms.pdf: 1895 chunks\n"
     ]
    }
   ],
   "source": [
    "analyze_chunks(all_chunks, chunk_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da313720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview_chunks(chunks, metadata, num_chunks=3):\n",
    "    \"\"\"Preview some chunks to verify quality\"\"\"\n",
    "    \n",
    "    for i in range(min(num_chunks, len(chunks))):\n",
    "        chunk = chunks[i]\n",
    "        meta = metadata[i]\n",
    "        \n",
    "        print(f\"\\n--- CHUNK {i+1} ---\")\n",
    "        print(f\"Source: {meta['source_file']}\")\n",
    "        print(f\"Length: {len(chunk)} chars\")\n",
    "        print(f\"Content: {chunk[:200]}...\")\n",
    "        print(\"--- END CHUNK ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "115ad266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- CHUNK 1 ---\n",
      "Source: Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf\n",
      "Length: 802 chars\n",
      "Content: Information Science and Statistics Series Editors: M. Jordan J. Kleinberg B. Scho¬®lkopf Information Science and Statistics Akaike and Kitagawa: The Practice of Time Series Analysis. Bishop: Pattern Re...\n",
      "--- END CHUNK ---\n",
      "\n",
      "--- CHUNK 2 ---\n",
      "Source: Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf\n",
      "Length: 526 chars\n",
      "Content: Marchette: Computer Intrusion Detection and Network Monitoring: A Statistical Viewpoint. Rubinstein and Kroese: The Cross-Entropy Method: A Unified Approach to Combinatorial Optimization, Monte Carlo ...\n",
      "--- END CHUNK ---\n",
      "\n",
      "--- CHUNK 3 ---\n",
      "Source: Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf\n",
      "Length: 777 chars\n",
      "Content: Wallace: Statistical and Inductive Inference by Minimum Massage Length. Christopher M. Bishop Pattern Recognition and Machine Learning Christopher M. Bishop F.R.Eng. Assistant Director Microsoft Resea...\n",
      "--- END CHUNK ---\n"
     ]
    }
   ],
   "source": [
    "preview_chunks(all_chunks, chunk_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bb021b",
   "metadata": {},
   "source": [
    "# Save Chunks for Next Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1fe6ae59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_chunks_and_metadata(chunks, metadata, chunks_file = 'data/processed/chunks.pkl', metadata_file = 'data/processed/metadata.pkl'):\n",
    "    \"Save chunks and metadata for next phase\"\n",
    "\n",
    "    # Save chunks\n",
    "    with open(chunks_file, 'wb') as f:\n",
    "        pickle.dump(chunks, f)\n",
    "\n",
    "    # Save metadata\n",
    "    with open(metadata_file, 'wb') as f:\n",
    "        pickle.dump(metadata, f)\n",
    "    \n",
    "    print(f\"‚úÖ Saved {len(chunks)} chunks and metadata\")\n",
    "    print(f\"   Chunks: {chunks_file}\")\n",
    "    print(f\"   Metadata: {metadata_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "43dd48a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved 12561 chunks and metadata\n",
      "   Chunks: data/processed/chunks.pkl\n",
      "   Metadata: data/processed/metadata.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save processed chunks\n",
    "save_chunks_and_metadata(all_chunks, chunk_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83222a79",
   "metadata": {},
   "source": [
    "# Embedding Generation and FAISS Database Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b9141522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_gemini_embeddings():\n",
    "    \"Initialize GEMINI Embeddings model\"\n",
    "    try:\n",
    "        embedding_model = GoogleGenerativeAIEmbeddings(\n",
    "            model = 'models/embedding-001',\n",
    "            google_api_key = GEMINI_API_KEY,\n",
    "            output_dimensionality=1536\n",
    "        )\n",
    "\n",
    "        test_embeddings = embedding_model.embed_query(\"This is a test sentence.\")\n",
    "        embedding_dimension = len(test_embeddings)\n",
    "\n",
    "        print(f\"‚úÖ Gemini embedding initialized !\")\n",
    "        print(f\"Embedding dimension: {embedding_dimension}\")\n",
    "        return embedding_model, embedding_dimension\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to initialize Gemini embeddings: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "87184b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Gemini embedding initialized !\n",
      "Embedding dimension: 768\n"
     ]
    }
   ],
   "source": [
    "embedding_model, embedding_dim = initialize_gemini_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873a3582",
   "metadata": {},
   "source": [
    "# Batch Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a11418bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings_batch(chunks, embeddings_model, batch_size = 50):\n",
    "    \"Generate embeddings for chunks in batches\"\n",
    "    all_embeddings = []\n",
    "    total_batches = (len(chunks) + batch_size - 1) // batch_size\n",
    "\n",
    "    print(f\"Generating embeddings for {len(chunks)} chunks in {total_batches} batches...\")\n",
    "\n",
    "    for i in tqdm(range(0, len(chunks), batch_size), desc = \"Embedding batches\"):\n",
    "        batch_chunks = chunks[i:i + batch_size]\n",
    "\n",
    "        try:\n",
    "            # Generate embeddings for batch\n",
    "            batch_embeddings = embeddings_model.embed_documents(batch_chunks)\n",
    "            all_embeddings.extend(batch_embeddings)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {i//batch_size + 1}: {e}\")\n",
    "            # Add empty Embeddings as placeholder\n",
    "            all_embeddings.extend([None] * len(batch_chunks))\n",
    "    print(f\"‚úÖ Generated {len(all_embeddings)} embeddings\")\n",
    "    return all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "82708eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 12561 chunks in 252 batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding batches:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 169/252 [08:06<04:26,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in batch 170: 'utf-8' codec can't encode character '\\ud835' in position 384: surrogates not allowed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding batches:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 190/252 [09:04<02:57,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in batch 191: 'utf-8' codec can't encode character '\\ud835' in position 123: surrogates not allowed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding batches:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 192/252 [09:07<02:12,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in batch 193: 'utf-8' codec can't encode character '\\ud835' in position 563: surrogates not allowed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding batches:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 212/252 [09:51<01:32,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in batch 213: 'utf-8' codec can't encode character '\\ud835' in position 240: surrogates not allowed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 252/252 [11:32<00:00,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated 12561 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "chunk_embeddings = generate_embeddings_batch(all_chunks, embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee3e1c6",
   "metadata": {},
   "source": [
    "# FAISS Database Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1fcd5ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_faiss_index(embeddings, embedding_dimensions):\n",
    "    \"Create and populate FAISS index\"\n",
    "\n",
    "    # Fileter out None embeddings (failed embeddings)\n",
    "    valid_embeddings = [emb for emb in embeddings if emb is not None]\n",
    "    valid_indices = [i for i, emb in enumerate(embeddings) if emb is not None]\n",
    "\n",
    "    print(f\"Creating FAISS index with {len(valid_embeddings)} valid embeddings\")\n",
    "\n",
    "    # Create FAISS index (Inner product for cosine similarity)\n",
    "    index = faiss.IndexFlatIP(embedding_dimensions)\n",
    "\n",
    "    # Convert to numpy index and normalize for cosine similarity\n",
    "    embeddings_array = np.array(valid_embeddings).astype('float32')\n",
    "    faiss.normalize_L2(embeddings_array)\n",
    "\n",
    "    # Add embeddings to index\n",
    "    index.add(embeddings_array)\n",
    "\n",
    "    print(f\"‚úÖ FAISS index created with {index.ntotal} vectors\")\n",
    "    return index, valid_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1d9147fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating FAISS index with 12361 valid embeddings\n",
      "‚úÖ FAISS index created with 12361 vectors\n"
     ]
    }
   ],
   "source": [
    "faiss_index, valid_chunk_indices = create_faiss_index(chunk_embeddings, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f304dd2",
   "metadata": {},
   "source": [
    "# Test Retrival System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d49dc66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç QUERY: What is machine learning?\n",
      "üìä Retrieved 5 chunks:\n",
      "\n",
      "--- RESULT 1 (Score: 0.745) ---\n",
      "Source: Hands_On_Machine_Learning_with_Scikit_Learn_and_TensorFlow.pdf\n",
      "Content: Machine Learning is the science (and art) of programming computers so they can learn from data. Here is a slightly more general definition: [Machine Learning is the] field of study that gives computer...\n",
      "\n",
      "--- RESULT 2 (Score: 0.743) ---\n",
      "Source: understanding-machine-learning-theory-algorithms.pdf\n",
      "Content: We are surrounded by a machine learning based technology: search engines learn how to bring us the best results (while placing proÔ¨Åtable ads), anti-spam software learns to Ô¨Ålter our email messages, an...\n",
      "\n",
      "--- RESULT 3 (Score: 0.740) ---\n",
      "Source: understanding-machine-learning-theory-algorithms.pdf\n",
      "Content: In a sense, machine learning can be viewed as a branch of AI (ArtiÔ¨Åcial Intelligence), since, after all, the ability to turn expe- rience into expertise or to detect meaningful patterns in complex sen...\n",
      "\n",
      "--- RESULT 4 (Score: 0.739) ---\n",
      "Source: understanding-machine-learning-theory-algorithms.pdf\n",
      "Content: However, one should note that, in contrast with traditional AI, machine learning is not trying to build automated imitation of intelligent behavior, but rather to use the strengths and 1.5 How to Read...\n",
      "\n",
      "--- RESULT 5 (Score: 0.736) ---\n",
      "Source: Deep Learning by Ian Goodfellow, Yoshua Bengio, Aaron Courville.pdf\n",
      "Content: 5.1 Learning Algorithms A machine learning algorithm is an algorithm that is able to learn from data. But what do we mean by learning? Mitchell 1997( ) provides the deÔ¨Ånition ‚ÄúA computer program is sa...\n",
      "\n",
      "\n",
      "üîç QUERY: How do neural networks work?\n",
      "üìä Retrieved 5 chunks:\n",
      "\n",
      "--- RESULT 1 (Score: 0.746) ---\n",
      "Source: Deep Learning by Ian Goodfellow, Yoshua Bengio, Aaron Courville.pdf\n",
      "Content: The training examples specify directly what the output layer must do at each point x; it must produce a value that is close to y. The behavior of the other layers is not directly speciÔ¨Åed by the train...\n",
      "\n",
      "--- RESULT 2 (Score: 0.739) ---\n",
      "Source: Deep Learning by Ian Goodfellow, Yoshua Bengio, Aaron Courville.pdf\n",
      "Content: An individual neuron or small collection of neurons is not particularly useful. Biological neurons are not especially densely connected. As seen in Ô¨Ågure ,1.10 our machine learning models have had a n...\n",
      "\n",
      "--- RESULT 3 (Score: 0.737) ---\n",
      "Source: Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf\n",
      "Content: In a Bayesian neural network, the prior distribution over the parameter vector w, in conjunction with the network function f(x,w), produces a prior distribution over functions from y(x) where y is the...\n",
      "\n",
      "--- RESULT 4 (Score: 0.734) ---\n",
      "Source: Deep Learning by Ian Goodfellow, Yoshua Bengio, Aaron Courville.pdf\n",
      "Content: Each unit resembles a neuron in the sense that it receives input from many other units and computes its own activation value. The idea of using many layers of vector-valued representation is drawn fro...\n",
      "\n",
      "--- RESULT 5 (Score: 0.733) ---\n",
      "Source: Deep Learning by Ian Goodfellow, Yoshua Bengio, Aaron Courville.pdf\n",
      "Content: Finally, these networks are called neural because they are loosely inspired by neuroscience. Each hidden layer of the network is typically vector-valued. The dimensionality of these hidden layers dete...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_faiss_retrieval(query, faiss_index, embeddings_model, chunks, metadata, valid_indices, k=5):\n",
    "    \"\"\"Test FAISS retrieval with a query\"\"\"\n",
    "    \n",
    "    # Generate query embedding\n",
    "    query_embedding = embeddings_model.embed_query(query)\n",
    "    query_vector = np.array([query_embedding]).astype('float32')\n",
    "    faiss.normalize_L2(query_vector)\n",
    "    \n",
    "    # Search FAISS index\n",
    "    scores, indices = faiss_index.search(query_vector, k)\n",
    "    \n",
    "    print(f\"\\nüîç QUERY: {query}\")\n",
    "    print(f\"üìä Retrieved {len(indices[0])} chunks:\\n\")\n",
    "    \n",
    "    results = []\n",
    "    for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
    "        if idx < len(valid_indices):\n",
    "            original_idx = valid_indices[idx]\n",
    "            chunk = chunks[original_idx]\n",
    "            meta = metadata[original_idx]\n",
    "            \n",
    "            print(f\"--- RESULT {i+1} (Score: {score:.3f}) ---\")\n",
    "            print(f\"Source: {meta['source_file']}\")\n",
    "            print(f\"Content: {chunk[:200]}...\")\n",
    "            print()\n",
    "            \n",
    "            results.append({\n",
    "                'chunk': chunk,\n",
    "                'metadata': meta,\n",
    "                'score': score\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test retrieval with sample queries\n",
    "test_queries = [\n",
    "    \"What is machine learning?\",\n",
    "    \"How do neural networks work?\",\n",
    "    \"What is backpropagation?\",\n",
    "    \"Explain supervised learning\"\n",
    "]\n",
    "\n",
    "for query in test_queries[:2]:  # Test first 2 queries\n",
    "    results = test_faiss_retrieval(query, faiss_index, embedding_model, \n",
    "                                 all_chunks, chunk_metadata, valid_chunk_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f5446e",
   "metadata": {},
   "source": [
    "# Save FAISS Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f8218bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_faiss_database(faiss_index, chunks, metadata, valid_indices, base_path = 'data/faiss_index'):\n",
    "    \"Save FAISS index and associated data\"\n",
    "\n",
    "    # Create Directory\n",
    "    Path(base_path).mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "    # Save FAISS index\n",
    "    faiss.write_index(faiss_index, f\"{base_path}/intellect_engine.index\")\n",
    "\n",
    "    # Save chunks and metadata\n",
    "    with open(f\"{base_path}/chunks.pkl\", 'wb') as f:\n",
    "        pickle.dump(chunks, f)\n",
    "    \n",
    "    with open(f\"{base_path}/metadata.pkl\", 'wb') as f:\n",
    "        pickle.dump(metadata, f)\n",
    "    \n",
    "    with open(f\"{base_path}/valid_indices.pkl\", 'wb') as f:\n",
    "        pickle.dump(valid_indices, f)\n",
    "\n",
    "    # Save index info\n",
    "    index_info = {\n",
    "        'total_chunks': len(chunks),\n",
    "        'valid_chunks': len(valid_indices),\n",
    "        'embedding_dimension': faiss_index.d,\n",
    "        'index_type': 'IndexFlatIP',\n",
    "        'created_at': datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "    with open(f\"{base_path}/index_info.json\", 'w') as f:\n",
    "        json.dump(index_info, f, indent = 2)\n",
    "    \n",
    "    print(f\"‚úÖ FAISS database saved to {base_path}\")\n",
    "    print(f\"   Index file: intellect_engine.index\")\n",
    "    print(f\"   Total Vectors: {faiss_index.ntotal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e91da24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FAISS database saved to data/faiss_index\n",
      "   Index file: intellect_engine.index\n",
      "   Total Vectors: 12361\n"
     ]
    }
   ],
   "source": [
    "save_faiss_database(faiss_index, all_chunks, chunk_metadata, valid_chunk_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f0c4ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
